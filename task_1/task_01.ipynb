{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Практикум по программированию на языке Python`\n",
    "\n",
    "## `Задание 01. Полносвязная нейронная сеть на numpy`.\n",
    "\n",
    "#### Фамилия, имя: \n",
    "\n",
    "Дата выдачи: <span style=\"color:red\">__20 февраля__</span>.\n",
    "\n",
    "Мягкий дедлайн: <span style=\"color:red\">__6 марта 23:59__</span>.\n",
    "\n",
    "Стоимость: __10 баллов__ (основная часть заданий) + __3 балла__ (дополнительные задания).\n",
    "\n",
    "<span style=\"color:red\">__В ноутбуке все клетки должны выполняться без ошибок при последовательном их выполнении.__</span>\n",
    "\n",
    "#### `Москва, 2024`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T13:10:06.486579600Z",
     "start_time": "2024-03-02T13:10:06.421914800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## `Теоретическая часть (3 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "В этом блоке вам нужно решить 3 задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Задание 1. Градиенты для слоя Batch normalization (1.5 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Рассмотрим слой Batch normalization. Пусть на вход этого слоя был подан батч из $n$ объектов, при этом у всех объектов по 1 признаку. Представим вход BN слоя в виде $X \\in \\mathbb{R}^{n \\times 1}$.\n",
    "\n",
    "Тогда в этом слое производятся следующие вычисления:\n",
    "\n",
    "$$ \\mu = \\frac1n \\sum_{i=1}^{n} X_i $$\n",
    "\n",
    "$$ \\sigma^2 = \\frac1n \\sum_{i=1}^{n} \\left( X_i - \\mu \\right) ^2 $$\n",
    "\n",
    "$$ \\tilde{y_i} = \\frac{X_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} $$\n",
    "\n",
    "$$ y_i = \\gamma \\tilde{y_i} + \\delta $$\n",
    "\n",
    "Выходом BN слоя является $y_i$, а $\\gamma,\\delta\\in\\mathbb{R}$ — параметры, которые подбираются во время обучения вместе с другими параметрами нейронной сети (наряду, например, с весами линейного слоя).\n",
    "\n",
    "Рассмотрим нейронную сеть, в которой есть BN слой. Предположим, что вычисления в нейронной сети завершаются подсчетом функции потерь $\\mathcal{L}$. Пусть мы выполнили прямой проход по нейронной сети и сейчас делаем обратный проход с помощью метода обратного распространения ошибки. Пусть BN слою пришел градиент функции потерь по выходу BN слоя ($\\nabla_{y} \\mathcal{L}$).\n",
    "\n",
    "В этом задании вам нужно записать вычисление градиента функции потерь по параметрам слоя BN $\\gamma$ и $\\delta$ ($\\nabla_{\\gamma} \\mathcal{L}$, $\\nabla_{\\delta} \\mathcal{L}$) через $\\nabla_{y} \\mathcal{L}$, а также вам нужно записать вычисление градиента функции потерь по входу слоя BN $X$ ($\\nabla_{X} \\mathcal{L}$) через $\\nabla_{y} \\mathcal{L}$.\n",
    "\n",
    "Хочу заметить, в данном задании мы рассматриваем объекты всего с 1 признаком, чтобы упростить выкладки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Ваше решение:__\n",
    "\n",
    "Перепишем формулу для $y_i$ в векторном виде:\n",
    "$$y_i = \\gamma \\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\delta $$\n",
    "Считаем, что $\\mathcal{L} = \\mathcal{L}(y(X, \\gamma, \\delta))$, тогда:\n",
    "1. $\\nabla_{\\gamma} {\\mathcal L} = (\\nabla_{y} {\\mathcal L})^T \\cdot \\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}$\n",
    "2. $\\nabla_{\\delta} {\\mathcal L} = (\\nabla_{y} {\\mathcal L})^T \\cdot (1)_{i=1}^{n} = \\sum_{i=1}^n (\\nabla_{y} {\\mathcal L})_i$\n",
    "3. $\\nabla_{X} {\\mathcal L} = \\nabla_{y} {\\mathcal L} \\cdot \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\varepsilon}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Задание 2. Вывод инициализации весов линейного слоя при использовании ReLU в качестве функции активации (1 балл)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Рассмотрим полносвязную нейронную сеть с функцией активации $g(y)$. Пусть сеть состоит из $L$ слоев и размер входа слоя $l$ равен $n_l\\ \\ (l = \\overline{1, L})$.\n",
    "\n",
    "Обозначим за $x^l \\in \\mathbb{R}^{n_{l}}$ вход слоя $l$, за $y^l \\in \\mathbb{R}^{n_{l+1}}$ — выход слоя $l$, за $W^l \\in \\mathbb{R}^{n_{l+1} \\times n_{l}}$ — веса слоя $l$, за $b^l \\in \\mathbb{R}^{n_{l+1}}$ — вектор сдвига слоя $l$.\n",
    "\n",
    "Тогда\n",
    "$$y^l = W^l x^l + b^l,$$\n",
    "$$x^{l+1} = g(y^l).$$\n",
    "\n",
    "На паре вы выводили хорошую инициализацию для линейного слоя в случае, когда в качестве функции активации $g(y)$ в нейронной сети используется гиперболический тангенс $g(y) = \\tanh(y)$. Сейчас вам нужно сделать подобный вывод для случая, когда в сети в качестве функций активации используется $g(y) = ReLU(y) = \\max(0, y)$.\n",
    "\n",
    "Сделаем следующие предположения насчет того, как распределены веса $W^l$, вектор сдвига $b^l$, входной вектор $x^l$, выходной вектор $y^l$ линейного слоя и градиенты функции потерь $\\frac{\\partial L}{\\partial y^{l}}$ $(l = \\overline{1, L}$):\n",
    "\n",
    "1. Все компоненты в $W^l$ распределены одинаково и независимо друг от друга;\n",
    "2. Все компоненты в $y^l$ распределены одинаково и независимо друг от друга;\n",
    "3. Все компоненты в $x^l$ распределены одинаково и независимо друг от друга;\n",
    "4. Все компоненты в $\\frac{\\partial L}{\\partial y^{l}}$ распределены одинаково и независимо друг от друга;\n",
    "5. Все компоненты в $W^l$ и все компоненты в $x^l$ независимы друг от друга;\n",
    "6. Все компоненты в $W^l$ имеют четную плотность распределения (то есть симметричную относительно нуля: $p_{W^l}(-x) = p_{W^l}(x)$);\n",
    "7. Все компоненты в $W^l$ имеют конечное матожидание;\n",
    "8. Вектор $b^l$ инициализирован нулями.\n",
    "\n",
    "\n",
    "Подсказки:\n",
    "1. Из пунктов 6 и 7 следует, что все компоненты в $W^l$ имеют нулевое среднее ($\\mathbb{E} W^l_{ij} = 0\\ \\ \\forall i = \\overline{1, n_{l+1}}, j = \\overline{1, n_{l}}$) (докажите);\n",
    "2. Из пунктов 6 и 7 следует, что все компоненты в $y^l$ имеют нулевое среднее и четную плотность распределения (докажите)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Ваше решение:__\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Задание 3. Почему функция активации` $ReLU(y) = \\max(0, y)$ `предпочтительней сигмоиды` $\\sigma(y) = \\frac{1}{1 + \\exp(-y)}$ `в нейронных сетях? (0.5 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Дайте развернутый ответ на вопрос \"Почему функция активации $ReLU(y) = \\max(0, y)$ предпочтительней сигмоиды $\\sigma(y) = \\frac{1}{1 + \\exp(-y)}$ в нейронных сетях?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Ваш ответ:__\n",
    "\n",
    "На практике, зачастую, используются именно глубокие нейронные сети. \n",
    "Из курса лекций известно, что $\\sup_{y \\in \\mathbb{R}} \\left| \\frac{d \\, \\sigma}{d \\, y} (y) \\right| = \\frac{1}{4}$.\n",
    "Рассмотрим глубокую сеть с $d$ слоями: ДОПИСАТЬ\n",
    "Тогда будет наблюдаться затухание градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## `Практическая часть (7 баллов)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Реализация нейронной сети (3 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "from typing import Tuple, List, Union"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:10:11.325372200Z",
     "start_time": "2024-03-02T13:10:11.259510900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.input = None\n",
    "        self.inference_mode = True\n",
    "        \n",
    "    def forward(self, input: NDArray) -> NDArray:\n",
    "        raise NotImplementedError(\"forward method not implemented\")\n",
    "    \n",
    "    def backward(self, grad_output: NDArray) -> Tuple[NDArray, NDArray]:\n",
    "        raise NotImplementedError(\"backward method not implemented\")\n",
    "    \n",
    "    def __call__(self, input: NDArray) -> NDArray:\n",
    "        return self.forward(input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:10:51.106228700Z",
     "start_time": "2024-03-02T13:10:51.061712100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:10:51.919722100Z",
     "start_time": "2024-03-02T13:10:51.873206800Z"
    }
   },
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "\n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "\n",
    "    Some layers also have learnable parameters.\n",
    "\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\n",
    "        \"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "\n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "\n",
    "        d `loss` / d `input` = (d `loss` / d `layer`) * (d `layer` / d `input`)\n",
    "\n",
    "        Luckily, you already receive d `loss` / d `layer` in argument, \n",
    "        so you only need to multiply it by d `layer` / d `input`.\n",
    "\n",
    "        NB: Sometimes d `layer` / d `input` can be a 3D or even 4D tensor.\n",
    "        So it's better to write down the `loss` differential and extract\n",
    "        d `layer` / d `input` from it so that only 2D tensors were present.\n",
    "\n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "\n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "\n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `Слой нелинейности ReLU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Для начала реализуем слой нелинейности $ReLU(y) = \\max(0, y)$. Параметров у слоя нет. Метод `forward` должен вернуть результат поэлементного применения $ReLU$ к входному массиву, метод `backward` — градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:11:42.628756300Z",
     "start_time": "2024-03-02T13:11:42.562372500Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        \n",
    "        self.input = input        \n",
    "        return np.maximum(0, input)\n",
    "        \n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.input is None:\n",
    "            raise RuntimeError('Call forward method before calling backward')\n",
    "        \n",
    "        return np.where(self.input > 0, grad_output, np.zeros_like(self.input)), []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Relu()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `Полносвязный слой`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига.\n",
    "\n",
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить `.ravel()` ко всем градиентам, а затем воспользоваться `np.r_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:11:44.931712800Z",
     "start_time": "2024-03-02T13:11:44.900183600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 2., 3.])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "np.r_[np.eye(3).ravel(), np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:45.620990300Z",
     "start_time": "2024-03-02T11:29:45.518475400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = x W + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units) * 0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = x W + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "        return input @ self.weights + self.biases\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(f\"DEBUG: {(self.input[:, :, None] @ grad_output[:, None, :]).shape=}\")\n",
    "        d_loss_d_w = self.input[:, :, None] @ grad_output[:, None, :]\n",
    "        d_loss_d_b = grad_output\n",
    "        d_loss_d_x = grad_output @ self.weights.T\n",
    "        \n",
    "        return d_loss_d_x, [d_loss_d_w, d_loss_d_b]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Dense({self.weights.shape[0]}, {self.weights.shape[1]})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `Проверка градиента`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция `eval_numerical_gradient` принимает на вход callable объект `f` (функцию от одного аргумента-матрицы) и аргумент `x` и вычисляет приближенный градиент функции `f` в точке `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:45.674427700Z",
     "start_time": "2024-03-02T11:29:45.621992100Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Выпишите аналитический градиент в этой ячейке:\n",
    "Пусть $x = (x_1, \\ldots, x_n)$\n",
    "\n",
    "$$ f(x) = \\sum_i \\left( ReLU(x) \\right)_i = \\sum_i max(0, x_i)$$\n",
    "$$f: \\mathbb{R} ^ {n \\times m} \\to \\mathbb{R}^{m}$$\n",
    "\n",
    "$$\\nabla_{x} f = \\chi (x), \\text{где } \\chi (s) = \\begin{cases} 1 \\; & s > 0 \\\\ 0 \\; & s \\le 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:45.735430100Z",
     "start_time": "2024-03-02T11:29:45.661395300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads.shape=(10, 12)\tnumeric_grads.shape=(10, 12)\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "# print(relu.forward(points).shape)\n",
    "\n",
    "### your code here\n",
    "\n",
    "def f(x):\n",
    "    out = relu.forward(x)\n",
    "    return np.sum(out, axis=1)\n",
    "\n",
    "f(points)\n",
    "grads = relu.backward(np.ones((10, 12)))[0]\n",
    "grads\n",
    "# numeric_grads = eval_numerical_gradient(f, points, verbose=True)\n",
    "numeric_grads = np.zeros_like(points)\n",
    "for i in range(points.shape[0]):\n",
    "    numeric_grads[i, :] = eval_numerical_gradient(f, points[i, :].reshape((1, -1)), verbose=False)\n",
    "\n",
    "print(f\"{grads.shape=}\\t{numeric_grads.shape=}\")\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$\n",
    "\n",
    "Выпишите аналитический градиент в этой ячейке (советуем выписать градиент через дифференциал функции $f$):\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:45.796489300Z",
     "start_time": "2024-03-02T11:29:45.736428300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads.shape=(10, 12)\tnumeric_grads.shape=(10, 12)\n"
     ]
    }
   ],
   "source": [
    "linear = Dense(12, 32)\n",
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "def foo(x):\n",
    "    out = linear.forward(x)\n",
    "    return np.sum(out, axis=1)\n",
    "\n",
    "foo(points)\n",
    "grads = linear.backward(np.ones((10, 32)))[0]\n",
    "\n",
    "numeric_grads = np.zeros_like(points)\n",
    "for i in range(10):\n",
    "    numeric_grads[i, :] = eval_numerical_gradient(foo, points[i, :].reshape((1, -1)), verbose=False)\n",
    "\n",
    "print(f\"{grads.shape=}\\t{numeric_grads.shape=}\")\n",
    "\n",
    "# grads - numeric_grads\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0), np.max(np.abs(grads - numeric_grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `Реализация softmax-слоя и функции потерь`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют $softmax$ в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\left \\{\\frac {\\exp(x^i)}{\\sum_{j=1}^K \\exp(x^j)} \\right \\}_{i=1}^K, \\quad K - \\text{число классов.}$$\n",
    "\n",
    "Здесь за $x^i$ мы обозначаем $i$-ый признак объекта $x$.\n",
    "\n",
    "В качестве функции потерь выберем отрицательный логарифм правдоподобия (по английски: negative log likelihood или NLL)\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y^i \\log \\hat y^i,$$\n",
    "где $y^i = 1$, если объект принадлежит $i$-му классу, и $y^i = 0$ иначе.\n",
    "\n",
    "NLL совпадает с выражением для [кросс-энтропии](https://ru.wikipedia.org/wiki/Перекрёстная_энтропия) (в качестве первого распределения берем вырожденное распределение $y$, в качестве второго — предсказанное распределение $\\hat y$). Очевидно, что эту функцию потерь также можно переписать через индексацию, если через $y$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y}$$\n",
    "\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T13:13:02.970075Z",
     "start_time": "2021-03-03T13:13:02.961134Z"
    },
    "hidden": true
   },
   "source": [
    "Для обучения нейронной сети будем оптимизировать эту функцию потерь по параметрам нейронной сети:\n",
    "\n",
    "$$ \\frac1N \\sum_{i=1}^N L(y_i, \\hat y_i) = \\frac1N \\sum_{i=1}^N L(y_i, \\text{NN}(x_i)) \\rightarrow \\min_{w}\\,,$$\n",
    "где за $x_i$ и $y_i$ мы обозначили признаки и таргет $i$-ого объекта обучающей выборки, за $\\text{NN}$ мы обозначили нейронную сеть, которая по признакам объекта $x_i$ выдает распределение вероятностей $\\hat y_i$, за $w$ мы обозначили все веса нейронной сети, а $N$ — это число объектов в обучающей выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Реализуйте слой `LogSoftmax` (у этого слоя нет параметров). Метод `forward` должен вычислять логарифм от $softmax$, а метод `backward` — пропускать градиенты. В общем случае в промежуточных вычислениях `backward` получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент `grad_output` — это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица).\n",
    "\n",
    "**Пожелание.** Постарайтесь максимально упростить формулу градиентов, чтобы получился лаконичный и стабильный код. Большие и страшные реализации часто оказываются нестабильными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:45.915563300Z",
     "start_time": "2024-03-02T11:29:45.769487Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:45.938636500Z",
     "start_time": "2024-03-02T11:29:45.874317600Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogSoftmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.output = input - logsumexp(input, axis=-1, keepdims=True)\n",
    "        \n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.input is None:\n",
    "            raise RuntimeError('Call forward method before calling backward')\n",
    "        \n",
    "        grad_output_sum = np.sum(grad_output, axis=-1, keepdims=True)\n",
    "        \n",
    "        # exp_arr = np.exp(self.input)\n",
    "        # \n",
    "        # exp_sum = np.sum(exp_arr, axis=-1, keepdims=True)\n",
    "        # \n",
    "        # d_loss_d_x_1 = grad_output - grad_output_sum * (exp_arr / exp_sum)\n",
    "        \n",
    "        # frac = np.sum(np.exp(self.input[:, :, None] - self.input[:, None, :]), axis=1)\n",
    "        # \n",
    "        # d_loss_d_x_2 = grad_output - grad_output_sum / frac\n",
    "        \n",
    "        d_loss_d_x_2 = grad_output - grad_output_sum * np.exp(self.output)\n",
    "        \n",
    "        # print(f\"Derivative deviation: {np.max(np.abs(d_loss_d_x_1 - d_loss_d_x_2))}\")\n",
    "        \n",
    "        return d_loss_d_x_2, []\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'LogSoftmax()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. Во время вычисления NLL усредняйте (а не суммируйте) значения функции потерь по батчу. Обычно так делают для того, чтобы при двух запусках обучения нейронной сети с разными размерами батча получаемые значения функции потерь у этих сетей были сравнимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:46.012634200Z",
     "start_time": "2024-03-02T11:29:45.937118800Z"
    }
   },
   "outputs": [],
   "source": [
    "def NLL(activations, target):\n",
    "    \"\"\"\n",
    "    Returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes, it's just output of LogSoftmax layer).\n",
    "    `activations` has shape [batch, num_classes], `target` has shape [batch]\n",
    "    Output shape: 1 (scalar).\n",
    "    \"\"\"\n",
    "    \n",
    "    return -np.mean(activations[np.arange(len(target)), target])\n",
    "\n",
    "\n",
    "def grad_NLL(activations, target):\n",
    "    \"\"\"\n",
    "    Returns gradient of negative log-likelihood w.r.t. activations.\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \"\"\"\n",
    "    \n",
    "    ohe_target = np.zeros_like(activations)\n",
    "    ohe_target[np.arange(len(target)), target] = 1\n",
    "    \n",
    "    # print(f\"Max delta {np.max(np.exp(activations))}\")\n",
    "    \n",
    "    return -ohe_target / len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Наконец, выполните проверку `LogSoftmax`-слоя, используя функцию потерь и ее градиент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:46.155427400Z",
     "start_time": "2024-03-02T11:29:46.013637300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max deviation: 3.1168190424424314e-11\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "d = 10\n",
    "lsm = LogSoftmax()\n",
    "np.random.seed(42)\n",
    "target = np.random.choice(d, size=n)\n",
    "points = np.random.uniform(-1, 1, n*d).reshape([n, d])\n",
    "\n",
    "def foo(x):\n",
    "    return NLL(lsm.forward(x), target)\n",
    "\n",
    "pred = lsm.forward(points)\n",
    "NLL(pred, target)\n",
    "grads, _ = lsm.backward(grad_NLL(pred, target))\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(foo, points, verbose=False)\n",
    "\n",
    "print(f\"Max deviation: {np.max(np.abs(grads - numeric_grads))}\")\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.116819064126475e-11"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Загрузка данных`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом `digits`, каждый объект в котором — это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:48.926147300Z",
     "start_time": "2024-03-02T11:29:46.780545300Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:49.144043600Z",
     "start_time": "2024-03-02T11:29:47.668868Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:49.332043800Z",
     "start_time": "2024-03-02T11:29:49.024148900Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:49.379638200Z",
     "start_time": "2024-03-02T11:29:49.332043800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((1797, 64), (1797,))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:49.727638500Z",
     "start_time": "2024-03-02T11:29:49.500924800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:49.730637800Z",
     "start_time": "2024-03-02T11:29:49.685420700Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:49.793638500Z",
     "start_time": "2024-03-02T11:29:49.689218400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((1347, 64), (450, 64))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x1000 with 9 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    ax[divmod(i, 3)].imshow(X[i, :].reshape((8, 8)), cmap=plt.get_cmap(\"gray\"))\n",
    "    ax[divmod(i, 3)].set_title(y[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.662397600Z",
     "start_time": "2024-03-02T11:29:49.693639Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Сборка и обучение нейронной сети (0.8 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "В нашей реализации мы представляем нейронную сеть в виде списка ее слоев. Например, следующая функция конструирует нейронную сеть заданной ширины (то есть с заданным размером скрытых слоев) и глубины (то есть с заданным количеством слоев) с заданным размером входа и выхода, а также с заданной функцией активации между линейными слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.826525800Z",
     "start_time": "2024-03-02T11:29:50.627559900Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_network(input_size, hidden_layers_size, output_size, n_layers=3, activation_class=ReLU):\n",
    "    network = []\n",
    "\n",
    "    for layer_idx in range(n_layers):\n",
    "        # Compute sizes of current linear layer\n",
    "        layer_in = input_size if layer_idx == 0 else hidden_layers_size\n",
    "        layer_out = output_size if layer_idx == n_layers - 1 else hidden_layers_size\n",
    "        \n",
    "        # Add linear layer to the network\n",
    "        network.append(Dense(layer_in, layer_out))\n",
    "\n",
    "        # Add activation after each layer except the last one\n",
    "        if layer_idx != n_layers - 1:\n",
    "            network.append(activation_class())\n",
    "\n",
    "    # Add LogSoftmax layer to the network\n",
    "    network.append(LogSoftmax())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.854523500Z",
     "start_time": "2024-03-02T11:29:50.631789400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Dense(64, 32), Relu(), Dense(32, 32), Relu(), Dense(32, 10), LogSoftmax()]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_layers_size = 32\n",
    "output_size = 10\n",
    "\n",
    "network = make_network(input_size, hidden_layers_size, output_size, 3, ReLU)\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Реализуйте функцию, которая выполнет прямой проход по нейронной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.894524300Z",
     "start_time": "2024-03-02T11:29:50.637483800Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward(network: List[Layer], X):\n",
    "    \"\"\"\n",
    "    Perform forward pass through the network.\n",
    "    \n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "\n",
    "    output: \n",
    "    output shape: [batch, out_features_num]\n",
    "    \"\"\"\n",
    "    \n",
    "    for layer in network:\n",
    "        X = layer.forward(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять долю правильных ответов (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте (логично в качестве предсказания на очередном объекте выдавать тот класс, для которого предсказанный логарифм вероятности максимален):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.897524800Z",
     "start_time": "2024-03-02T11:29:50.640001200Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(network: List[Layer], X):\n",
    "    \"\"\"\n",
    "    Returns predictions for each object in X.\n",
    "    \n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    \n",
    "    pred = forward(network, X)\n",
    "    \n",
    "    return np.argmax(pred, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля `scipy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.912525700Z",
     "start_time": "2024-03-02T11:29:50.642831100Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.919524900Z",
     "start_time": "2024-03-02T11:29:50.647241500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized.\n",
      "    \n",
      "            ``fun(x, *args) -> float``\n",
      "    \n",
      "        where ``x`` is a 1-D array with shape (n,) and ``args``\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where ``n`` is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object, see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending on whether or not the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr.\n",
      "        If it is a callable, it should be a function that returns the gradient\n",
      "        vector:\n",
      "    \n",
      "            ``jac(x, *args) -> array_like, shape (n,)``\n",
      "    \n",
      "        where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "        the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "        assumed to return a tuple ``(f, g)`` containing the objective\n",
      "        function and the gradient.\n",
      "        Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "        'trust-krylov' require that either a callable be supplied, or that\n",
      "        `fun` return the objective and gradient.\n",
      "        If None or False, the gradient will be estimated using 2-point finite\n",
      "        difference estimation with an absolute step size.\n",
      "        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "        to select a finite difference scheme for numerical estimation of the\n",
      "        gradient with a relative step size. These finite difference schemes\n",
      "        obey any specified `bounds`.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg, trust-krylov, trust-exact and trust-constr.\n",
      "        If it is callable, it should return the Hessian matrix:\n",
      "    \n",
      "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "    \n",
      "        where ``x`` is a (n,) ndarray and ``args`` is a tuple with the fixed\n",
      "        parameters.\n",
      "        The keywords {'2-point', '3-point', 'cs'} can also be used to select\n",
      "        a finite difference scheme for numerical estimation of the hessian.\n",
      "        Alternatively, objects implementing the `HessianUpdateStrategy`\n",
      "        interface can be used to approximate the Hessian. Available\n",
      "        quasi-Newton methods implementing this interface are:\n",
      "    \n",
      "            - `BFGS`;\n",
      "            - `SR1`.\n",
      "    \n",
      "        Not all of the options are available for each of the methods; for\n",
      "        availability refer to the notes.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given. If `hess` is\n",
      "        provided, then `hessp` will be ignored. `hessp` must compute the\n",
      "        Hessian times an arbitrary vector:\n",
      "    \n",
      "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "    \n",
      "        where ``x`` is a (n,) ndarray, ``p`` is an arbitrary vector with\n",
      "        dimension (n,) and ``args`` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell, and\n",
      "        trust-constr methods. There are two ways to specify the bounds:\n",
      "    \n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "    \n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition. Only for COBYLA, SLSQP and trust-constr.\n",
      "    \n",
      "        Constraints for 'trust-constr' are defined as a single object or a\n",
      "        list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "    \n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "    \n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. When `tol` is specified, the selected\n",
      "        minimization algorithm sets some relevant solver-specific tolerance(s)\n",
      "        equal to `tol`. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods except `TNC` accept the\n",
      "        following generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform. Depending on the\n",
      "                method each iteration may use several function evaluations.\n",
      "    \n",
      "                For `TNC` use `maxfun` instead of `maxiter`.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration. For 'trust-constr' it is a callable with\n",
      "        the signature:\n",
      "    \n",
      "            ``callback(xk, OptimizeResult state) -> bool``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector. and ``state``\n",
      "        is an `OptimizeResult` object, with the same fields\n",
      "        as the ones from the return. If callback returns True\n",
      "        the algorithm execution is terminated.\n",
      "        For all the other methods, the signature is:\n",
      "    \n",
      "            ``callback(xk)``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iterations\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Bound-Constrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken. If bounds are not provided, then an\n",
      "    unbounded line search will be used. If bounds are provided and\n",
      "    the initial guess is within the bounds, then every function\n",
      "    evaluation throughout the minimization procedure will be within\n",
      "    the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (default has full rank), then\n",
      "    some function evaluations during the first iteration may be\n",
      "    outside the bounds, but every function evaluation after the first\n",
      "    iteration will be within the bounds. If `direc` is not full rank,\n",
      "    then some parameters may not be optimized and the solution is not\n",
      "    guaranteed to be within the bounds.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    **Constrained Minimization**\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It swiches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints are imposed as well, it swiches to the trust-region\n",
      "    interior point method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "    \n",
      "    **Finite-Difference Options**\n",
      "    \n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handle complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as many operations. If the\n",
      "    gradient is estimated via finite-differences the Hessian must be\n",
      "    estimated using one of the quasi-Newton strategies.\n",
      "    \n",
      "    **Method specific options for the** `hess` **keyword**\n",
      "    \n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n",
      "    +==============+======+==========+=========================+=====+\n",
      "    | Newton-CG    | x    | (n, n)   | x                       | x   |\n",
      "    |              |      | LO       |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | dogleg       |      | (n, n)   |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-ncg    |      | (n, n)   | x                       | x   |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-krylov |      | (n, n)   | x                       | x   |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-exact  |      | (n, n)   |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-constr | x    | (n, n)   |  x                      | x   |\n",
      "    |              |      | LO       |                         |     |\n",
      "    |              |      | sp       |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    \n",
      "    where LO=LinearOperator, sp=Sparse matrix, HUS=HessianUpdateStrategy\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", :arxiv:`1611.04718`\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации — начальное приближение (одномерный `numpy`-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список `layer.params`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.937524600Z",
     "start_time": "2024-03-02T11:29:50.653560200Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].reshape(param.shape)\n",
    "            i += l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в `minimize`. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (NLL) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.940525700Z",
     "start_time": "2024-03-02T11:29:50.656840300Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    \n",
    "    set_weights(weights, network)\n",
    "    \n",
    "    activations = forward(network, X)\n",
    "    loss = NLL(activations, y)\n",
    "    \n",
    "    weights_grad_arrays = []\n",
    "    \n",
    "    d_loss_d_input = grad_NLL(activations, y)\n",
    "    for layer in reversed(network):\n",
    "        d_loss_d_input, d_loss_d_layer = layer.backward(d_loss_d_input)\n",
    "        if isinstance(layer, Dense):\n",
    "            # print(f\"{layer=}\\tdW shape = {d_loss_d_layer[0].shape}\\tdb shape = {d_loss_d_layer[1].shape}\")\n",
    "            # weights_grad += (np.sum(d_loss_d_layer[0], axis=0).ravel().tolist())\n",
    "            # weights_grad += (np.sum(d_loss_d_layer[1], axis=0).ravel().tolist())\n",
    "            weights_grad_arrays.append(np.sum(d_loss_d_layer[1], axis=0).ravel().tolist())\n",
    "            weights_grad_arrays.append(np.sum(d_loss_d_layer[0], axis=0).ravel().tolist())\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"{layer=}\")\n",
    "        # weights_grad += d_loss_d_layer\n",
    "    \n",
    "    weights_grad_arrays.reverse()\n",
    "    weights_grad = []\n",
    "    for wg in weights_grad_arrays:\n",
    "        weights_grad += wg\n",
    "    \n",
    "    # print(f\"Mean abs grad value: {np.mean(np.abs(weights_grad))}\\tMax abs grad value: {np.max(np.abs(weights_grad))}\")\n",
    "    # print(f\"Loss = {loss}\")\n",
    "    \n",
    "    return loss, np.array(weights_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:50.998525200Z",
     "start_time": "2024-03-02T11:29:50.661397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.01217283,  0.00159463, -0.00892788, ...,  0.        ,\n        0.        ,  0.        ])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = make_network(input_size, hidden_layers_size, output_size, 3, ReLU)\n",
    "weights = get_weights(network)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.026912800Z",
     "start_time": "2024-03-02T11:29:50.671524Z"
    }
   },
   "outputs": [],
   "source": [
    "res = minimize(\n",
    "    compute_loss_grad, weights,       # fun and start point\n",
    "    args=[network, X_train, y_train], # args passed to fun\n",
    "    method=\"L-BFGS-B\",                # optimization method\n",
    "    jac=True,                         # says that gradient is computed in fun,\n",
    "    options={'disp': True},\n",
    "    tol=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.036823200Z",
     "start_time": "2024-03-02T11:29:53.026912800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['fun', 'jac', 'nfev', 'njev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.043833200Z",
     "start_time": "2024-03-02T11:29:53.030805700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "107"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.066919600Z",
     "start_time": "2024-03-02T11:29:53.035825600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"message\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.077917500Z",
     "start_time": "2024-03-02T11:29:53.040120300Z"
    }
   },
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.100920600Z",
     "start_time": "2024-03-02T11:29:53.043833200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.21728269e-02,  1.59462507e-03, -8.92787515e-03, ...,\n       -1.20743288e+00, -2.57884499e+00, -4.05903066e-01])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Выведите качество на обучении (`X_train`, `y_train`) и на контроле (`X_test`, `y_test`). Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.192921600Z",
     "start_time": "2024-03-02T11:29:53.048461400Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Dense(64, 32), Relu(), Dense(32, 32), Relu(), Dense(32, 10), LogSoftmax()]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.214920700Z",
     "start_time": "2024-03-02T11:29:53.049977900Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.218919400Z",
     "start_time": "2024-03-02T11:29:53.054704600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train NLL: 0.0000031\t\tTest NLL: 0.7393872\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "set_weights(weights=res[\"x\"], network=network)\n",
    "\n",
    "train_NLL = NLL(forward(network, X_train), y_train)\n",
    "test_NLL = NLL(forward(network, X_test), y_test)\n",
    "\n",
    "print(f\"Train NLL: {train_NLL:.7f}\\t\\tTest NLL: {test_NLL:.7f}\")\n",
    "\n",
    "train_accuracy = accuracy_score(y_true=y_train, y_pred=predict(network, X_train))\n",
    "test_accuracy = accuracy_score(y_true=y_test, y_pred=predict(network, X_test))\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}\\t\\tTest accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "У `minimize` есть также аргумент `callback` — в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе `Callback`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:53.251921800Z",
     "start_time": "2024-03-02T11:29:53.062293400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        self.train_NLL = []\n",
    "        self.test_NLL = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        Computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc.\n",
    "        If self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        \n",
    "        train_NLL = NLL(forward(network, X_train), y_train)\n",
    "        test_NLL = NLL(forward(network, X_test), y_test)\n",
    "        \n",
    "        train_accuracy = accuracy_score(y_true=y_train, y_pred=predict(network, X_train))\n",
    "        test_accuracy = accuracy_score(y_true=y_test, y_pred=predict(network, X_test))\n",
    "        \n",
    "        self.train_acc.append(train_accuracy)\n",
    "        self.test_acc.append(test_accuracy)\n",
    "        self.train_NLL.append(train_NLL)\n",
    "        self.test_NLL.append(test_NLL)\n",
    "        \n",
    "        if self.print:\n",
    "            print(f\"Train accuracy: {train_accuracy:.3f}\\t\\tTest accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:55.098717500Z",
     "start_time": "2024-03-02T11:29:53.067918900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.106\t\tTest accuracy: 0.084\n",
      "Train accuracy: 0.106\t\tTest accuracy: 0.084\n",
      "Train accuracy: 0.106\t\tTest accuracy: 0.084\n",
      "Train accuracy: 0.131\t\tTest accuracy: 0.104\n",
      "Train accuracy: 0.328\t\tTest accuracy: 0.340\n",
      "Train accuracy: 0.321\t\tTest accuracy: 0.338\n",
      "Train accuracy: 0.395\t\tTest accuracy: 0.400\n",
      "Train accuracy: 0.410\t\tTest accuracy: 0.416\n",
      "Train accuracy: 0.493\t\tTest accuracy: 0.509\n",
      "Train accuracy: 0.566\t\tTest accuracy: 0.578\n",
      "Train accuracy: 0.638\t\tTest accuracy: 0.609\n",
      "Train accuracy: 0.687\t\tTest accuracy: 0.669\n",
      "Train accuracy: 0.710\t\tTest accuracy: 0.673\n",
      "Train accuracy: 0.696\t\tTest accuracy: 0.684\n",
      "Train accuracy: 0.710\t\tTest accuracy: 0.684\n",
      "Train accuracy: 0.722\t\tTest accuracy: 0.711\n",
      "Train accuracy: 0.714\t\tTest accuracy: 0.711\n",
      "Train accuracy: 0.728\t\tTest accuracy: 0.729\n",
      "Train accuracy: 0.753\t\tTest accuracy: 0.727\n",
      "Train accuracy: 0.759\t\tTest accuracy: 0.749\n",
      "Train accuracy: 0.790\t\tTest accuracy: 0.796\n",
      "Train accuracy: 0.801\t\tTest accuracy: 0.800\n",
      "Train accuracy: 0.835\t\tTest accuracy: 0.822\n",
      "Train accuracy: 0.842\t\tTest accuracy: 0.842\n",
      "Train accuracy: 0.845\t\tTest accuracy: 0.827\n",
      "Train accuracy: 0.852\t\tTest accuracy: 0.851\n",
      "Train accuracy: 0.860\t\tTest accuracy: 0.853\n",
      "Train accuracy: 0.852\t\tTest accuracy: 0.853\n",
      "Train accuracy: 0.871\t\tTest accuracy: 0.867\n",
      "Train accuracy: 0.898\t\tTest accuracy: 0.893\n",
      "Train accuracy: 0.899\t\tTest accuracy: 0.904\n",
      "Train accuracy: 0.910\t\tTest accuracy: 0.911\n",
      "Train accuracy: 0.909\t\tTest accuracy: 0.913\n",
      "Train accuracy: 0.915\t\tTest accuracy: 0.916\n",
      "Train accuracy: 0.927\t\tTest accuracy: 0.922\n",
      "Train accuracy: 0.932\t\tTest accuracy: 0.931\n",
      "Train accuracy: 0.942\t\tTest accuracy: 0.931\n",
      "Train accuracy: 0.944\t\tTest accuracy: 0.938\n",
      "Train accuracy: 0.947\t\tTest accuracy: 0.931\n",
      "Train accuracy: 0.944\t\tTest accuracy: 0.931\n",
      "Train accuracy: 0.943\t\tTest accuracy: 0.933\n",
      "Train accuracy: 0.952\t\tTest accuracy: 0.940\n",
      "Train accuracy: 0.958\t\tTest accuracy: 0.938\n",
      "Train accuracy: 0.961\t\tTest accuracy: 0.936\n",
      "Train accuracy: 0.964\t\tTest accuracy: 0.949\n",
      "Train accuracy: 0.964\t\tTest accuracy: 0.947\n",
      "Train accuracy: 0.968\t\tTest accuracy: 0.949\n",
      "Train accuracy: 0.970\t\tTest accuracy: 0.947\n",
      "Train accuracy: 0.972\t\tTest accuracy: 0.953\n",
      "Train accuracy: 0.976\t\tTest accuracy: 0.953\n",
      "Train accuracy: 0.977\t\tTest accuracy: 0.949\n",
      "Train accuracy: 0.980\t\tTest accuracy: 0.949\n",
      "Train accuracy: 0.979\t\tTest accuracy: 0.953\n",
      "Train accuracy: 0.981\t\tTest accuracy: 0.947\n",
      "Train accuracy: 0.984\t\tTest accuracy: 0.944\n",
      "Train accuracy: 0.987\t\tTest accuracy: 0.940\n",
      "Train accuracy: 0.987\t\tTest accuracy: 0.947\n",
      "Train accuracy: 0.988\t\tTest accuracy: 0.947\n",
      "Train accuracy: 0.990\t\tTest accuracy: 0.944\n",
      "Train accuracy: 0.990\t\tTest accuracy: 0.942\n",
      "Train accuracy: 0.993\t\tTest accuracy: 0.944\n",
      "Train accuracy: 0.991\t\tTest accuracy: 0.944\n",
      "Train accuracy: 0.993\t\tTest accuracy: 0.947\n",
      "Train accuracy: 0.993\t\tTest accuracy: 0.949\n",
      "Train accuracy: 0.993\t\tTest accuracy: 0.953\n",
      "Train accuracy: 0.994\t\tTest accuracy: 0.953\n",
      "Train accuracy: 0.996\t\tTest accuracy: 0.953\n",
      "Train accuracy: 0.995\t\tTest accuracy: 0.951\n",
      "Train accuracy: 0.996\t\tTest accuracy: 0.947\n",
      "Train accuracy: 0.997\t\tTest accuracy: 0.942\n",
      "Train accuracy: 0.997\t\tTest accuracy: 0.942\n",
      "Train accuracy: 0.998\t\tTest accuracy: 0.942\n",
      "Train accuracy: 0.999\t\tTest accuracy: 0.942\n",
      "Train accuracy: 0.999\t\tTest accuracy: 0.942\n",
      "Train accuracy: 0.999\t\tTest accuracy: 0.951\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.951\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.951\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.951\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.953\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.960\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.962\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.960\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.960\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.960\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.962\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.962\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.960\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.953\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.956\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "\n",
    "res = minimize(\n",
    "    compute_loss_grad, weights,  \n",
    "    args=[network, X_train, y_train], \n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=True,\n",
    "    callback=cb.call\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Изобразите на графике кривую качества на обучени и контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:55.204786Z",
     "start_time": "2024-03-02T11:29:55.096717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(cb.train_acc, label=f\"train acc [{cb.train_acc[-1]:3.2f}]\")\n",
    "ax.plot(cb.test_acc, label=f\"test acc [{cb.test_acc[-1]:3.2f}]\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Эксперименты с числом слоев (0.6 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучить нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы `accs_train` и `accs_test`. В позиции `[i, j]` должна стоять величина доли правильных ответов сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "[Dense(64, 32), Relu(), Dense(32, 32), Relu(), Dense(32, 10), LogSoftmax()]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:55.210989700Z",
     "start_time": "2024-03-02T11:29:55.203129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def train_network(network: List[Layer], history: bool = False, optimizer=\"L-BFGS-B\", options: dict = None, tol: float = 1e-7, cb_print: bool = False) -> List[Layer]:\n",
    "    cb = Callback(network, X_train, y_train, X_test, y_test, print=cb_print) if history else lambda *args: None\n",
    "    options = dict() if options is None else options\n",
    "\n",
    "    weights = get_weights(network)\n",
    "    res = minimize(\n",
    "        compute_loss_grad, weights,  \n",
    "        args=[network, X_train, y_train], \n",
    "        method=optimizer,\n",
    "        jac=True,\n",
    "        callback=cb.call,\n",
    "        options=options,\n",
    "        tol=tol\n",
    "    )\n",
    "    \n",
    "    if not res[\"success\"]:\n",
    "        raise RuntimeError(f\"Optimization failure: {res['message']}\")\n",
    "    \n",
    "    if history:\n",
    "        return network, cb\n",
    "    else:\n",
    "        return network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:55.222044600Z",
     "start_time": "2024-03-02T11:29:55.207793300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 15))\n",
    "accs_test = np.zeros_like(accs_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:29:55.238045300Z",
     "start_time": "2024-03-02T11:29:55.209990700Z"
    }
   },
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:22.983891600Z",
     "start_time": "2024-03-02T11:29:55.213045400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\tj= 0\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9422\n",
      "i= 0\tj= 1\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9489\n",
      "i= 0\tj= 2\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9489\n",
      "i= 0\tj= 3\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9511\n",
      "i= 0\tj= 4\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9533\n",
      "i= 0\tj= 5\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9422\n",
      "i= 0\tj= 6\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9556\n",
      "i= 0\tj= 7\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9356\n",
      "i= 0\tj= 8\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9489\n",
      "i= 0\tj= 9\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9467\n",
      "i= 0\tj=10\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9511\n",
      "i= 0\tj=11\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9356\n",
      "i= 0\tj=12\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9444\n",
      "i= 0\tj=13\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9467\n",
      "i= 0\tj=14\tlayer_c=1\ttrain acc: 1.0000 test acc: 0.9489\n",
      "i= 1\tj= 0\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9533\n",
      "i= 1\tj= 1\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9711\n",
      "i= 1\tj= 2\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9600\n",
      "i= 1\tj= 3\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9600\n",
      "i= 1\tj= 4\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9489\n",
      "i= 1\tj= 5\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9467\n",
      "i= 1\tj= 6\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9556\n",
      "i= 1\tj= 7\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9489\n",
      "i= 1\tj= 8\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9511\n",
      "i= 1\tj= 9\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9444\n",
      "i= 1\tj=10\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9644\n",
      "i= 1\tj=11\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9622\n",
      "i= 1\tj=12\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9511\n",
      "i= 1\tj=13\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9622\n",
      "i= 1\tj=14\tlayer_c=2\ttrain acc: 1.0000 test acc: 0.9556\n",
      "i= 2\tj= 0\tlayer_c=3\ttrain acc: 1.0000 test acc: 0.9622\n",
      "i= 2\tj= 1\tlayer_c=3\ttrain acc: 1.0000 test acc: 0.9444\n",
      "i= 2\tj= 2\tlayer_c=3\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for i, layer_c in zip(range(5), range(1, 5+1)):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{layer_c=}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        network, cb = train_network(network, history=True)\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]\n",
    "        print(f\"train acc: {accs_train[i, j]:5.4f} test acc: {accs_test[i, j]:5.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце — среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-03-02T11:30:22.982891500Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "axes[0].boxplot(accs_train.T, showfliers=False)\n",
    "axes[1].boxplot(accs_test.T, showfliers=False)\n",
    "\n",
    "axes[0].set_xlabel(\"Number of layers\")\n",
    "axes[1].set_xlabel(\"Number of layers\")\n",
    "\n",
    "axes[0].set_ylabel(\"Train accuracy\")\n",
    "axes[1].set_ylabel(\"Test accuracy\")\n",
    "\n",
    "axes[0].set_title(f\"Train quality in {accs_train.shape[1]} runs\")\n",
    "axes[1].set_title(f\"Test quality in {accs_train.shape[1]} runs\")\n",
    "\n",
    "axes[0].grid(True)\n",
    "axes[1].grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Дайте развёрнутый ответ на вопросы (в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?\n",
    "\n",
    "__Ответы:__\n",
    "\n",
    "Линейная модель показывает худший результат среди изученных моделей. \n",
    "На обучающей выборке модели стабильно получают точность, сравнимую с 1.\n",
    "На валидационной выборке модели получают качество на несколько процентов хуже, чем на тренировочной. \n",
    "\n",
    "Оптимальное число слоев --- 3 или 4 слоя. \n",
    "При числе слоёв > 4 начинает усиляться переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Эксперименты c различными инициализациями весов (0.6 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Как уже было сказано, начальная инициализация весов нейронной сети может сильно влиять на процесс ее обучения и, как следствие, на ее качество.\n",
    "\n",
    "В этом пункте вам предлагается попробовать обучить несколько нейронных сетей с различными инициализациями слоев.\n",
    "\n",
    "Для этого необходимо реализовать функцию, инициализирующую веса линейных слоёв нашей нейронной сети. Добавьте в функционал данного метода возможность инициализировать его веса с помощью инициализации Kaiming (используется, если в нейронной сети в качестве функций активации используется ReLU) и инициализации Xavier (используется, если в нейронной сети в качестве функций активации используется Tanh или Sigmoid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:26.944372100Z",
     "start_time": "2024-03-02T11:30:26.893860900Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_network(network, initialization):\n",
    "    for layer in network:\n",
    "        if isinstance(layer, Dense):\n",
    "            input_units, output_units = layer.weights.shape\n",
    "            if initialization == 'Kaiming':\n",
    "                sigma = 2 / layer.weights.shape[0]\n",
    "                layer.weights = np.random.normal(loc=0, scale=sigma, size=layer.weights.shape)\n",
    "                \n",
    "            elif initialization == 'Xavier':\n",
    "                sigma = np.sqrt(2 / (layer.weights.shape[0] + layer.weights.shape[1]))\n",
    "                layer.weights = np.random.normal(loc=0, scale=sigma, size=layer.weights.shape)\n",
    "                \n",
    "            else:\n",
    "                # Initialize weights with small random numbers from normal distribution.\n",
    "                # In this case `initialization` represents a standard deviation\n",
    "                # for normal distribution.\n",
    "                layer.weights = np.random.randn(input_units, output_units) * initialization\n",
    "                \n",
    "            layer.biases = np.zeros_like(layer.biases)\n",
    "            \n",
    "            layer.params = [layer.weights, layer.biases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Теперь попробуйте для каждой из 3 инициализаций обучить нейронную сеть несколько раз. Попробуйте проделать данную операцию при зафиксированном числе слоев равным 3, 4 и 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `3 слоя`\n",
    "\n",
    "Зафиксируйте в сети число слоев равное трем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Заполните матрицы `accs_train` и `accs_test`. В позиции `[i, j]` должна стоять величина доли правильных ответов сети при $j$-м запуске (все запуски идентичны) с инициализацией Kaiming при $i = 3$, с инициализацией Xavier при $i = 4$ и с инициализацией из нормального распределения с фиксированными параметрами при $0 \\leqslant i \\leqslant 2$ (попробуйте здесь 3 разных параметра для стандартного отклонения для нормального распределения, например: `1e-3`, `1e-2`, `1e-1`). Заметьте, что при большом числе слоев слишком низкое стандартное отклонение может не давать нейронной сети нормально обучиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:22.991889800Z",
     "start_time": "2024-03-02T11:30:22.983891600Z"
    }
   },
   "outputs": [],
   "source": [
    "init_vars = [1e-3, 1e-2, 1e-1, 'Kaiming', 'Xavier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-03-02T11:30:22.984891400Z"
    }
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 15))\n",
    "accs_test = np.zeros_like(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-03-02T11:30:22.985890600Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "layer_c = 3\n",
    "\n",
    "for i, initialization in enumerate(init_vars):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{initialization=}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, initialization)\n",
    "        network, cb = train_network(network, history=True)\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]\n",
    "        print(f\"train acc: {accs_train[i, j]:5.4f} test acc: {accs_test[i, j]:5.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце — среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2024-03-02T11:30:22.986892600Z"
    }
   },
   "outputs": [],
   "source": [
    "init_vars_for_plot = [(x if isinstance(x, str) else fr\"$\\sigma = {x}$\") for x in init_vars]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=init_vars_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `4 слоя`\n",
    "\n",
    "Выполните тут тот же код, что и в предыдущем пункте, но только уже с 4 слоями в сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.857228800Z"
    }
   },
   "outputs": [],
   "source": [
    "init_vars = [5e-3, 1e-2, 1e-1, 'Kaiming', 'Xavier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.858229300Z"
    }
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 15))\n",
    "accs_test = np.zeros_like(accs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.859227600Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "layer_c = 4\n",
    "\n",
    "for i, initialization in enumerate(init_vars):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{initialization=}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, initialization)\n",
    "        network, cb = train_network(network, history=True)\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]\n",
    "        print(f\"train acc: {accs_train[i, j]:5.4f} test acc: {accs_test[i, j]:5.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце — среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.860228800Z"
    }
   },
   "outputs": [],
   "source": [
    "init_vars_for_plot = [(x if isinstance(x, str) else fr\"$\\sigma = {x}$\") for x in init_vars]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=init_vars_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `5 слоев`\n",
    "\n",
    "Выполните тут тот же код, что и в предыдущем пункте, но только уже с 5 слоями в сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.861227400Z"
    }
   },
   "outputs": [],
   "source": [
    "init_vars = [1e-2, 1e-1, 1e0, 'Kaiming', 'Xavier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.862227200Z"
    }
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 15))\n",
    "accs_test = np.zeros_like(accs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.863229600Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "layer_c = 5\n",
    "\n",
    "for i, initialization in enumerate(init_vars):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{initialization=}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, initialization)\n",
    "        network, cb = train_network(network, history=True)\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]\n",
    "        print(f\"train acc: {accs_train[i, j]:5.4f} test acc: {accs_test[i, j]:5.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце — среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.864228800Z"
    }
   },
   "outputs": [],
   "source": [
    "init_vars_for_plot = [(x if isinstance(x, str) else fr\"$\\sigma = {x}$\") for x in init_vars]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=init_vars_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Дайте развёрнутый ответ на вопросы (в этой же ячейке):\n",
    "* Как отличаются качество на обучении и контроле и устойчивость процесса обучения при различных инициализациях?\n",
    "* Какие инициализации помогают обучать более глубокие сети?\n",
    "\n",
    "__Ответы:__\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Теперь сравним скорость обучения при различных инициализациях. Создайте два списка списков `accs_test_on_iterations`, `accs_train_on_iterations` в каждом из которых в позиции `[i]` (см. описание `i` в предыдущем пункте) будет лежать список из значений `accuracy` на тестовой и обучающей выборках соотвественно, полученных во время обучения модели. Количество слоев в сети зафиксируйте равным 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.865228800Z"
    }
   },
   "outputs": [],
   "source": [
    "init_vars = [1e-3, 1e-2, 1e-1, 'Kaiming', 'Xavier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.866227900Z"
    }
   },
   "outputs": [],
   "source": [
    "accs_test_on_iterations = []\n",
    "accs_train_on_iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.867229800Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "layer_c = 4\n",
    "\n",
    "for i, initialization in enumerate(init_vars):\n",
    "    print(f\"{i=}\\t{initialization=}\")\n",
    "    network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "    initialize_network(network, initialization)\n",
    "    network, cb = train_network(network, history=True)\n",
    "    \n",
    "    accs_test_on_iterations.append(cb.train_acc)\n",
    "    accs_train_on_iterations.append(cb.test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.868230100Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "for idx, label in enumerate(init_vars_for_plot):\n",
    "    ax.plot(accs_test_on_iterations[idx], label=label)\n",
    "\n",
    "ax.set_title(\"Test quality for different initializations\")\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:37:15.087902Z",
     "start_time": "2021-03-03T14:37:15.065996Z"
    },
    "hidden": true
   },
   "source": [
    "Дайте развёрнутый ответ на вопросы (в этой же ячейке):\n",
    "* Как меняется скорость обучения в зависимости от выбранной инициализации?\n",
    "\n",
    "__Ответы:__\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `Эксперименты c различными функциями активации (0.6 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Проверим теперь, с какой функцией активации нейронная сеть будет обучаться лучше.\n",
    "\n",
    "В этом пункте вам предлагается попробовать обучить несколько нейронных сетей с различными функциями активации.\n",
    "\n",
    "Для этого нам нужно реализовать еще 2 слоя: для функций активации `Tanh` и `Sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:33.343047300Z",
     "start_time": "2024-03-02T11:30:33.301533900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"\n",
    "    tanh(y) = (e^y - e^(-y)) / (e^y + e^(-y))\n",
    "    Используйте функцию np.tanh для подсчета гиперболического тангенса.\n",
    "    Вы можете сами реализовать подсчет tanh, но тогда вам нужно устойчиво его вычислять.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.params = [] # Tanh has no parameters\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply elementwise Tanh to [batch, num_units] matrix\n",
    "        \"\"\"\n",
    "        self.cosh = np.cosh(input)\n",
    "        return np.sinh(input) / self.cosh\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss w.r.t. Tanh input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        return -grad_output / np.power(self.cosh, 2)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Tanh()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:33.509099300Z",
     "start_time": "2024-03-02T11:30:33.456954300Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    sigmoid(y) = 1 / (1 + e^(-y))\n",
    "    Используйте функцию expit для подсчета сигмоиды.\n",
    "    Вы можете сами реализовать подсчет сигмоиды, но тогда вам нужно устойчиво ее вычислять.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.params = [] # Sigmoid has no parameters\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply elementwise Sigmoid to [batch, num_units] matrix\n",
    "        \"\"\"\n",
    "        self.output = expit(input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss w.r.t. Sigmoid input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        return self.output * (1 - self.output) * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Sigmoid()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Теперь попробуйте для каждой из 3 функций активации обучить нейронную сеть несколько раз. Число слоев зафиксируйте равным 3. В случае `Tanh` и `Sigmoid` используйте инициализацию `Xavier`, а в случае `ReLU` используйте инициализацию `Kaiming`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Заполните матрицы `accs_train` и `accs_test`. В позиции `[i, j]` должна стоять величина доли правильных ответов сети при $j$-м запуске (все запуски идентичны) с функцией активации $ReLU$ при $i = 0$, с функцией активации $Tanh$ при $i = 1$ и с функцией активации $Sigmoid$ при $i = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:36:15.332224900Z",
     "start_time": "2024-02-29T18:36:15.216707800Z"
    }
   },
   "outputs": [],
   "source": [
    "act_func_vars = ['Tanh', 'Sigmoid', 'ReLU']\n",
    "init_for_act_funcs = ['Xavier', 'Xavier', 'Kaiming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.872229600Z"
    }
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((3, 50))\n",
    "accs_test = np.zeros_like(accs_train)\n",
    "layer_c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.873227400Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for i, (act_func, init_strategy) in enumerate(zip(act_func_vars, init_for_act_funcs)):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{act_func=}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, init_strategy)\n",
    "        network, cb = train_network(network, history=True)\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]\n",
    "        print(f\"train acc: {accs_train[i, j]:5.4f} test acc: {accs_test[i, j]:5.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце — среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.874228Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=act_func_vars, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_train.shape[1]} runs with {layer_c} layers\")\n",
    "ax.set_xlabel(\"Activation function\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Дайте развёрнутый ответ на вопросы (в этой же ячейке):\n",
    "* Как отличаются качество на обучении и контроле и устойчивость процесса обучения при различных функциях активации?\n",
    "\n",
    "__Ответы:__\n",
    "\n",
    "При использовании в качестве функций активации сигмоиды и гиперболического тангенса среднее качество оказывается сравнимым. Однако, дисперсия при использовании сигмоиды оказывается выше. \n",
    "При использовании ReLU среднее качество оказывается ниже.\n",
    "\n",
    "Преимущества ReLU не реализуются при малом количестве слоев (3), так как затухание градиентов оказывается незначительным. \n",
    "Для проверки данной гипотезы будет поставлен *дополнительный* эксперимент с количеством слоев = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accs_train = np.zeros((3, 15))\n",
    "accs_test = np.zeros_like(accs_train)\n",
    "layer_c = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.875226800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for i, (act_func, init_strategy) in enumerate(zip(act_func_vars, init_for_act_funcs)):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{act_func=}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, init_strategy)\n",
    "        network, cb = train_network(network, history=True)\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]\n",
    "        print(f\"train acc: {accs_train[i, j]:5.4f} test acc: {accs_test[i, j]:5.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.876227200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=act_func_vars, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_train.shape[1]} runs with {layer_c} layers\")\n",
    "ax.set_xlabel(\"Activation function\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.877228100Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Реализация метода оптимизации (1.4 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части задания реализуйте метод оптимизации SGD + momentum. Упрощённая версия более общего алгоритма [отсюда](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\rule{70mm}{0.4pt}                                                             \\\\\n",
    "    &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: X \\text{ (data)},       \\\\\n",
    "    &\\hspace{13mm} \\: f(\\theta) \\text{ (objective)}, \\:\\mu \\text{ (momentum)}       \\\\[-1.ex]\n",
    "    &\\rule{70mm}{0.4pt}                                                             \\\\\n",
    "    &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                    \\\\\n",
    "    &\\hspace{5mm}\\mathcal{X} \\leftarrow \\texttt{list\\_of\\_random\\_batches}(X)\\\\\n",
    "    &\\hspace{5mm}\\textbf{for} \\: i=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                    \\\\\n",
    "    &\\hspace{10mm}g_t\\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1},\\mathcal{X}_i)      \\\\\n",
    "    &\\hspace{10mm}\\textbf{if} \\: \\mu \\neq 0                                          \\\\\n",
    "    &\\hspace{15mm}\\textbf{if} \\: t > 1                                              \\\\\n",
    "    &\\hspace{20mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + g_t               \\\\\n",
    "    &\\hspace{15mm}\\textbf{else}                                                     \\\\\n",
    "    &\\hspace{20mm} \\textbf{b}_t \\leftarrow g_t                                      \\\\\n",
    "    &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                     \\\\\n",
    "    &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                      \\\\[-1.ex]\n",
    "    &\\rule{70mm}{0.4pt}                                                             \\\\[-1.ex]\n",
    "    &\\bf{return} \\:  \\theta_t                                                       \\\\[-1.ex]\n",
    "    &\\rule{70mm}{0.4pt}                                                             \\\\[-1.ex]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Проще всего будет написать `custom minimizer` (смотри [документацию scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:39.212776600Z",
     "start_time": "2024-03-02T11:30:39.165445800Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import OptimizeResult\n",
    "\n",
    "\n",
    "def SGD(fun, x0, args, **kwargs) -> OptimizeResult:\n",
    "    callback = kwargs['callback']   # feed weights from each iteration to update network and log metrics\n",
    "    mu = kwargs['momentum']\n",
    "    n_iter = kwargs['n_iter']\n",
    "    gamma = kwargs['lr']\n",
    "    jac = kwargs['jac']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    tolerance = kwargs['tol'] if 'tol' in kwargs else None\n",
    "    tolerance = -1.0 if tolerance is None else tolerance\n",
    "    \n",
    "    if isinstance(args, tuple) and (len(args) == 1):\n",
    "        args = args[0]\n",
    "    \n",
    "    net, X, y = args\n",
    "    \n",
    "    m = 0.0\n",
    "    weights = x0\n",
    "    nit = 0\n",
    "    msg = \"Max iterations reached\"\n",
    "    indxs = np.arange(X.shape[0])\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        np.random.shuffle(indxs)\n",
    "        batch_indxs = np.array_split(indxs, X.shape[0] // batch_size)\n",
    "        loss_sum = 0.0\n",
    "        \n",
    "        for batch_ind in batch_indxs:\n",
    "            loss = fun(weights, (net, X[batch_ind, :], y[batch_ind]))\n",
    "            grad = fun.derivative(weights, (net, X[batch_ind, :], y[batch_ind]))\n",
    "            m = mu * m + grad\n",
    "            weights -= gamma * m\n",
    "            loss_sum += loss\n",
    "        \n",
    "        if callback is not None:\n",
    "            callback(weights)\n",
    "        \n",
    "        nit += 1\n",
    "        loss_sum /= len(batch_indxs)\n",
    "        if loss_sum <= tolerance:\n",
    "            msg = f\"Convergence: loss ({loss_sum}) <= tolerance ({tolerance})\"\n",
    "            break\n",
    "    \n",
    "    res = {\n",
    "        \"message\": msg,\n",
    "        \"success\": True,\n",
    "        \"nit\": nit,\n",
    "        \"x\": weights\n",
    "    }\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1. (0.5 балла)** Продемонстрируйте правильную работу метода оптимизации, получив `test_accuracy>=0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:43.809641100Z",
     "start_time": "2024-03-02T11:30:40.252458800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('Convergence: loss (0.000996467330418903) <= tolerance (0.001)', 153)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_c = 3\n",
    "network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "weights = get_weights(network)\n",
    "\n",
    "res = minimize(\n",
    "    compute_loss_grad, weights,       # fun and start point\n",
    "    args=(network, X_train, y_train), # args passed to fun\n",
    "    method=SGD,                       # optimization method\n",
    "    jac=True,                         # says that gradient is computed in fun,\n",
    "    options={\n",
    "        'n_iter': 1_250,\n",
    "        'momentum': 0.9,\n",
    "        'lr': 1e-2,\n",
    "        'batch_size': 100,\n",
    "    },\n",
    "    tol=1e-3,\n",
    ")\n",
    "\n",
    "res['message'], res['nit']"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train NLL: 0.0009714\t\tTest NLL: 0.2181584\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.962\n"
     ]
    }
   ],
   "source": [
    "train_NLL = NLL(forward(network, X_train), y_train)\n",
    "test_NLL = NLL(forward(network, X_test), y_test)\n",
    "\n",
    "print(f\"Train NLL: {train_NLL:.7f}\\t\\tTest NLL: {test_NLL:.7f}\")\n",
    "\n",
    "train_accuracy = accuracy_score(y_true=y_train, y_pred=predict(network, X_train))\n",
    "test_accuracy = accuracy_score(y_true=y_test, y_pred=predict(network, X_test))\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}\\t\\tTest accuracy: {test_accuracy:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:43.812641200Z",
     "start_time": "2024-03-02T11:30:43.799164500Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2. (0.8 балла)** Сравните три алгоритма:\n",
    "1. LBFGS-B\n",
    "2. SGD\n",
    "3. SGD + momentum\n",
    "\n",
    "Для этого одновременно переберите следующие гиперпараметры:\n",
    "- значения `lr` на отрезке `[1e-3, 1e-1]` по логарифмической сетке\n",
    "- два значения `momentum`: `0` и `0.9`\n",
    "- значения `batch_size`: `8` и `32`\n",
    "\n",
    "В каждом запуске сохраняйте три метрики: итоговое accuracy на тесте и на трейне и время обучения.\n",
    "\n",
    "Для каждого из трёх алгоритмов выберите лучшие `lr` и `batch_size` по `test_accuracy`. Постройте для них кривые обучения (пример ниже). Сделайте выводы.\n",
    "\n",
    "![](comparison.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.882229700Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = SGD\n",
    "optimizer_params = []\n",
    "\n",
    "for lr in [1e-1, 1e-2, 1e-3]:\n",
    "    for batch_size in [9, 32]:\n",
    "        optimizer_params.append({\n",
    "            'n_iter': 250,\n",
    "            'momentum': 0.0,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "        })\n",
    "\n",
    "\n",
    "accs_train = np.zeros((len(optimizer_params), 15))\n",
    "accs_test = np.zeros_like(accs_train)\n",
    "layer_c = 3\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "for i, opt_params in enumerate(optimizer_params):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, \"Kaiming\")\n",
    "        network, cb = train_network(\n",
    "            network, \n",
    "            history=True, \n",
    "            optimizer=optimizer,\n",
    "            options=opt_params,\n",
    "        )\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "momentum_for_plot = [f\"LR = {opt_params['lr']:.3f}\\nBatch size = {opt_params['batch_size']}\" for opt_params in optimizer_params]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=momentum_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.883227100Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "momentum_for_plot = [f\"LR = {opt_params['lr']:.3f}\\nBatch size = {opt_params['batch_size']}\" for opt_params in optimizer_params]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test[1:, :].T, labels=momentum_for_plot[1:], showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.884228300Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = SGD\n",
    "optimizer_params = []\n",
    "\n",
    "for lr in [1e-1, 1e-2, 1e-3]:\n",
    "    for batch_size in [9, 32]:\n",
    "        optimizer_params.append({\n",
    "            'n_iter': 250,\n",
    "            'momentum': 0.9,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "        })\n",
    "\n",
    "\n",
    "accs_train = np.zeros((len(optimizer_params), 15))\n",
    "accs_test = np.zeros_like(accs_train)\n",
    "layer_c = 3\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "for i, opt_params in enumerate(optimizer_params):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, \"Kaiming\")\n",
    "        network, cb = train_network(\n",
    "            network, \n",
    "            history=True, \n",
    "            optimizer=optimizer,\n",
    "            options=opt_params,\n",
    "        )\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.885229700Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "momentum_for_plot = [f\"LR = {opt_params['lr']:.3f}\\nBatch size = {opt_params['batch_size']}\" for opt_params in optimizer_params]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=momentum_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.886228300Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "momentum_for_plot = [f\"LR = {opt_params['lr']:.3f}\\nBatch size = {opt_params['batch_size']}\" for opt_params in optimizer_params]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test[3:, :].T, labels=momentum_for_plot[3:], showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.887229500Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizers = [SGD, SGD, \"L-BFGS-B\"]\n",
    "optimizer_params = []\n",
    "\n",
    "optimizer_params.append({\n",
    "    'n_iter': 1_250,\n",
    "    'momentum': 0.0,\n",
    "    'lr': 0.039811,\n",
    "    'batch_size': 8,\n",
    "    'tol': 1e-4,\n",
    "})\n",
    "\n",
    "optimizer_params.append({\n",
    "    'n_iter': 1_250,\n",
    "    'momentum': 0.9,\n",
    "    'lr': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'tol': 1e-4,\n",
    "})\n",
    "\n",
    "optimizer_params.append({\n",
    "})\n",
    "\n",
    "accs_train = np.zeros((len(optimizer_params), 15))\n",
    "accs_test = np.zeros_like(accs_train)\n",
    "layer_c = 3\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "for i, (opt, opt_params) in enumerate(zip(optimizers, optimizer_params)):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "        initialize_network(network, \"Kaiming\")\n",
    "        network, cb = train_network(\n",
    "            network, \n",
    "            history=True, \n",
    "            optimizer=opt,\n",
    "            options=opt_params,\n",
    "        )\n",
    "        \n",
    "        accs_train[i, j] = cb.train_acc[-1]\n",
    "        accs_test[i, j] = cb.test_acc[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.888229Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "momentum_for_plot = [\"SGD\", \"SGD with Momentum\", \"L-BFGS-B\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=momentum_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Optimizer\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.889227800Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizers = [SGD, SGD, \"L-BFGS-B\"]\n",
    "optimizer_params = []\n",
    "\n",
    "optimizer_params.append({\n",
    "    'n_iter': 250,\n",
    "    'momentum': 0.0,\n",
    "    'lr': 0.1,\n",
    "    'batch_size': 32,\n",
    "})\n",
    "\n",
    "optimizer_params.append({\n",
    "    'n_iter': 250,\n",
    "    'momentum': 0.9,\n",
    "    'lr': 0.01,\n",
    "    'batch_size': 32,\n",
    "})\n",
    "\n",
    "optimizer_params.append({})\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "layer_c = 3\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "for i, (opt, opt_params) in enumerate(zip(optimizers, optimizer_params)):\n",
    "    network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "    initialize_network(network, \"Kaiming\")\n",
    "    network, cb = train_network(\n",
    "        network, \n",
    "        history=True, \n",
    "        optimizer=opt,\n",
    "        options=opt_params,\n",
    "    )\n",
    "    \n",
    "    accs_train.append(cb.train_acc)\n",
    "    accs_test.append(cb.test_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.890227600Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n",
    "colors = ['r', 'g', 'b']\n",
    "optimizer_labels = [\"SGD\", \"SGD with Momentum\", \"L-BFGS-B\"]\n",
    "\n",
    "for train_acc, test_acc, color, label in zip(accs_train, accs_test, colors, optimizer_labels):\n",
    "    ax[0].plot(train_acc, f'{color}--', label=f'{label} train')\n",
    "    ax[1].plot(test_acc, f'{color}-', label=f'{label} test')\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"Iteration\")\n",
    "    ax[i].set_ylabel(\"Accuracy\")\n",
    "    ax[i].grid(True)\n",
    "    ax[i].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.891229900Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3. (0.1 балла)** Для отобранных в прошлом задании трёх алгоритмов ответье на вопрос: как меняются запуски для двух значений `batch_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## `Бонусная часть`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Доп. эксперимент. Исследование работы SGD с неточной (усл-я Армиха-Вульфа) оптимизацией шага обучения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.892228100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### `Реализация метода оптимизации (1 балл)`\n",
    "\n",
    "Реализуйте метод оптимизации Adam и сравните его работу с SGD + momentum, проведя эксперимент, как в задании 2 (перебор гиперпараметров + кривые обучения)."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def Adam(fun, x0, args, **kwargs) -> OptimizeResult:\n",
    "    callback = kwargs['callback']   # feed weights from each iteration to update network and log metrics\n",
    "    beta_momentum = kwargs['beta_momentum']\n",
    "    beta_grad = kwargs['beta_grad']\n",
    "    n_iter = kwargs['n_iter']\n",
    "    gamma = kwargs['lr']\n",
    "    jac = kwargs['jac']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    tolerance = kwargs['tol'] if 'tol' in kwargs else None\n",
    "    tolerance = -1.0 if tolerance is None else tolerance\n",
    "    eps = 1e-6\n",
    "    \n",
    "    if isinstance(args, tuple) and (len(args) == 1):\n",
    "        args = args[0]\n",
    "    \n",
    "    net, X, y = args\n",
    "    \n",
    "    m = 0.0\n",
    "    v = 0.0\n",
    "    weights = x0\n",
    "    nit = 0\n",
    "    msg = \"Max iterations reached\"\n",
    "    \n",
    "    for i in range(1, n_iter+1):\n",
    "        batch_indxs = np.array_split(np.arange(X.shape[0]), X.shape[0] // batch_size)\n",
    "        loss_sum = 0.0\n",
    "        \n",
    "        for batch_ind in batch_indxs:\n",
    "            # print(f\"NEGRO\")\n",
    "            loss = fun(weights, (net, X[batch_ind, :], y[batch_ind]))\n",
    "            grad = fun.derivative(weights, (net, X[batch_ind, :], y[batch_ind]))\n",
    "            m = beta_momentum * m + (1 - beta_momentum) * grad\n",
    "            v = beta_grad * v + (1 - beta_grad) * np.power(grad, 2)\n",
    "            \n",
    "            m_hat = m / (1 - np.power(beta_momentum, i))\n",
    "            v_hat = v / (1 - np.power(beta_grad, i))\n",
    "            \n",
    "            delta = (m_hat / (v_hat + eps))\n",
    "            print(f\"Grad norm = {np.linalg.norm(delta) / len(delta)}\")\n",
    "            \n",
    "            weights -= gamma * delta\n",
    "            loss_sum += loss\n",
    "        \n",
    "        if callback is not None:\n",
    "            callback(weights)\n",
    "        \n",
    "        nit += 1\n",
    "        loss_sum /= len(batch_indxs)\n",
    "        if loss_sum <= tolerance:\n",
    "            msg = f\"Convergence: loss ({loss_sum}) <= tolerance ({tolerance})\"\n",
    "            break\n",
    "    \n",
    "    res = {\n",
    "        \"message\": msg,\n",
    "        \"success\": True,\n",
    "        \"nit\": nit,\n",
    "        \"x\": weights\n",
    "    }\n",
    "    \n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:30:53.324668700Z",
     "start_time": "2024-03-02T11:30:53.281181300Z"
    }
   },
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad norm = 4.196501554474134\n",
      "Grad norm = 2.573210470420377\n",
      "Grad norm = 2.0839486249884542\n",
      "Grad norm = 1.9079376177588399\n",
      "Grad norm = 1.7374175759488446\n",
      "Grad norm = 1.6519860890357785\n",
      "Grad norm = 1.6050825694665845\n",
      "Grad norm = 1.5506450550541793\n",
      "Grad norm = 1.4979233561589997\n",
      "Grad norm = 1.4324480997298108\n",
      "Grad norm = 1.3674555491406648\n",
      "Grad norm = 1.330186081168956\n",
      "Grad norm = 1.2979691565800824\n",
      "Grad norm = 1.2749018996194488\n",
      "Grad norm = 1.2508070774682123\n",
      "Grad norm = 1.2151893618853506\n",
      "Grad norm = 1.1764660398134554\n",
      "Grad norm = 1.1272006469811784\n",
      "Grad norm = 1.0810900912782055\n",
      "Grad norm = 1.0328670025195104\n",
      "Grad norm = 0.9993508922442585\n",
      "Grad norm = 0.9653209582519505\n",
      "Grad norm = 0.9234388756595987\n",
      "Grad norm = 0.8922081153334116\n",
      "Grad norm = 0.8652153308610405\n",
      "Grad norm = 0.8364936341601255\n",
      "Grad norm = 0.8047563680368969\n",
      "Grad norm = 0.7752789034192589\n",
      "Grad norm = 0.7557151364343112\n",
      "Grad norm = 0.7325023374588455\n",
      "Grad norm = 0.7074911855745237\n",
      "Grad norm = 0.683974401739793\n",
      "Grad norm = 0.6606265909726923\n",
      "Grad norm = 0.6421834313602431\n",
      "Grad norm = 0.6241129483433911\n",
      "Grad norm = 0.5984099492342232\n",
      "Grad norm = 0.5722913148404148\n",
      "Grad norm = 0.5475795178909263\n",
      "Grad norm = 0.5199690278438704\n",
      "Grad norm = 0.49016683537890915\n",
      "Grad norm = 0.460382575808352\n",
      "Grad norm = 0.43005921050245743\n",
      "Grad norm = 0.4024308334840196\n",
      "Grad norm = 0.37964831326110043\n",
      "Grad norm = 0.36375038916684743\n",
      "Grad norm = 0.3552123911842563\n",
      "Grad norm = 0.35189594150161063\n",
      "Grad norm = 0.3491137429037447\n",
      "Grad norm = 0.34290886113698726\n",
      "Grad norm = 0.3339273381615868\n",
      "Grad norm = 0.32462005657532833\n",
      "Grad norm = 0.3148033582034538\n",
      "Grad norm = 0.30502731332056343\n",
      "Grad norm = 0.2942522469679646\n",
      "Grad norm = 0.2818494989469267\n",
      "Grad norm = 0.2688459950555145\n",
      "Grad norm = 0.2558484863358809\n",
      "Grad norm = 0.24424940628145236\n",
      "Grad norm = 0.23330042191903202\n",
      "Grad norm = 0.2224998208929254\n",
      "Grad norm = 0.21204124364543617\n",
      "Grad norm = 0.20160202276073735\n",
      "Grad norm = 0.19179683514460394\n",
      "Grad norm = 0.18179967788301277\n",
      "Grad norm = 0.17308203355935478\n",
      "Grad norm = 0.1657041961527968\n",
      "Grad norm = 0.15887663586377368\n",
      "Grad norm = 0.1506264707625569\n",
      "Grad norm = 0.14386497977049434\n",
      "Grad norm = 0.13897739476421042\n",
      "Grad norm = 0.13301916104338501\n",
      "Grad norm = 0.12723039854116414\n",
      "Grad norm = 0.12155774168150725\n",
      "Grad norm = 0.11612919918381817\n",
      "Grad norm = 0.11103746573136085\n",
      "Grad norm = 0.10613935806561449\n",
      "Grad norm = 0.10142007833332907\n",
      "Grad norm = 0.09706934963308282\n",
      "Grad norm = 0.09324833035873319\n",
      "Grad norm = 0.0898639756602836\n",
      "Grad norm = 0.08642114798447886\n",
      "Grad norm = 0.08296656926381014\n",
      "Grad norm = 0.07975468704556717\n",
      "Grad norm = 0.07687993591590615\n",
      "Grad norm = 0.07448212807687912\n",
      "Grad norm = 0.0718519782247653\n",
      "Grad norm = 0.06930333951388339\n",
      "Grad norm = 0.06686627615270002\n",
      "Grad norm = 0.0644827140224141\n",
      "Grad norm = 0.06199778271431975\n",
      "Grad norm = 0.059832524335672724\n",
      "Grad norm = 0.05817535449799737\n",
      "Grad norm = 0.056181519111024036\n",
      "Grad norm = 0.05439627076492517\n",
      "Grad norm = 0.05288512933135901\n",
      "Grad norm = 0.05167552700770078\n",
      "Grad norm = 0.05052879450929774\n",
      "Grad norm = 0.04876775408970927\n",
      "Grad norm = 0.04727548827610397\n",
      "Grad norm = 0.04621174236281522\n",
      "Grad norm = 0.04532862881933641\n",
      "Grad norm = 0.043969404619887326\n",
      "Grad norm = 0.04273782630503277\n",
      "Grad norm = 0.04167194937518407\n",
      "Grad norm = 0.04070045807561127\n",
      "Grad norm = 0.03989992459192992\n",
      "Grad norm = 0.038791508264216366\n",
      "Grad norm = 0.03781427349010818\n",
      "Grad norm = 0.03702221846888401\n",
      "Grad norm = 0.03632989959906151\n",
      "Grad norm = 0.03571471291270097\n",
      "Grad norm = 0.03519446584330524\n",
      "Grad norm = 0.03463529276207998\n",
      "Grad norm = 0.03393091898329469\n",
      "Grad norm = 0.033323177711055293\n",
      "Grad norm = 0.03279328311523235\n",
      "Grad norm = 0.03211263890969069\n",
      "Grad norm = 0.03157927834571303\n",
      "Grad norm = 0.03115822668631381\n",
      "Grad norm = 0.0307957519546011\n",
      "Grad norm = 0.030221522458819997\n",
      "Grad norm = 0.029712721036955145\n",
      "Grad norm = 0.029176434546313718\n",
      "Grad norm = 0.028884352952396317\n",
      "Grad norm = 0.028366449714786638\n",
      "Grad norm = 0.0279018053193598\n",
      "Grad norm = 0.027461850622576268\n",
      "Grad norm = 0.027180885573742573\n",
      "Grad norm = 0.02700083088319331\n",
      "Grad norm = 0.026675233299357365\n",
      "Grad norm = 0.026270246108384618\n",
      "Grad norm = 0.025904117580679415\n",
      "Grad norm = 0.025675634663754963\n",
      "Grad norm = 0.02548780760939927\n",
      "Grad norm = 0.02502785407594748\n",
      "Grad norm = 0.02474331921203486\n",
      "Grad norm = 0.024537232056232535\n",
      "Grad norm = 0.024363968438212938\n",
      "Grad norm = 0.02432361162871332\n",
      "Grad norm = 0.023831577823885734\n",
      "Grad norm = 0.023460206114191966\n",
      "Grad norm = 0.023215316409615182\n",
      "Grad norm = 0.02289381437081219\n",
      "Grad norm = 0.02270981487135216\n",
      "Grad norm = 0.02236537988446747\n",
      "Grad norm = 0.02206152108467503\n",
      "Grad norm = 0.02179868945199515\n",
      "Grad norm = 0.021647334869462537\n",
      "Grad norm = 0.02153862205466874\n",
      "Grad norm = 0.02145653529794983\n",
      "Grad norm = 0.02118631175099042\n",
      "Grad norm = 0.02096409578506935\n",
      "Grad norm = 0.020820604158260664\n",
      "Grad norm = 0.02066157228091659\n",
      "Grad norm = 0.020594399597135528\n",
      "Grad norm = 0.020359348778030507\n",
      "Grad norm = 0.020193384687903722\n",
      "Grad norm = 0.0200305941202868\n",
      "Grad norm = 0.019806413905680754\n",
      "Grad norm = 0.019732175786692393\n",
      "Grad norm = 0.019460979762215166\n",
      "Grad norm = 0.019273907098538125\n",
      "Grad norm = 0.01915553964424756\n",
      "Grad norm = 0.019051773150252485\n",
      "Grad norm = 0.01906228941220246\n",
      "Grad norm = 0.01877910189913919\n",
      "Grad norm = 0.018569388488690493\n",
      "Grad norm = 0.018441709441983003\n",
      "Grad norm = 0.018273182182510964\n",
      "Grad norm = 0.018294177384580116\n",
      "Grad norm = 0.018032732992028847\n",
      "Grad norm = 0.017843083843716436\n",
      "Grad norm = 0.01770027627638153\n",
      "Grad norm = 0.017599705242662803\n",
      "Grad norm = 0.017449586948121112\n",
      "Grad norm = 0.017466721839089604\n",
      "Grad norm = 0.017228618731419496\n",
      "Grad norm = 0.017096527149721503\n",
      "Grad norm = 0.017021647691633064\n",
      "Grad norm = 0.016902089491450768\n",
      "Grad norm = 0.016898201014773784\n",
      "Grad norm = 0.016815659100017696\n",
      "Grad norm = 0.016600972669245876\n",
      "Grad norm = 0.01645593556139237\n",
      "Grad norm = 0.016357611206545365\n",
      "Grad norm = 0.01625586968591238\n",
      "Grad norm = 0.016195047601687284\n",
      "Grad norm = 0.01608753051693684\n",
      "Grad norm = 0.015955280489296394\n",
      "Grad norm = 0.015879583468780364\n",
      "Grad norm = 0.015806766912038927\n",
      "Grad norm = 0.015787677911831433\n",
      "Grad norm = 0.01558929247415242\n",
      "Grad norm = 0.015498201560112402\n",
      "Grad norm = 0.015389351471793713\n",
      "Grad norm = 0.015293497730139181\n",
      "Grad norm = 0.015160725001035573\n",
      "Grad norm = 0.015095955912514496\n",
      "Grad norm = 0.014988287185306332\n",
      "Grad norm = 0.014895843510310936\n",
      "Grad norm = 0.014862970894826232\n",
      "Grad norm = 0.014721499370009347\n",
      "Grad norm = 0.014639229583885673\n",
      "Grad norm = 0.01454966769111121\n",
      "Grad norm = 0.014542535475170997\n",
      "Grad norm = 0.014421806958295999\n",
      "Grad norm = 0.0142891299162189\n",
      "Grad norm = 0.014242793661761687\n",
      "Grad norm = 0.014186318689773148\n",
      "Grad norm = 0.01406765342209314\n",
      "Grad norm = 0.013993889271890023\n",
      "Grad norm = 0.013922932083532803\n",
      "Grad norm = 0.013876852072164972\n",
      "Grad norm = 0.013734961269888615\n",
      "Grad norm = 0.013643166016784102\n",
      "Grad norm = 0.01361005322828856\n",
      "Grad norm = 0.013586662936591044\n",
      "Grad norm = 0.01361054896766192\n",
      "Grad norm = 0.01342159439391281\n",
      "Grad norm = 0.013337540780641076\n",
      "Grad norm = 0.013272092669725446\n",
      "Grad norm = 0.013169210006481637\n",
      "Grad norm = 0.013140834991362609\n",
      "Grad norm = 0.013140040206356047\n",
      "Grad norm = 0.012976598996238048\n",
      "Grad norm = 0.012893692812567165\n",
      "Grad norm = 0.012811755011159037\n",
      "Grad norm = 0.012756615734856896\n",
      "Grad norm = 0.01281615775110532\n",
      "Grad norm = 0.012628216826930057\n",
      "Grad norm = 0.012546965143624257\n",
      "Grad norm = 0.012503162336670377\n",
      "Grad norm = 0.012416397302477948\n",
      "Grad norm = 0.012373382984870085\n",
      "Grad norm = 0.012363018309383708\n",
      "Grad norm = 0.012206288682062586\n",
      "Grad norm = 0.012130816147222402\n",
      "Grad norm = 0.01208756372886366\n",
      "Grad norm = 0.011974808839297276\n",
      "Grad norm = 0.011943179186616453\n",
      "Grad norm = 0.011930976824142394\n",
      "Grad norm = 0.011802896228379982\n",
      "Grad norm = 0.011721623281224757\n",
      "Grad norm = 0.011677747397721053\n",
      "Grad norm = 0.011621397313778894\n",
      "Grad norm = 0.011532439001121698\n",
      "Grad norm = 0.011506100575430308\n",
      "Grad norm = 0.011448426791617908\n",
      "Grad norm = 0.011364247665257876\n",
      "Grad norm = 0.011327790341629094\n",
      "Grad norm = 0.011360597048470882\n",
      "Grad norm = 0.011239082609359543\n",
      "Grad norm = 0.01119184439892424\n",
      "Grad norm = 0.011153529894560523\n",
      "Grad norm = 0.01104296464087287\n",
      "Grad norm = 0.011010126971783966\n",
      "Grad norm = 0.010981124452687013\n",
      "Grad norm = 0.010866079147663025\n",
      "Grad norm = 0.010798066400754801\n",
      "Grad norm = 0.010769967754573097\n",
      "Grad norm = 0.010698657202866144\n",
      "Grad norm = 0.010663388608441132\n",
      "Grad norm = 0.010579614290787326\n",
      "Grad norm = 0.01053648260690214\n",
      "Grad norm = 0.010492109302905735\n",
      "Grad norm = 0.010470524561568984\n",
      "Grad norm = 0.010400285409992471\n",
      "Grad norm = 0.010338323868717315\n",
      "Grad norm = 0.01030478355499895\n",
      "Grad norm = 0.010258357267746819\n",
      "Grad norm = 0.0102239704692339\n",
      "Grad norm = 0.010136864853205441\n",
      "Grad norm = 0.010084078500064776\n",
      "Grad norm = 0.010014041040143997\n",
      "Grad norm = 0.009989495744832386\n",
      "Grad norm = 0.009912572009561593\n",
      "Grad norm = 0.009871470056231632\n",
      "Grad norm = 0.009838154595641788\n",
      "Grad norm = 0.009817535020973846\n",
      "Grad norm = 0.009742653449689683\n",
      "Grad norm = 0.009716820875166263\n",
      "Grad norm = 0.009663701993497711\n",
      "Grad norm = 0.009677728606966286\n",
      "Grad norm = 0.009592755623227372\n",
      "Grad norm = 0.009547729201497579\n",
      "Grad norm = 0.009509738042366543\n",
      "Grad norm = 0.00942612113357141\n",
      "Grad norm = 0.009456896167635006\n",
      "Grad norm = 0.009436395998155057\n",
      "Grad norm = 0.009354117976662647\n",
      "Grad norm = 0.009302441090510413\n",
      "Grad norm = 0.009305352302461312\n",
      "Grad norm = 0.00920377995642615\n",
      "Grad norm = 0.009190290537819243\n",
      "Grad norm = 0.009208315558927819\n",
      "Grad norm = 0.009075141922325008\n",
      "Grad norm = 0.009016316556803265\n",
      "Grad norm = 0.009013087039289193\n",
      "Grad norm = 0.008938014275898373\n",
      "Grad norm = 0.00893341144658254\n",
      "Grad norm = 0.008937909701764602\n",
      "Grad norm = 0.008883644103383603\n",
      "Grad norm = 0.008812253066344474\n",
      "Grad norm = 0.008781650867998984\n",
      "Grad norm = 0.00870707625412332\n",
      "Grad norm = 0.008694281307341992\n",
      "Grad norm = 0.008695517789573582\n",
      "Grad norm = 0.008599933767544476\n",
      "Grad norm = 0.008552027488894812\n",
      "Grad norm = 0.008529676892438958\n",
      "Grad norm = 0.008483252042196535\n",
      "Grad norm = 0.008460839901115003\n",
      "Grad norm = 0.008422963357800696\n",
      "Grad norm = 0.008366243552805372\n",
      "Grad norm = 0.008341913914497491\n",
      "Grad norm = 0.00832196674751932\n",
      "Grad norm = 0.008252393343166265\n",
      "Grad norm = 0.008262646286525155\n",
      "Grad norm = 0.008192945690686494\n",
      "Grad norm = 0.00814843217037573\n",
      "Grad norm = 0.00811824536399227\n",
      "Grad norm = 0.008074594217847925\n",
      "Grad norm = 0.008040098769113426\n",
      "Grad norm = 0.008024697580289758\n",
      "Grad norm = 0.007983688325044242\n",
      "Grad norm = 0.007953802777098888\n",
      "Grad norm = 0.007923018114338319\n",
      "Grad norm = 0.007905976169154188\n",
      "Grad norm = 0.007865269545589176\n",
      "Grad norm = 0.007823707388822902\n",
      "Grad norm = 0.007796450771481942\n",
      "Grad norm = 0.007767726932874207\n",
      "Grad norm = 0.00772183604085531\n",
      "Grad norm = 0.007685092994668021\n",
      "Grad norm = 0.007655840105775487\n",
      "Grad norm = 0.007615206005055297\n",
      "Grad norm = 0.007614414394332756\n",
      "Grad norm = 0.007561652183265627\n",
      "Grad norm = 0.007519555506044944\n",
      "Grad norm = 0.007508686996993323\n",
      "Grad norm = 0.007467223902643496\n",
      "Grad norm = 0.007463486283672054\n",
      "Grad norm = 0.007431901960411771\n",
      "Grad norm = 0.007375531524787783\n",
      "Grad norm = 0.00735524590436973\n",
      "Grad norm = 0.0073149645173301355\n",
      "Grad norm = 0.007293522963430699\n",
      "Grad norm = 0.007269336397866202\n",
      "Grad norm = 0.0072213033639949645\n",
      "Grad norm = 0.007190611926263732\n",
      "Grad norm = 0.007166826577050029\n",
      "Grad norm = 0.007133664446978212\n",
      "Grad norm = 0.007127362585895704\n",
      "Grad norm = 0.007100868763727737\n",
      "Grad norm = 0.007063397373254303\n",
      "Grad norm = 0.007031382286678056\n",
      "Grad norm = 0.007002247414418096\n",
      "Grad norm = 0.00701839377315017\n",
      "Grad norm = 0.006943756944689097\n",
      "Grad norm = 0.006917841520904997\n",
      "Grad norm = 0.006884231366733321\n",
      "Grad norm = 0.006865037813646341\n",
      "Grad norm = 0.0068315331614501925\n",
      "Grad norm = 0.0068149471762488515\n",
      "Grad norm = 0.006780139778962917\n",
      "Grad norm = 0.006747348104256099\n",
      "Grad norm = 0.0067560099383857045\n",
      "Grad norm = 0.006683883725647366\n",
      "Grad norm = 0.006682575000894377\n",
      "Grad norm = 0.0066687924540914\n",
      "Grad norm = 0.006621045173609295\n",
      "Grad norm = 0.006617432984796552\n",
      "Grad norm = 0.006580563551905074\n",
      "Grad norm = 0.006545154780016703\n",
      "Grad norm = 0.00654045323142826\n",
      "Grad norm = 0.006505133279203102\n",
      "Grad norm = 0.006453383946397306\n",
      "Grad norm = 0.00644046581589087\n",
      "Grad norm = 0.006411806303086067\n",
      "Grad norm = 0.006385990857690848\n",
      "Grad norm = 0.0063751651447379026\n",
      "Grad norm = 0.006322630812122435\n",
      "Grad norm = 0.006311018402298359\n",
      "Grad norm = 0.0062768233538765\n",
      "Grad norm = 0.006254409132855376\n",
      "Grad norm = 0.006237423326852725\n",
      "Grad norm = 0.006206221109812009\n",
      "Grad norm = 0.006186583462604039\n",
      "Grad norm = 0.006160887849841687\n",
      "Grad norm = 0.006154056530190997\n",
      "Grad norm = 0.006147975330946869\n",
      "Grad norm = 0.006093893925562394\n",
      "Grad norm = 0.006065249353252319\n",
      "Grad norm = 0.006045606249080465\n",
      "Grad norm = 0.0060259684902204845\n",
      "Grad norm = 0.006005675354009056\n",
      "Grad norm = 0.005977766998195855\n",
      "Grad norm = 0.005946378736536262\n",
      "Grad norm = 0.005934453045725313\n",
      "Grad norm = 0.005931969516070294\n",
      "Grad norm = 0.005890030632316706\n",
      "Grad norm = 0.0058610246647435105\n",
      "Grad norm = 0.0058418965893589055\n",
      "Grad norm = 0.005835007005827531\n",
      "Grad norm = 0.005826798929811652\n",
      "Grad norm = 0.005775570990062391\n",
      "Grad norm = 0.0057548884667298385\n",
      "Grad norm = 0.00573993496540674\n",
      "Grad norm = 0.005715791174981543\n",
      "Grad norm = 0.005699587571021925\n",
      "Grad norm = 0.005686425506224275\n",
      "Grad norm = 0.0056441962840887115\n",
      "Grad norm = 0.005621642797462171\n",
      "Grad norm = 0.005606805921568954\n",
      "Grad norm = 0.00558039296973004\n",
      "Grad norm = 0.00556933513601312\n",
      "Grad norm = 0.005553066965218885\n",
      "Grad norm = 0.005514408295077555\n",
      "Grad norm = 0.005499863487198585\n",
      "Grad norm = 0.005477075705282578\n",
      "Grad norm = 0.005471754962863611\n",
      "Grad norm = 0.005445262529610788\n",
      "Grad norm = 0.00543749566414847\n",
      "Grad norm = 0.005425099784892836\n",
      "Grad norm = 0.00538570761668871\n",
      "Grad norm = 0.005357032152944818\n",
      "Grad norm = 0.0053432642803629546\n",
      "Grad norm = 0.00533599982729944\n",
      "Grad norm = 0.005297819280115358\n",
      "Grad norm = 0.005289379044993962\n",
      "Grad norm = 0.005263039013723378\n",
      "Grad norm = 0.005238257104900931\n",
      "Grad norm = 0.005240510692966198\n",
      "Grad norm = 0.005202235725941835\n",
      "Grad norm = 0.005199273142985143\n",
      "Grad norm = 0.0051654288699073654\n",
      "Grad norm = 0.005168859595578305\n",
      "Grad norm = 0.005132756457638109\n",
      "Grad norm = 0.0051249468178585144\n",
      "Grad norm = 0.005099957407671421\n",
      "Grad norm = 0.005071973787727472\n",
      "Grad norm = 0.00505338227784344\n",
      "Grad norm = 0.005030219581482604\n",
      "Grad norm = 0.00501661407285021\n",
      "Grad norm = 0.005001410477946623\n",
      "Grad norm = 0.004989518904734937\n",
      "Grad norm = 0.0049696569377949956\n",
      "Grad norm = 0.004946956811883153\n",
      "Grad norm = 0.004921882873971743\n",
      "Grad norm = 0.004901040842423873\n",
      "Grad norm = 0.004888813744988105\n",
      "Grad norm = 0.004880061050266638\n",
      "Grad norm = 0.0048521967232514635\n",
      "Grad norm = 0.004837425158285687\n",
      "Grad norm = 0.004811456058066091\n",
      "Grad norm = 0.00479311287765764\n",
      "Grad norm = 0.004777640948068706\n",
      "Grad norm = 0.004769337626856038\n",
      "Grad norm = 0.0047621453895481105\n",
      "Grad norm = 0.004730692842002876\n",
      "Grad norm = 0.004711679704173151\n",
      "Grad norm = 0.0047081882809570375\n",
      "Grad norm = 0.004681379189422173\n",
      "Grad norm = 0.0046787308171308516\n",
      "Grad norm = 0.004649257634381209\n",
      "Grad norm = 0.0046467840063686905\n",
      "Grad norm = 0.00461860573206495\n",
      "Grad norm = 0.004595558012290077\n",
      "Grad norm = 0.004582110754604996\n",
      "Grad norm = 0.004577185091940387\n",
      "Grad norm = 0.004547806210700185\n",
      "Grad norm = 0.004525849431049282\n",
      "Grad norm = 0.00452235417357836\n",
      "Grad norm = 0.004503147411691022\n",
      "Grad norm = 0.00449044723528014\n",
      "Grad norm = 0.004467675856870418\n",
      "Grad norm = 0.004457573995677873\n",
      "Grad norm = 0.0044555354907236545\n",
      "Grad norm = 0.0044262952144990654\n",
      "Grad norm = 0.004423398898308656\n",
      "Grad norm = 0.004399476917801169\n",
      "Grad norm = 0.004387673373688324\n",
      "Grad norm = 0.004388938449673903\n",
      "Grad norm = 0.004357127059758316\n",
      "Grad norm = 0.004337644355162972\n",
      "Grad norm = 0.00432809099556855\n",
      "Grad norm = 0.004302262934940911\n",
      "Grad norm = 0.0042926999390067045\n",
      "Grad norm = 0.004283782598987267\n",
      "Grad norm = 0.004249061485420446\n",
      "Grad norm = 0.004237928102955809\n",
      "Grad norm = 0.004226599676888765\n",
      "Grad norm = 0.004213522115335783\n",
      "Grad norm = 0.004206853313194641\n",
      "Grad norm = 0.004191763546057515\n",
      "Grad norm = 0.004164401157795289\n",
      "Grad norm = 0.0041530255328615\n",
      "Grad norm = 0.004142966781923164\n",
      "Grad norm = 0.00412090829939355\n",
      "Grad norm = 0.00412388390358351\n",
      "Grad norm = 0.004093055200858122\n",
      "Grad norm = 0.004076658385006874\n",
      "Grad norm = 0.004070299795917592\n",
      "Grad norm = 0.0040518604796861285\n",
      "Grad norm = 0.004049057268604076\n",
      "Grad norm = 0.004042450899602316\n",
      "Grad norm = 0.004013066736143177\n",
      "Grad norm = 0.003995823577304764\n",
      "Grad norm = 0.003989439806248802\n",
      "Grad norm = 0.003967960540384705\n",
      "Grad norm = 0.0039573450603137975\n",
      "Grad norm = 0.003951687049352003\n",
      "Grad norm = 0.003928162492921665\n",
      "Grad norm = 0.003912314401506511\n",
      "Grad norm = 0.003894176575466542\n",
      "Grad norm = 0.0038772840312842863\n",
      "Grad norm = 0.0038728190399552756\n",
      "Grad norm = 0.003870242559406602\n",
      "Grad norm = 0.0038450020366193054\n",
      "Grad norm = 0.003831994118743862\n",
      "Grad norm = 0.0038149286248547083\n",
      "Grad norm = 0.003809362240735565\n",
      "Grad norm = 0.0037877352735602417\n",
      "Grad norm = 0.0037776315289482975\n",
      "Grad norm = 0.003766950997461634\n",
      "Grad norm = 0.0037482740658526395\n",
      "Grad norm = 0.0037366317082389664\n",
      "Grad norm = 0.0037238501598973995\n",
      "Grad norm = 0.0037187564499075437\n",
      "Grad norm = 0.00369881506330683\n",
      "Grad norm = 0.0036882827346641357\n",
      "Grad norm = 0.003682783590107814\n",
      "Grad norm = 0.0036681452125802375\n",
      "Grad norm = 0.003658678216309863\n",
      "Grad norm = 0.0036357594446983356\n",
      "Grad norm = 0.0036348309342681007\n",
      "Grad norm = 0.0036139711177818347\n",
      "Grad norm = 0.0036060914357983827\n",
      "Grad norm = 0.003592314524541371\n",
      "Grad norm = 0.003574879355833682\n",
      "Grad norm = 0.003571617483351694\n",
      "Grad norm = 0.0035573008972830857\n",
      "Grad norm = 0.0035406074616833303\n",
      "Grad norm = 0.003539086252992823\n",
      "Grad norm = 0.0035210214198348384\n",
      "Grad norm = 0.0035057694303739224\n",
      "Grad norm = 0.0034927908841864484\n",
      "Grad norm = 0.003495631002424604\n",
      "Grad norm = 0.003476244069731503\n",
      "Grad norm = 0.0034572443963767456\n",
      "Grad norm = 0.0034416379749382447\n",
      "Grad norm = 0.003440692770366137\n",
      "Grad norm = 0.003426061207196218\n",
      "Grad norm = 0.0034080719414724373\n",
      "Grad norm = 0.0033990782164322647\n",
      "Grad norm = 0.003391813966222429\n",
      "Grad norm = 0.0033748553715050264\n",
      "Grad norm = 0.0033644505335813047\n",
      "Grad norm = 0.003352713317326928\n",
      "Grad norm = 0.003345357720519325\n",
      "Grad norm = 0.0033406338683965473\n",
      "Grad norm = 0.0033172385724416463\n",
      "Grad norm = 0.003311471112750587\n",
      "Grad norm = 0.003298282861726813\n",
      "Grad norm = 0.0032953871378860584\n",
      "Grad norm = 0.003273415757541887\n",
      "Grad norm = 0.0032654389317154243\n",
      "Grad norm = 0.003251603856859359\n",
      "Grad norm = 0.0032412402485895173\n",
      "Grad norm = 0.0032338225680826173\n",
      "Grad norm = 0.0032173043706075787\n",
      "Grad norm = 0.003204058458409828\n",
      "Grad norm = 0.003194645616604321\n",
      "Grad norm = 0.0031940508062840117\n",
      "Grad norm = 0.0031719438563893366\n",
      "Grad norm = 0.003165479337790748\n",
      "Grad norm = 0.0031555409519446386\n",
      "Grad norm = 0.003145262579757503\n",
      "Grad norm = 0.003146933341752931\n",
      "Grad norm = 0.00312420114960365\n",
      "Grad norm = 0.003113727349790362\n",
      "Grad norm = 0.003105556135714194\n",
      "Grad norm = 0.003096341428121364\n",
      "Grad norm = 0.0030973089363033586\n",
      "Grad norm = 0.003080949798956951\n",
      "Grad norm = 0.0030608029615606376\n",
      "Grad norm = 0.003056455135692737\n",
      "Grad norm = 0.0030452995784034874\n",
      "Grad norm = 0.003040034083297459\n",
      "Grad norm = 0.0030259212279909633\n",
      "Grad norm = 0.0030155524003461325\n",
      "Grad norm = 0.003002091415086337\n",
      "Grad norm = 0.0029923973185591933\n",
      "Grad norm = 0.002990536861750761\n",
      "Grad norm = 0.0029746983309830743\n",
      "Grad norm = 0.0029660596569656174\n",
      "Grad norm = 0.0029525050016388915\n",
      "Grad norm = 0.0029473632141966384\n",
      "Grad norm = 0.0029423258881018594\n",
      "Grad norm = 0.0029236347772333333\n",
      "Grad norm = 0.0029155233341775677\n",
      "Grad norm = 0.002914474055025251\n",
      "Grad norm = 0.002895406229065677\n",
      "Grad norm = 0.002889437505346836\n",
      "Grad norm = 0.002889585013315988\n",
      "Grad norm = 0.002869809346661651\n",
      "Grad norm = 0.002865346836339907\n",
      "Grad norm = 0.0028502832902572283\n",
      "Grad norm = 0.002845417552490253\n",
      "Grad norm = 0.0028315276988053026\n",
      "Grad norm = 0.0028223474723204837\n",
      "Grad norm = 0.0028148708593907943\n",
      "Grad norm = 0.002809855272360658\n",
      "Grad norm = 0.0028008046486111228\n",
      "Grad norm = 0.002783240074300206\n",
      "Grad norm = 0.00277603173988131\n",
      "Grad norm = 0.002770550885040116\n",
      "Grad norm = 0.002756295248330296\n",
      "Grad norm = 0.0027487264913420516\n",
      "Grad norm = 0.0027427147688510703\n",
      "Grad norm = 0.0027350888186808337\n",
      "Grad norm = 0.0027245680547303814\n",
      "Grad norm = 0.0027130502379913244\n",
      "Grad norm = 0.002704246485242894\n",
      "Grad norm = 0.0026978418134838895\n",
      "Grad norm = 0.0026875771575806556\n",
      "Grad norm = 0.0026791766057835563\n",
      "Grad norm = 0.002670928819307424\n",
      "Grad norm = 0.0026613037444941914\n",
      "Grad norm = 0.0026605146465785005\n",
      "Grad norm = 0.002647270228482182\n",
      "Grad norm = 0.0026391633994321223\n",
      "Grad norm = 0.002626902001933693\n",
      "Grad norm = 0.0026265899671428176\n",
      "Grad norm = 0.0026151092341472936\n",
      "Grad norm = 0.002602548699622704\n",
      "Grad norm = 0.002596589657061657\n",
      "Grad norm = 0.0025899611800931535\n",
      "Grad norm = 0.002579441877547758\n",
      "Grad norm = 0.0025733335443740134\n",
      "Grad norm = 0.002561615489611484\n",
      "Grad norm = 0.0025591732197671755\n",
      "Grad norm = 0.002555239744735284\n",
      "Grad norm = 0.0025374124780434336\n",
      "Grad norm = 0.0025322090848801044\n",
      "Grad norm = 0.0025210331274939644\n",
      "Grad norm = 0.002517942406903306\n",
      "Grad norm = 0.0025168916665202932\n",
      "Grad norm = 0.0025020056165548686\n",
      "Grad norm = 0.00250079724404465\n",
      "Grad norm = 0.002483612274261039\n",
      "Grad norm = 0.0024808136827339324\n",
      "Grad norm = 0.0024744188601333863\n",
      "Grad norm = 0.002460866256379578\n",
      "Grad norm = 0.00245286168798602\n",
      "Grad norm = 0.002447490379453436\n",
      "Grad norm = 0.0024448802385466425\n",
      "Grad norm = 0.002429130767117258\n",
      "Grad norm = 0.0024238195148770015\n",
      "Grad norm = 0.0024162638144127574\n",
      "Grad norm = 0.0024106731558064167\n",
      "Grad norm = 0.0024085633065244138\n",
      "Grad norm = 0.0023967796956558957\n",
      "Grad norm = 0.0023909139370151865\n",
      "Grad norm = 0.0023780887236496337\n",
      "Grad norm = 0.0023721349423362556\n",
      "Grad norm = 0.0023708707593334795\n",
      "Grad norm = 0.0023564009414035842\n",
      "Grad norm = 0.0023506724723300564\n",
      "Grad norm = 0.0023426621908479934\n",
      "Grad norm = 0.0023420555606928875\n",
      "Grad norm = 0.002325158708737067\n",
      "Grad norm = 0.002318643204992974\n",
      "Grad norm = 0.0023144578027257893\n",
      "Grad norm = 0.0023064543900573648\n",
      "Grad norm = 0.0023004739940282867\n",
      "Grad norm = 0.0022923654986536177\n",
      "Grad norm = 0.0022878839818075702\n",
      "Grad norm = 0.002276631735578272\n",
      "Grad norm = 0.002272150432672428\n",
      "Grad norm = 0.0022662136532247044\n",
      "Grad norm = 0.002262078005261262\n",
      "Grad norm = 0.0022501862266486325\n",
      "Grad norm = 0.002248222917984267\n",
      "Grad norm = 0.0022382644699905987\n",
      "Grad norm = 0.0022350134284204845\n",
      "Grad norm = 0.0022317324710061635\n",
      "Grad norm = 0.0022182397095198217\n",
      "Grad norm = 0.0022176469578563023\n",
      "Grad norm = 0.002208992098273146\n",
      "Grad norm = 0.002196568401298669\n",
      "Grad norm = 0.0021976213668664455\n",
      "Grad norm = 0.0021960918568407063\n",
      "Grad norm = 0.002178105102038974\n",
      "Grad norm = 0.0021729219735495704\n",
      "Grad norm = 0.002165364923727936\n",
      "Grad norm = 0.0021622771784390606\n",
      "Grad norm = 0.002155961390843058\n",
      "Grad norm = 0.002144248351934582\n",
      "Grad norm = 0.002142279419317642\n",
      "Grad norm = 0.002133106703526499\n",
      "Grad norm = 0.002129679176461078\n",
      "Grad norm = 0.0021278748826253457\n",
      "Grad norm = 0.002114945120162429\n",
      "Grad norm = 0.002110645056255329\n",
      "Grad norm = 0.0021102707879944124\n",
      "Grad norm = 0.0020963646695228546\n",
      "Grad norm = 0.0020907687315998274\n",
      "Grad norm = 0.002093712812589669\n",
      "Grad norm = 0.0020943941364308756\n",
      "Grad norm = 0.0020744196200544263\n",
      "Grad norm = 0.0020659787922383045\n",
      "Grad norm = 0.0020684357712579195\n",
      "Grad norm = 0.002054563064244389\n",
      "Grad norm = 0.0020479136356023864\n",
      "Grad norm = 0.002045773517663604\n",
      "Grad norm = 0.0020450748970183377\n",
      "Grad norm = 0.0020293873583605445\n",
      "Grad norm = 0.0020234292431207377\n",
      "Grad norm = 0.0020239602257792364\n",
      "Grad norm = 0.002009848445020031\n",
      "Grad norm = 0.0020068722662209613\n",
      "Grad norm = 0.0020075249026249827\n",
      "Grad norm = 0.0019965185518750245\n",
      "Grad norm = 0.001987281410202885\n",
      "Grad norm = 0.001988954328188502\n",
      "Grad norm = 0.0019749840445778665\n",
      "Grad norm = 0.0019706441651193927\n",
      "Grad norm = 0.0019710016148681442\n",
      "Grad norm = 0.0019584702378299956\n",
      "Grad norm = 0.0019558890050698587\n",
      "Grad norm = 0.0019478081499608333\n",
      "Grad norm = 0.0019422312129451765\n",
      "Grad norm = 0.001942904619663751\n",
      "Grad norm = 0.0019308935642285987\n",
      "Grad norm = 0.0019257831141578914\n",
      "Grad norm = 0.001917634787715372\n",
      "Grad norm = 0.0019151203122773613\n",
      "Grad norm = 0.0019130684008569331\n",
      "Grad norm = 0.0019023134551741715\n",
      "Grad norm = 0.0018995559408263574\n",
      "Grad norm = 0.0018909674825186213\n",
      "Grad norm = 0.0018863628676453823\n",
      "Grad norm = 0.0018855900673019964\n",
      "Grad norm = 0.0018742311227172117\n",
      "Grad norm = 0.0018721969420420995\n",
      "Grad norm = 0.0018624750648806414\n",
      "Grad norm = 0.0018607634784382\n",
      "Grad norm = 0.0018533156992137795\n",
      "Grad norm = 0.0018484479944086165\n",
      "Grad norm = 0.0018426561162473494\n",
      "Grad norm = 0.0018401987355782053\n",
      "Grad norm = 0.0018309977993890068\n",
      "Grad norm = 0.001829483529687455\n",
      "Grad norm = 0.0018212105480868402\n",
      "Grad norm = 0.001817053858620555\n",
      "Grad norm = 0.0018193752169087547\n",
      "Grad norm = 0.0018063394718530169\n",
      "Grad norm = 0.0018025094599550518\n",
      "Grad norm = 0.0017956764592979688\n",
      "Grad norm = 0.0017905747977626696\n",
      "Grad norm = 0.0017916084424699215\n",
      "Grad norm = 0.0017795404720397482\n",
      "Grad norm = 0.0017794198249325847\n",
      "Grad norm = 0.001769988396235574\n",
      "Grad norm = 0.0017665319670754557\n",
      "Grad norm = 0.0017638868818522847\n",
      "Grad norm = 0.0017560217790400466\n",
      "Grad norm = 0.0017535873221338186\n",
      "Grad norm = 0.0017456320442378046\n",
      "Grad norm = 0.0017420550401584097\n",
      "Grad norm = 0.0017434018700867716\n",
      "Grad norm = 0.0017322711052930955\n",
      "Grad norm = 0.0017300210053516844\n",
      "Grad norm = 0.0017212525021484432\n",
      "Grad norm = 0.0017203383159735406\n",
      "Grad norm = 0.0017133326510913433\n",
      "Grad norm = 0.0017081742569555495\n",
      "Grad norm = 0.0017013119927442434\n",
      "Grad norm = 0.001700313585949557\n",
      "Grad norm = 0.0016941170423119014\n",
      "Grad norm = 0.0016894809714411865\n",
      "Grad norm = 0.001682812172426303\n",
      "Grad norm = 0.001682328152346086\n",
      "Grad norm = 0.0016748569798203435\n",
      "Grad norm = 0.0016684446029744085\n",
      "Grad norm = 0.001663943705878429\n",
      "Grad norm = 0.001661571732403836\n",
      "Grad norm = 0.001654461298440026\n",
      "Grad norm = 0.0016515279414733134\n",
      "Grad norm = 0.0016466711658263908\n",
      "Grad norm = 0.0016417026165092996\n",
      "Grad norm = 0.0016380749752242182\n",
      "Grad norm = 0.0016349814336297673\n",
      "Grad norm = 0.001630542106947039\n",
      "Grad norm = 0.0016246115693345615\n",
      "Grad norm = 0.001623629610078488\n",
      "Grad norm = 0.0016163549958176642\n",
      "Grad norm = 0.0016151397900420184\n",
      "Grad norm = 0.0016068800976418419\n",
      "Grad norm = 0.0016047933090758946\n",
      "Grad norm = 0.0015974461908804794\n",
      "Grad norm = 0.001596890775548979\n",
      "Grad norm = 0.0015912349157429246\n",
      "Grad norm = 0.0015870249068794602\n",
      "Grad norm = 0.0015808410182926737\n",
      "Grad norm = 0.0015796953342780047\n",
      "Grad norm = 0.0015748954681432388\n",
      "Grad norm = 0.0015689153927831172\n",
      "Grad norm = 0.0015673251306888047\n",
      "Grad norm = 0.0015591995250672452\n",
      "Grad norm = 0.0015580527599592504\n",
      "Grad norm = 0.0015572173259216701\n",
      "Grad norm = 0.0015465447703354497\n",
      "Grad norm = 0.001545513711356767\n",
      "Grad norm = 0.0015390885925596705\n",
      "Grad norm = 0.0015355602874087837\n",
      "Grad norm = 0.0015341929206687864\n",
      "Grad norm = 0.0015285193042953856\n",
      "Grad norm = 0.0015248893446806247\n",
      "Grad norm = 0.0015185638807793235\n",
      "Grad norm = 0.0015154008187516384\n",
      "Grad norm = 0.0015162122518336872\n",
      "Grad norm = 0.0015061607546878544\n",
      "Grad norm = 0.001505757649777201\n",
      "Grad norm = 0.0014985743457220672\n",
      "Grad norm = 0.0014960059377528117\n",
      "Grad norm = 0.0014909753767200415\n",
      "Grad norm = 0.0014875522449720325\n",
      "Grad norm = 0.0014829866526090889\n",
      "Grad norm = 0.0014818732051812777\n",
      "Grad norm = 0.0014743792111855086\n",
      "Grad norm = 0.0014715570720980758\n",
      "Grad norm = 0.0014671052405123698\n",
      "Grad norm = 0.0014664054858901797\n",
      "Grad norm = 0.0014586603620257395\n",
      "Grad norm = 0.0014555404626520301\n",
      "Grad norm = 0.0014524479241069645\n",
      "Grad norm = 0.0014475470074002002\n",
      "Grad norm = 0.001443738794382288\n",
      "Grad norm = 0.0014405229797110049\n",
      "Grad norm = 0.001436394220973727\n",
      "Grad norm = 0.001432943936561906\n",
      "Grad norm = 0.0014291776536400483\n",
      "Grad norm = 0.0014257110841189617\n",
      "Grad norm = 0.001420383873397828\n",
      "Grad norm = 0.0014190035961061014\n",
      "Grad norm = 0.0014135322787842393\n",
      "Grad norm = 0.0014127699426300828\n",
      "Grad norm = 0.001405843461143358\n",
      "Grad norm = 0.0014053261308589803\n",
      "Grad norm = 0.0013986485440049636\n",
      "Grad norm = 0.0013975804123600626\n",
      "Grad norm = 0.0013936532457354972\n",
      "Grad norm = 0.0013883369124702378\n",
      "Grad norm = 0.0013875244316562728\n",
      "Grad norm = 0.0013798474461024551\n",
      "Grad norm = 0.001377569971985797\n",
      "Grad norm = 0.0013788001924029789\n",
      "Grad norm = 0.0013697232037324554\n",
      "Grad norm = 0.0013686421929845652\n",
      "Grad norm = 0.0013668291912754085\n",
      "Grad norm = 0.001358505249319015\n",
      "Grad norm = 0.0013572614752964435\n",
      "Grad norm = 0.0013594552940383454\n",
      "Grad norm = 0.001349804946994289\n",
      "Grad norm = 0.0013452391025809767\n",
      "Grad norm = 0.0013462844063989055\n",
      "Grad norm = 0.0013393359986666607\n",
      "Grad norm = 0.0013356298112615073\n",
      "Grad norm = 0.0013356414219219887\n",
      "Grad norm = 0.0013341094663868436\n",
      "Grad norm = 0.0013238679800298041\n",
      "Grad norm = 0.0013239281812679967\n",
      "Grad norm = 0.0013243120168655244\n",
      "Grad norm = 0.0013138976432302857\n",
      "Grad norm = 0.0013117580525697805\n",
      "Grad norm = 0.0013141055627713576\n",
      "Grad norm = 0.0013058604434195565\n",
      "Grad norm = 0.0013010819223870749\n",
      "Grad norm = 0.0013020160185175985\n",
      "Grad norm = 0.0012936840034732946\n",
      "Grad norm = 0.001291895758897972\n",
      "Grad norm = 0.0012930005992645944\n",
      "Grad norm = 0.0012840105010457873\n",
      "Grad norm = 0.0012837994285183325\n",
      "Grad norm = 0.0012780379941076142\n",
      "Grad norm = 0.0012740551069307141\n",
      "Grad norm = 0.0012720390366142385\n",
      "Grad norm = 0.0012701290362950638\n",
      "Grad norm = 0.0012653998689290328\n",
      "Grad norm = 0.0012639486932683093\n",
      "Grad norm = 0.0012580190607502312\n",
      "Grad norm = 0.0012577031190745005\n",
      "Grad norm = 0.0012536751767309132\n",
      "Grad norm = 0.0012491750430017607\n",
      "Grad norm = 0.0012480275639473596\n",
      "Grad norm = 0.0012432229984011478\n",
      "Grad norm = 0.0012421598703357864\n",
      "Grad norm = 0.0012367950862293001\n",
      "Grad norm = 0.0012333011798498963\n",
      "Grad norm = 0.0012309374383015113\n",
      "Grad norm = 0.0012281217518950484\n",
      "Grad norm = 0.0012257420923932844\n",
      "Grad norm = 0.001221733198391576\n",
      "Grad norm = 0.0012208353740225408\n",
      "Grad norm = 0.0012155057651988585\n",
      "Grad norm = 0.0012138755340301733\n",
      "Grad norm = 0.0012157417499711694\n",
      "Grad norm = 0.0012061920768491185\n",
      "Grad norm = 0.001205490137323057\n",
      "Grad norm = 0.0012078912148656932\n",
      "Grad norm = 0.0011982141809887856\n",
      "Grad norm = 0.001194438197992924\n",
      "Grad norm = 0.0011956826465758955\n",
      "Grad norm = 0.001193680561242198\n",
      "Grad norm = 0.0011852544277287272\n",
      "Grad norm = 0.0011835096527690922\n",
      "Grad norm = 0.001179299246481099\n",
      "Grad norm = 0.001176321361392194\n",
      "Grad norm = 0.0011741982627046788\n",
      "Grad norm = 0.0011723348529613876\n",
      "Grad norm = 0.001167489825455497\n",
      "Grad norm = 0.0011658967687978303\n",
      "Grad norm = 0.0011621940434006527\n",
      "Grad norm = 0.0011590390753432227\n",
      "Grad norm = 0.0011578227792244599\n",
      "Grad norm = 0.0011534310527484487\n",
      "Grad norm = 0.0011512207276370085\n",
      "Grad norm = 0.0011494901672891291\n",
      "Grad norm = 0.001147280613375837\n",
      "Grad norm = 0.0011417725370001762\n",
      "Grad norm = 0.0011420819530436432\n",
      "Grad norm = 0.0011372054804764057\n",
      "Grad norm = 0.0011367235246310474\n",
      "Grad norm = 0.0011315776592912845\n",
      "Grad norm = 0.00112994739005772\n",
      "Grad norm = 0.0011261937890048267\n",
      "Grad norm = 0.0011235347357505534\n",
      "Grad norm = 0.001120468814312863\n",
      "Grad norm = 0.0011187094537927129\n",
      "Grad norm = 0.0011169798304400383\n",
      "Grad norm = 0.0011123126851464624\n",
      "Grad norm = 0.0011117661274725901\n",
      "Grad norm = 0.0011077047201417543\n",
      "Grad norm = 0.0011075397578923243\n",
      "Grad norm = 0.0011052902792831958\n",
      "Grad norm = 0.0010988869266120326\n",
      "Grad norm = 0.0010998443897855608\n",
      "Grad norm = 0.0010944996364197265\n",
      "Grad norm = 0.0010936050414115217\n",
      "Grad norm = 0.0010894592322944138\n",
      "Grad norm = 0.0010867722114687554\n",
      "Grad norm = 0.0010863596695786426\n",
      "Grad norm = 0.0010822789129141957\n",
      "Grad norm = 0.0010809594169966948\n",
      "Grad norm = 0.0010829941125094806\n",
      "Grad norm = 0.0010750685142381343\n",
      "Grad norm = 0.0010726235742505774\n",
      "Grad norm = 0.0010748596281119364\n",
      "Grad norm = 0.0010674382186591771\n",
      "Grad norm = 0.0010649376145395086\n",
      "Grad norm = 0.0010650025508128396\n",
      "Grad norm = 0.0010599248133099808\n",
      "Grad norm = 0.0010573908251064992\n",
      "Grad norm = 0.0010559569275160623\n",
      "Grad norm = 0.0010529909804589466\n",
      "Grad norm = 0.001049852127643901\n",
      "Grad norm = 0.001047578716884652\n",
      "Grad norm = 0.0010442962854842115\n",
      "Grad norm = 0.0010424944564333411\n",
      "Grad norm = 0.0010399458845843192\n",
      "Grad norm = 0.0010389458563138754\n",
      "Grad norm = 0.0010354079905446536\n",
      "Grad norm = 0.0010343621846875964\n",
      "Grad norm = 0.0010302152647807517\n",
      "Grad norm = 0.0010305739022314821\n",
      "Grad norm = 0.0010254975999711294\n",
      "Grad norm = 0.001025609412860868\n",
      "Grad norm = 0.0010213319571112304\n",
      "Grad norm = 0.0010207791979931348\n",
      "Grad norm = 0.0010162945022602033\n",
      "Grad norm = 0.0010170628735529596\n",
      "Grad norm = 0.001012010676811149\n",
      "Grad norm = 0.001010769228480017\n",
      "Grad norm = 0.001011707096877685\n",
      "Grad norm = 0.0010054378507470968\n",
      "Grad norm = 0.0010047046170687077\n",
      "Grad norm = 0.0010014281622651435\n",
      "Grad norm = 0.001000043078862033\n",
      "Grad norm = 0.0009977064554504212\n",
      "Grad norm = 0.0009950876060545108\n",
      "Grad norm = 0.0009919446928386398\n",
      "Grad norm = 0.0009915940626685515\n",
      "Grad norm = 0.0009889260104328607\n",
      "Grad norm = 0.0009848535847531536\n",
      "Grad norm = 0.0009854116545263334\n",
      "Grad norm = 0.000980835534137365\n",
      "Grad norm = 0.0009789478942646393\n",
      "Grad norm = 0.0009780764953673934\n"
     ]
    },
    {
     "data": {
      "text/plain": "('Max iterations reached', 1000)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_c = 3\n",
    "network = make_network(input_size, 32, output_size, layer_c, ReLU)\n",
    "initialize_network(network, \"Kaiming\")\n",
    "weights = get_weights(network)\n",
    "\n",
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=False)\n",
    "\n",
    "res = minimize(\n",
    "    compute_loss_grad, weights,       # fun and start point\n",
    "    args=(network, X_train, y_train), # args passed to fun\n",
    "    method=Adam,                       # optimization method\n",
    "    jac=True,                         # says that gradient is computed in fun,\n",
    "    options={\n",
    "        'n_iter': 1_000,\n",
    "        'beta_momentum': 0.9,\n",
    "        'beta_grad': 0.99,\n",
    "        'lr': 12 * 1e-5,\n",
    "        'batch_size': X_train.shape[0],\n",
    "    },\n",
    "    tol=1e-8,\n",
    "    callback=cb.call,\n",
    ")\n",
    "\n",
    "res['message'], res['nit']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:10.499354200Z",
     "start_time": "2024-03-02T11:30:54.013971400Z"
    }
   },
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(cb.train_acc, label=f\"train acc [{cb.train_acc[-1]:5.4f}]\")\n",
    "ax.plot(cb.test_acc, label=f\"test acc{cb.test_acc[-1]:5.4f}\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:13.166763Z",
     "start_time": "2024-03-02T11:31:13.050192900Z"
    }
   },
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9711111111111111"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.test_acc[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:13.930369600Z",
     "start_time": "2024-03-02T11:31:13.870428300Z"
    }
   },
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### `Dropout (1 балл)`\n",
    "\n",
    "Реализуйте слой Dropout. Сравните обучение сети из большого числа слоёв при использовании Dropout и без его использования (предварительно подберите адекватный параметр $p$). Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, input: NDArray) -> NDArray:\n",
    "        if self.inference_mode:\n",
    "            return input\n",
    "        \n",
    "        mask = np.random.binomial(n=1, p=1 - self.p, size=input.shape)\n",
    "        # print(mask)\n",
    "        self.mask = mask / (1 - self.p)\n",
    "        return input * self.mask\n",
    "    \n",
    "    def backward(self, grad_output: NDArray) -> Tuple[NDArray, NDArray]:\n",
    "        if self.inference_mode:\n",
    "            return grad_output, []\n",
    "        \n",
    "        return grad_output * self.mask, []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Dropout(p={self.p}, enabled={not self.inference_mode})\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:15.152133700Z",
     "start_time": "2024-03-02T11:31:15.108620300Z"
    }
   },
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_network(input_size, hidden_layers_size, output_size, n_layers=3, activation_class=ReLU, dropout: bool = False, dropout_p: float = 0.5):\n",
    "    network = []\n",
    "\n",
    "    for layer_idx in range(n_layers):\n",
    "        # Compute sizes of current linear layer\n",
    "        layer_in = input_size if layer_idx == 0 else hidden_layers_size\n",
    "        layer_out = output_size if layer_idx == n_layers - 1 else hidden_layers_size\n",
    "        \n",
    "        # Add linear layer to the network\n",
    "        network.append(Dense(layer_in, layer_out))\n",
    "\n",
    "        # Add activation after each layer except the last one\n",
    "        if layer_idx != n_layers - 1:\n",
    "            network.append(activation_class())\n",
    "            if dropout:\n",
    "                network.append(Dropout(dropout_p))\n",
    "\n",
    "    # Add LogSoftmax layer to the network\n",
    "    network.append(LogSoftmax())\n",
    "\n",
    "    return network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:15.816973800Z",
     "start_time": "2024-03-02T11:31:15.773964800Z"
    }
   },
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def set_inference_mode(network: List[Layer], inference_mode: bool):\n",
    "    for layer in network:\n",
    "        layer.inference_mode = inference_mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:16.396693200Z",
     "start_time": "2024-03-02T11:31:16.371174800Z"
    }
   },
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Dense(64, 32),\n Relu(),\n Dropout(p=0.2, enabled=True),\n Dense(32, 32),\n Relu(),\n Dropout(p=0.2, enabled=True),\n Dense(32, 10),\n LogSoftmax()]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = make_network(input_size, 32, output_size, layer_c, ReLU, dropout=True, dropout_p=0.2)\n",
    "set_inference_mode(network, False)\n",
    "weights = get_weights(network)\n",
    "network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:17.296105Z",
     "start_time": "2024-03-02T11:31:17.279113Z"
    }
   },
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(500, True, 'Max iterations reached')"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    compute_loss_grad, weights,       # fun and start point\n",
    "    args=[network, X_train, y_train], # args passed to fun\n",
    "    method=SGD,                       # optimization method\n",
    "    jac=True,                         # says that gradient is computed in fun,\n",
    "    options={'disp': True, 'momentum': 0.9, 'batch_size': 64, 'n_iter': 5_00, 'lr': 0.005},\n",
    "    tol=1e-7\n",
    ")\n",
    "\n",
    "res['nit'], res['success'], res['message']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:31.718610700Z",
     "start_time": "2024-03-02T11:31:20.570627100Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train NLL: 0.0197534\t\tTest NLL: 0.3305767\n",
      "[Dense(64, 32), Relu(), Dropout(p=0.2, enabled=False), Dense(32, 32), Relu(), Dropout(p=0.2, enabled=False), Dense(32, 10), LogSoftmax()]\n",
      "Train accuracy: 1.000\t\tTest accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "set_weights(weights=res[\"x\"], network=network)\n",
    "\n",
    "train_NLL = NLL(forward(network, X_train), y_train)\n",
    "test_NLL = NLL(forward(network, X_test), y_test)\n",
    "\n",
    "print(f\"Train NLL: {train_NLL:.7f}\\t\\tTest NLL: {test_NLL:.7f}\")\n",
    "\n",
    "set_inference_mode(network, True)\n",
    "print(network)\n",
    "\n",
    "train_accuracy = accuracy_score(y_true=y_train, y_pred=predict(network, X_train))\n",
    "test_accuracy = accuracy_score(y_true=y_test, y_pred=predict(network, X_test))\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}\\t\\tTest accuracy: {test_accuracy:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:31:31.743611400Z",
     "start_time": "2024-03-02T11:31:31.708315600Z"
    }
   },
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\tj= 0\tdropout_p=0.00\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9689\n",
      "i= 0\tj= 1\tdropout_p=0.00\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9689\n",
      "i= 0\tj= 2\tdropout_p=0.00\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9644\n",
      "i= 0\tj= 3\tdropout_p=0.00\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9689\n",
      "i= 0\tj= 4\tdropout_p=0.00\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9600\n",
      "i= 1\tj= 0\tdropout_p=0.10\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9733\n",
      "i= 1\tj= 1\tdropout_p=0.10\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9711\n",
      "i= 1\tj= 2\tdropout_p=0.10\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9733\n",
      "i= 1\tj= 3\tdropout_p=0.10\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9711\n",
      "i= 1\tj= 4\tdropout_p=0.10\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9756\n",
      "i= 2\tj= 0\tdropout_p=0.20\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9556\n",
      "i= 2\tj= 1\tdropout_p=0.20\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9733\n",
      "i= 2\tj= 2\tdropout_p=0.20\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9733\n",
      "i= 2\tj= 3\tdropout_p=0.20\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9733\n",
      "i= 2\tj= 4\tdropout_p=0.20\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9667\n",
      "i= 3\tj= 0\tdropout_p=0.30\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9644\n",
      "i= 3\tj= 1\tdropout_p=0.30\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9689\n",
      "i= 3\tj= 2\tdropout_p=0.30\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9711\n",
      "i= 3\tj= 3\tdropout_p=0.30\tn_iter=500\ttrain acc: 0.9993\ttest acc: 0.9667\n",
      "i= 3\tj= 4\tdropout_p=0.30\tn_iter=500\ttrain acc: 1.0000\ttest acc: 0.9756\n",
      "i= 4\tj= 0\tdropout_p=0.40\tn_iter=500\ttrain acc: 0.9993\ttest acc: 0.9756\n",
      "i= 4\tj= 1\tdropout_p=0.40\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_vars = np.linspace(0, 0.9, num=10)\n",
    "layer_c = 3\n",
    "\n",
    "accs_train = np.zeros((p_vars.shape[0], 5))\n",
    "accs_test = np.zeros_like(accs_train)\n",
    "\n",
    "for i, dropout_p in enumerate(p_vars):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{dropout_p=:3.2f}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU, dropout=True, dropout_p=dropout_p)\n",
    "        initialize_network(network, 'Kaiming')\n",
    "        set_inference_mode(network, False)\n",
    "        network, cb = train_network(network, history=True, tol=1e-7, optimizer=SGD, options={'momentum': 0.9, 'batch_size': 32, 'n_iter': 500, 'lr': 0.1 * 1e-2})\n",
    "        set_inference_mode(network, True)\n",
    "        \n",
    "        accs_train[i, j] = accuracy_score(y_true=y_train, y_pred=predict(network, X_train))\n",
    "        accs_test[i, j] = accuracy_score(y_true=y_test, y_pred=predict(network, X_test))\n",
    "        print(f\"n_iter={len(cb.test_acc)}\\ttrain acc: {accs_train[i, j]:5.4f}\\ttest acc: {accs_test[i, j]:5.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T18:44:31.819482Z",
     "start_time": "2024-02-29T18:39:31.099745400Z"
    }
   },
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "drop_proba_for_plot = [f\"P = {p:3.2f}\" for p in p_vars]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=drop_proba_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.903228300Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "drop_proba_for_plot = [f\"P = {p:3.2f}\" for p in p_vars]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test[:-3, :].T, labels=drop_proba_for_plot[:-3], showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.903228300Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "p_vars = np.linspace(0, 0.9, num=10)\n",
    "layer_c = 6\n",
    "\n",
    "accs_train = np.zeros((p_vars.shape[0], 15))\n",
    "accs_test = np.zeros_like(accs_train)\n",
    "\n",
    "for i, dropout_p in enumerate(p_vars):\n",
    "    for j in range(accs_train.shape[1]):\n",
    "        print(f\"{i=:2}\\t{j=:2}\\t{dropout_p=:3.2f}\\t\", end='')\n",
    "        network = make_network(input_size, 32, output_size, layer_c, ReLU, dropout=True, dropout_p=dropout_p)\n",
    "        initialize_network(network, 'Kaiming')\n",
    "        set_inference_mode(network, False)\n",
    "        network, cb = train_network(network, history=True, tol=1e-7, optimizer=SGD, options={'momentum': 0.9, 'batch_size': 100, 'n_iter': 1_000, 'lr': 0.2 * 1e-2})\n",
    "        set_inference_mode(network, True)\n",
    "        \n",
    "        accs_train[i, j] = accuracy_score(y_true=y_train, y_pred=predict(network, X_train))\n",
    "        accs_test[i, j] = accuracy_score(y_true=y_test, y_pred=predict(network, X_test))\n",
    "        print(f\"train acc: {accs_train[i, j]:5.4f} test acc: {accs_test[i, j]:5.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.904228600Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "drop_proba_for_plot = [f\"P = {p:3.2f}\" for p in p_vars]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.boxplot(accs_test.T, labels=drop_proba_for_plot, showfliers=False)\n",
    "\n",
    "ax.set_title(f\"Test quality in {accs_test.shape[1]} runs\")\n",
    "ax.set_xlabel(\"Initialization\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-29T18:35:59.905228500Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### `BatchNormalization (1 балл)`\n",
    "\n",
    "Реализуйте слой `BatchNormalization`. Сравните обучение сети из большого числа слоёв при использовании `BatchNormalization` и без его использования. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Ваше решение:__\n",
    "\n",
    "Перепишем формулу для $y_i$ в векторном виде:\n",
    "$$y_i = \\gamma \\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\delta $$\n",
    "Считаем, что $\\mathcal{L} = \\mathcal{L}(y(X, \\gamma, \\delta))$, тогда:\n",
    "1. $\\nabla_{\\gamma} {\\mathcal L} = (\\nabla_{y} {\\mathcal L})^T \\cdot \\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}$\n",
    "2. $\\nabla_{\\delta} {\\mathcal L} = (\\nabla_{y} {\\mathcal L})^T \\cdot (1)_{i=1}^{n} = \\sum_{i=1}^n (\\nabla_{y} {\\mathcal L})_i$\n",
    "3. $\\nabla_X{L} = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\varepsilon}}\\nabla_y{L}\\left(E - \\frac{\\mathbb{I}(\\mathbb{I})^T}{n} - \\frac{\\overline y \\overline y^T}{n} \\right)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BatchNormalization(Layer):\n",
    "    def __init__(self, rho: float = 0.75):\n",
    "        super().__init__()\n",
    "        self.mu_mean = 0\n",
    "        self.std_mean = 0\n",
    "        self.rho = rho\n",
    "        self.normed = None\n",
    "        self.eps = 1e-7\n",
    "        \n",
    "        self.gamma = np.array([1.0])\n",
    "        self.delta = np.array([0.0])\n",
    "        self.params = [self.gamma, self.delta]\n",
    "    \n",
    "    def forward(self, input: NDArray) -> NDArray:\n",
    "        # print(f\"gamma = {self.gamma}\\tdelta = {self.delta}\")\n",
    "        if self.inference_mode:\n",
    "            output = self.gamma * ((input - self.mu_mean) / self.std_mean) + self.delta\n",
    "            return output\n",
    "            \n",
    "        mu = np.mean(input, axis=0, keepdims=True)\n",
    "        std = np.std(input, axis=0, keepdims=True) + self.eps\n",
    "        \n",
    "        self.mu_mean = (1 - self.rho) * self.mu_mean + self.rho * mu\n",
    "        self.std_mean = (1 - self.rho) * self.std_mean + self.rho * std\n",
    "        \n",
    "        self.input = input\n",
    "        self.normed = (input - mu) / std\n",
    "        self.bathc_mu = mu\n",
    "        self.batch_std = std\n",
    "        output = self.gamma * self.normed + self.delta\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output: NDArray) -> Tuple[NDArray, NDArray]:\n",
    "        d_loss_d_gamma = np.sum(grad_output * self.normed, axis=0, keepdims=True)\n",
    "        d_loss_d_delta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        loss_dot_y = np.sum(grad_output * (self.gamma * self.normed + self.delta), axis=0, keepdims=True)\n",
    "        loss_dot_1 = d_loss_d_delta\n",
    "        \n",
    "        n = grad_output.shape[0]\n",
    "        # print(f\"{loss_dot_1.shape=}\\n{loss_dot_y.shape=}\\n{self.normed.shape=}\")\n",
    "        d_loss_d_x = (grad_output - (1 / n) * (loss_dot_1 + (self.gamma * self.normed + self.delta) * loss_dot_y))\n",
    "        d_loss_d_x = (self.gamma / self.batch_std) * d_loss_d_x\n",
    "        \n",
    "        # print(f\"{d_loss_d_x.shape=}\\t{d_loss_d_gamma.shape=}\\t{d_loss_d_delta.shape=}\")\n",
    "        \n",
    "        return d_loss_d_x, [np.sum(d_loss_d_gamma), np.sum(d_loss_d_delta)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BatchNormalization()\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:27:25.539792900Z",
     "start_time": "2024-03-02T12:27:25.438573400Z"
    }
   },
   "execution_count": 242
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batchnorm = BatchNormalization()\n",
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "batchnorm.inference_mode = False\n",
    "batchnorm.forward(points)\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(lambda x: np.sum((batchnorm.forward(x)) ** 4), points)\n",
    "res = batchnorm.forward(points)\n",
    "batchnorm_grads, params = batchnorm.backward(4 * (res ** 3))\n",
    "\n",
    "batchnorm_grads - numeric_grads\n",
    "\n",
    "assert np.allclose(numeric_grads, batchnorm_grads, rtol=1e-3, atol=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:26:04.114826300Z",
     "start_time": "2024-03-02T12:26:04.034105200Z"
    }
   },
   "execution_count": 232
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([8.6277737e-08])"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm = BatchNormalization()\n",
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "batchnorm.inference_mode = False\n",
    "batchnorm.forward(points)\n",
    "\n",
    "def foo(gamma):\n",
    "    old_gamma = batchnorm.gamma\n",
    "    batchnorm.gamma = gamma\n",
    "    val = np.sum((batchnorm.forward(points)) ** 4)\n",
    "    batchnorm.gamma = old_gamma\n",
    "    return val\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(foo, np.array([1.0]))\n",
    "res = batchnorm.forward(points)\n",
    "batchnorm_grads, params = batchnorm.backward(4 * (res ** 3))\n",
    "\n",
    "# batchnorm_grads - numeric_grads\n",
    "\n",
    "numeric_grads - params[0]\n",
    "\n",
    "# assert np.allclose(numeric_grads, batchnorm_grads, rtol=1e-3, atol=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:26:04.621992100Z",
     "start_time": "2024-03-02T12:26:04.544854700Z"
    }
   },
   "execution_count": 233
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.0658141e-14])"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm = BatchNormalization()\n",
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "batchnorm.inference_mode = False\n",
    "batchnorm.forward(points)\n",
    "\n",
    "def foo(delta):\n",
    "    old_delta = batchnorm.delta\n",
    "    batchnorm.delta = delta\n",
    "    val = np.sum((batchnorm.forward(points)) ** 4)\n",
    "    batchnorm.delta = old_delta\n",
    "    return val\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(foo, np.array([0.0]))\n",
    "res = batchnorm.forward(points)\n",
    "batchnorm_grads, params = batchnorm.backward(4 * (res ** 3))\n",
    "\n",
    "# batchnorm_grads - numeric_grads\n",
    "\n",
    "numeric_grads - params[1]\n",
    "\n",
    "# assert np.allclose(numeric_grads, batchnorm_grads, rtol=1e-3, atol=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:26:05.291835600Z",
     "start_time": "2024-03-02T12:26:05.200512800Z"
    }
   },
   "execution_count": 234
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    \n",
    "    set_weights(weights, network)\n",
    "    \n",
    "    activations = forward(network, X)\n",
    "    loss = NLL(activations, y)\n",
    "    \n",
    "    weights_grad_arrays = []\n",
    "    \n",
    "    d_loss_d_input = grad_NLL(activations, y)\n",
    "    for layer in reversed(network):\n",
    "        d_loss_d_input, d_loss_d_layer = layer.backward(d_loss_d_input)\n",
    "        if isinstance(layer, Dense):\n",
    "            weights_grad_arrays.append(np.sum(d_loss_d_layer[1], axis=0).ravel().tolist())\n",
    "            weights_grad_arrays.append(np.sum(d_loss_d_layer[0], axis=0).ravel().tolist())\n",
    "        elif isinstance(layer, BatchNormalization):\n",
    "            weights_grad_arrays.append([d_loss_d_layer[1]])\n",
    "            weights_grad_arrays.append([d_loss_d_layer[0]])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    weights_grad_arrays.reverse()\n",
    "    weights_grad = []\n",
    "    for wg in weights_grad_arrays:\n",
    "        weights_grad += wg\n",
    "    \n",
    "    # print(f\"Mean abs grad value: {np.mean(np.abs(weights_grad))}\\tMax abs grad value: {np.max(np.abs(weights_grad))}\")\n",
    "    # print(f\"Loss = {loss}\")\n",
    "    \n",
    "    # print(f\"{len(weights_grad)=}\")\n",
    "    \n",
    "    return loss, np.array(weights_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:26:06.033699Z",
     "start_time": "2024-03-02T12:26:05.960222200Z"
    }
   },
   "execution_count": 235
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_network(input_size, hidden_layers_size, output_size, n_layers=3, activation_class=ReLU, dropout: bool = False, dropout_p: float = 0.5, batch_norm: bool = False):\n",
    "    network = []\n",
    "\n",
    "    for layer_idx in range(n_layers):\n",
    "        # Compute sizes of current linear layer\n",
    "        layer_in = input_size if layer_idx == 0 else hidden_layers_size\n",
    "        layer_out = output_size if layer_idx == n_layers - 1 else hidden_layers_size\n",
    "        \n",
    "        # Add linear layer to the network\n",
    "        network.append(Dense(layer_in, layer_out))\n",
    "\n",
    "        # Add activation after each layer except the last one\n",
    "        if layer_idx != n_layers - 1:\n",
    "            network.append(activation_class())\n",
    "            if batch_norm:\n",
    "                network.append(BatchNormalization())\n",
    "            if dropout:\n",
    "                network.append(Dropout(dropout_p))\n",
    "\n",
    "    # Add LogSoftmax layer to the network\n",
    "    network.append(LogSoftmax())\n",
    "\n",
    "    return network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:26:07.853300700Z",
     "start_time": "2024-03-02T12:26:07.786780500Z"
    }
   },
   "execution_count": 236
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Dense(64, 32),\n Relu(),\n BatchNormalization(),\n Dense(32, 32),\n Relu(),\n BatchNormalization(),\n Dense(32, 10),\n LogSoftmax()]"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = make_network(input_size, 32, output_size, layer_c, ReLU, dropout=False, dropout_p=0.1, batch_norm=True)\n",
    "set_inference_mode(network, False)\n",
    "weights = get_weights(network)\n",
    "network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:27:37.817011500Z",
     "start_time": "2024-03-02T12:27:37.644625600Z"
    }
   },
   "execution_count": 243
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[array([1.]), array([0.])]"
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network[2].params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:27:38.787992400Z",
     "start_time": "2024-03-02T12:27:38.726277100Z"
    }
   },
   "execution_count": 244
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1000, True, 'Max iterations reached')"
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cb = Callback(network, X_train, y_train, X_test, y_test, print=False)\n",
    "set_inference_mode(network, False)\n",
    "res = minimize(\n",
    "    compute_loss_grad, weights,       # fun and start point\n",
    "    args=[network, X_train, y_train], # args passed to fun\n",
    "    method=SGD,                       # optimization method\n",
    "    jac=True,                         # says that gradient is computed in fun,\n",
    "    options={'disp': True, 'momentum': 0.9, 'batch_size': 100, 'n_iter': 1_000, 'lr': 0.05},\n",
    "    tol=1e-7,\n",
    "    # callback=cb.call\n",
    ")\n",
    "\n",
    "res['nit'], res['success'], res['message']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:27:57.848599200Z",
     "start_time": "2024-03-02T12:27:39.528464500Z"
    }
   },
   "execution_count": 245
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dense(64, 32), Relu(), BatchNormalization(), Dense(32, 32), Relu(), BatchNormalization(), Dense(32, 10), LogSoftmax()]\n",
      "Train NLL: 1.9284110\t\tTest NLL: 2.0499594\n",
      "Train accuracy: 0.286\t\tTest accuracy: 0.222\n"
     ]
    }
   ],
   "source": [
    "set_weights(weights=res[\"x\"], network=network)\n",
    "\n",
    "train_NLL = NLL(forward(network, X_train), y_train)\n",
    "set_inference_mode(network, True)\n",
    "print(network)\n",
    "test_NLL = NLL(forward(network, X_test), y_test)\n",
    "\n",
    "print(f\"Train NLL: {train_NLL:.7f}\\t\\tTest NLL: {test_NLL:.7f}\")\n",
    "\n",
    "\n",
    "train_accuracy = accuracy_score(y_true=y_train, y_pred=predict(network, X_train))\n",
    "test_accuracy = accuracy_score(y_true=y_test, y_pred=predict(network, X_test))\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}\\t\\tTest accuracy: {test_accuracy:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:28:00.576561700Z",
     "start_time": "2024-03-02T12:28:00.536407100Z"
    }
   },
   "execution_count": 246
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
