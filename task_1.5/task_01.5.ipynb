{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Практикум по программированию на языке Python`\n",
    "\n",
    "## `Задание 1.5. Классификация изображений`.\n",
    "\n",
    "#### Фамилия, имя: Богачев Владимир\n",
    "\n",
    "Дата выдачи: <span style=\"color:red\">__5 марта__</span>.\n",
    "\n",
    "Мягкий дедлайн: <span style=\"color:red\">__12 марта 23:59__</span>.\n",
    "\n",
    "Стоимость: __10 баллов__ (основная часть заданий) + __3 балла__ (дополнительные задания).\n",
    "\n",
    "<span style=\"color:red\">__В ноутбуке все клетки должны выполняться без ошибок при последовательном их выполнении.__</span>\n",
    "\n",
    "#### `Москва, 2024`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:26.767931500Z",
     "start_time": "2024-03-12T19:12:24.967910100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Задание 1. Загрузка данных (0.5 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой домашке работа с данными будем максимально облегчённой. Мы скачаем MNIST с помощью [стандартных средств](https://pytorch.org/vision/0.8/datasets.html#mnist) торча. Посмотреть список других доступных датасетов можно [здесь](https://pytorch.org/vision/0.8/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T20:28:52.683484300Z",
     "start_time": "2024-03-09T20:28:52.652476300Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_mnist(train, transform=None):    \n",
    "    if transform is None:\n",
    "        transform = T.ToTensor()\n",
    "    return torchvision.datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=train,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пиксели изображений принимают значения [0,1]. Скачайте датасет и посчитайте выборочное среднее и выборочное стандартное отклонение для значения пикселя. Т.е. должно получиться число `mean` для среднего, и число `std` для стандартного отклонения.\n",
    "\n",
    "*Подсказка.* Подумайте, на какой части датасета нужно считать эти статистики (обучение или валидация), чтобы предотвратить утечку данных.\n",
    "\n",
    "*Подсказка.* У торч датасета реализован метод `__getitem__`, т.е. его можно индексировать `dataset[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:37.465476100Z",
     "start_time": "2024-03-09T09:14:31.594463200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.Size([60000, 1, 28, 28])\n",
      "mean = 0.13, std = 0.31\n"
     ]
    }
   ],
   "source": [
    "norm_transform = T.Compose([\n",
    "    T.PILToTensor(),\n",
    "    T.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "data_train = load_mnist(True, transform=norm_transform)\n",
    "\n",
    "X_train = torch.stack([data_train[i][0] for i in range(len(data_train))])\n",
    "\n",
    "print(X_train.dtype, X_train.shape)\n",
    "\n",
    "mean =  torch.mean(X_train)\n",
    "std =  torch.std(X_train)\n",
    "print(f\"mean = {mean:.2f}, std = {std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные значения можно применить для нормализации изображений, когда будем подавать их в нейросеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:37.526020700Z",
     "start_time": "2024-03-09T09:14:37.465476100Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_mnist = load_mnist(True, norm_transform)\n",
    "val_mnist = load_mnist(False, norm_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во время обучения данные подаются в нейросеть батчами.\n",
    "\n",
    "Обычно batch_size выбирается так, чтобы во время обучения получалась меньше дисперсия градиента (ну вы уже знакомы с sgd), также часто batch_size выбирается максимально возможным по ресурсам.\n",
    "\n",
    "Для этой задачи нам хватит batch_size = 64 во время обучения.\n",
    "\n",
    "*Ответьте на вопрос:* Почему в большинстве задач для валидации и тестирования можно выбирать batch_size больше? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:37.528022400Z",
     "start_time": "2024-03-09T09:14:37.503603100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_mnist_loader = DataLoader(\n",
    "    dataset=train_mnist,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "val_mnist_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_mnist,\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:40.333355500Z",
     "start_time": "2024-03-09T09:14:37.507020700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(train_mnist_loader))\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:40.425437900Z",
     "start_time": "2024-03-09T09:14:40.333355500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 3\n",
    "plt.title(f'digit {labels[i]}')\n",
    "plt.imshow(images[i, 0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Задание 2. Цикл обучения (1.5 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже предоставлен базовый вариант цикла обучения, представленный тремя функциями: `train_epoch`, `val_epoch`, `train_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:40.432048600Z",
     "start_time": "2024-03-09T09:14:40.424437400Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "):\n",
    "    \"\"\"train `network` with `optimizer` for one epoch with data from `train_loader` to minimize `criterion`\"\"\"\n",
    "    \n",
    "    network.train()  # switch network submodules to train mode, e.g. it influences on batch-norm, dropout\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)   # send data to device\n",
    "        optimizer.zero_grad()   # zero out grads, collected from previous batch\n",
    "        logits = network(images)  # forward pass\n",
    "        loss = criterion(logits, labels, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:40.433056800Z",
     "start_time": "2024-03-09T09:14:40.430356700Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_epoch(\n",
    "    network,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "):\n",
    "    \"\"\"calculate loss and accuracy on validation data\"\"\"\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    network.eval()  # switch network submodules to test mode\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = network(images)\n",
    "        val_loss += criterion(logits, labels, reduction='sum').item()\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(pred == labels).item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accur = correct / len(val_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f'Test set: Avg. loss: {val_loss:.4f}',\n",
    "        f'Accuracy: {correct}/{len(val_loader.dataset)}',\n",
    "        f'({100. * val_accur:.0f}%)',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:40.489363600Z",
     "start_time": "2024-03-09T09:14:40.434056500Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val(\n",
    "    network,\n",
    "    n_epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "):\n",
    "    \"\"\"full cycle of neural network training\"\"\"\n",
    "\n",
    "    val_epoch(network, val_loader, criterion)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_epoch(network, train_loader, criterion, optimizer)\n",
    "        val_epoch(network, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, что все работает, обучите бейзлайн -- однослойную [полносвязную](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) сеть на [кросс-энтропийный лосс](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html). \n",
    "\n",
    "\n",
    "*Заметьте*, что на вход этому лоссу нужно подавать сырой выход нейросети, а не результат применения софтмакса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:14:40.491363700Z",
     "start_time": "2024-03-09T09:14:40.449363600Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(in_features=input_size, out_features=output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"{x.shape=}\", end='\\t')\n",
    "        x = self.flatten(x)\n",
    "        # print(f\"{x.shape=}\", end='\\t')\n",
    "        x = self.linear(x)\n",
    "        # print(f\"{x.shape=}\", end='\\n')\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:16:55.653598900Z",
     "start_time": "2024-03-09T09:14:40.449363600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Avg. loss: 2.4114 Accuracy: 843/10000 (8%)\n",
      "Test set: Avg. loss: 0.4583 Accuracy: 8855/10000 (89%)\n",
      "Test set: Avg. loss: 0.3604 Accuracy: 9037/10000 (90%)\n",
      "Test set: Avg. loss: 0.3255 Accuracy: 9083/10000 (91%)\n",
      "Test set: Avg. loss: 0.3065 Accuracy: 9149/10000 (91%)\n",
      "Test set: Avg. loss: 0.2956 Accuracy: 9182/10000 (92%)\n",
      "Test set: Avg. loss: 0.2887 Accuracy: 9188/10000 (92%)\n",
      "Test set: Avg. loss: 0.2847 Accuracy: 9202/10000 (92%)\n",
      "Test set: Avg. loss: 0.2810 Accuracy: 9205/10000 (92%)\n",
      "Test set: Avg. loss: 0.2774 Accuracy: 9209/10000 (92%)\n",
      "Test set: Avg. loss: 0.2761 Accuracy: 9227/10000 (92%)\n",
      "Test set: Avg. loss: 0.2738 Accuracy: 9224/10000 (92%)\n",
      "Test set: Avg. loss: 0.2720 Accuracy: 9237/10000 (92%)\n",
      "Test set: Avg. loss: 0.2725 Accuracy: 9238/10000 (92%)\n",
      "Test set: Avg. loss: 0.2718 Accuracy: 9238/10000 (92%)\n",
      "Test set: Avg. loss: 0.2706 Accuracy: 9216/10000 (92%)\n",
      "Test set: Avg. loss: 0.2694 Accuracy: 9240/10000 (92%)\n",
      "Test set: Avg. loss: 0.2682 Accuracy: 9257/10000 (93%)\n",
      "Test set: Avg. loss: 0.2687 Accuracy: 9249/10000 (92%)\n",
      "Test set: Avg. loss: 0.2667 Accuracy: 9249/10000 (92%)\n",
      "Test set: Avg. loss: 0.2666 Accuracy: 9239/10000 (92%)\n"
     ]
    }
   ],
   "source": [
    "model = LogReg(28 * 28, 10).to(device)\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "    train_loader=train_mnist_loader,\n",
    "    val_loader=val_mnist_loader,\n",
    "    n_epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test set: Avg. loss: 2.4114 Accuracy: 843/10000 (8%)\n",
    "Test set: Avg. loss: 0.2837 Accuracy: 9179/10000 (92%)\n",
    "Test set: Avg. loss: 0.2796 Accuracy: 9201/10000 (92%)\n",
    "Test set: Avg. loss: 0.2812 Accuracy: 9199/10000 (92%)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Задание 3. Логирование (3 балл)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скопируйте и перепишите функции `train_epoch`, `val_epoch`, `train_val` таким образом, чтобы\n",
    "- во время обучения раз в `logging_interval` шагов оптимизации выводились лосс и аккураси на одном батче (пример ниже, но не обязательно делать идентично)\n",
    "- происходило сохранение весов лучшей (по аккураси на валидации) модели и состояния оптимизатора ([в помощь](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended))\n",
    "- в конце обучения выводился и сохранялся график-саммари (пример ниже, но не обязательно делать идентично)\n",
    "\n",
    "*Ответьте на вопрос:* Что хранится в состоянии оптимизатора? Зачем нужно его сохранять? Приведите хотя бы один пример оптимизатора, для которого есть смысл сохранять состояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:16:55.661113600Z",
     "start_time": "2024-03-09T09:16:55.653598900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "        self.best_val_acc = -1\n",
    "        self.best_model_wts = None\n",
    "        self.best_optimizer_st = None\n",
    "    \n",
    "    def log_training(self, epoch, train_loss, train_acc):\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.train_acc.append(train_acc)\n",
    "    \n",
    "    def log_validation(self, epoch, val_loss, val_acc, model, optimizer):\n",
    "        self.val_loss.append(val_loss)\n",
    "        self.val_acc.append(val_acc)\n",
    "        \n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            # TODO: save model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    logger: Logger,\n",
    "    logging_interval: int = 1,\n",
    "):\n",
    "    \"\"\"train `network` with `optimizer` for one epoch with data from `train_loader` to minimize `criterion`\"\"\"\n",
    "    \n",
    "    network.train()  # switch network submodules to train mode, e.g. it influences on batch-norm, dropout\n",
    "    logger_step = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)   # send data to device\n",
    "        optimizer.zero_grad()   # zero out grads, collected from previous batch\n",
    "        logits = network(images)  # forward pass\n",
    "        loss = criterion(logits, labels, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logger_step = (logger_step + 1) % logging_interval\n",
    "        \n",
    "        if logger_step == 0:\n",
    "            pred = logits.argmax(dim=1)\n",
    "            train_accuracy = torch.sum(pred == labels).item() / len(labels)\n",
    "            logger.log_training(-1, loss.item(), train_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T09:16:55.667862900Z",
     "start_time": "2024-03-09T09:16:55.662347800Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_epoch(\n",
    "    network,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    logger: Logger\n",
    "):\n",
    "    \"\"\"calculate loss and accuracy on validation data\"\"\"\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    network.eval()  # switch network submodules to test mode\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = network(images)\n",
    "        val_loss += criterion(logits, labels, reduction='sum').item()\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(pred == labels).item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accur = correct / len(val_loader.dataset)\n",
    "\n",
    "    logger.log_validation(-1, val_loss, val_accur, None, None)\n",
    "\n",
    "    print(\n",
    "        f'Test set: Avg. loss: {val_loss:.4f}',\n",
    "        f'Accuracy: {correct}/{len(val_loader.dataset)}',\n",
    "        f'({100. * val_accur:.0f}%)',\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T09:16:55.673743100Z",
     "start_time": "2024-03-09T09:16:55.667862900Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_val(\n",
    "    network,\n",
    "    n_epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    logger: Logger,\n",
    "    logging_interval: int = 5,\n",
    "):\n",
    "    \"\"\"full cycle of neural network training\"\"\"\n",
    "\n",
    "    val_epoch(network, val_loader, criterion, logger)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_epoch(network, train_loader, criterion, optimizer, logger, logging_interval)\n",
    "        val_epoch(network, val_loader, criterion, logger)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T09:16:55.674742600Z",
     "start_time": "2024-03-09T09:16:55.670861100Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T09:19:34.136674800Z",
     "start_time": "2024-03-09T09:17:19.232224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Avg. loss: 2.4668 Accuracy: 927/10000 (9%)\n",
      "Test set: Avg. loss: 0.4624 Accuracy: 8866/10000 (89%)\n",
      "Test set: Avg. loss: 0.3609 Accuracy: 9058/10000 (91%)\n",
      "Test set: Avg. loss: 0.3254 Accuracy: 9125/10000 (91%)\n",
      "Test set: Avg. loss: 0.3066 Accuracy: 9152/10000 (92%)\n",
      "Test set: Avg. loss: 0.2954 Accuracy: 9176/10000 (92%)\n",
      "Test set: Avg. loss: 0.2898 Accuracy: 9187/10000 (92%)\n",
      "Test set: Avg. loss: 0.2850 Accuracy: 9192/10000 (92%)\n",
      "Test set: Avg. loss: 0.2812 Accuracy: 9203/10000 (92%)\n",
      "Test set: Avg. loss: 0.2791 Accuracy: 9208/10000 (92%)\n",
      "Test set: Avg. loss: 0.2766 Accuracy: 9206/10000 (92%)\n",
      "Test set: Avg. loss: 0.2749 Accuracy: 9208/10000 (92%)\n",
      "Test set: Avg. loss: 0.2726 Accuracy: 9221/10000 (92%)\n",
      "Test set: Avg. loss: 0.2721 Accuracy: 9219/10000 (92%)\n",
      "Test set: Avg. loss: 0.2699 Accuracy: 9223/10000 (92%)\n",
      "Test set: Avg. loss: 0.2700 Accuracy: 9224/10000 (92%)\n",
      "Test set: Avg. loss: 0.2693 Accuracy: 9240/10000 (92%)\n",
      "Test set: Avg. loss: 0.2690 Accuracy: 9238/10000 (92%)\n",
      "Test set: Avg. loss: 0.2674 Accuracy: 9245/10000 (92%)\n",
      "Test set: Avg. loss: 0.2664 Accuracy: 9237/10000 (92%)\n",
      "Test set: Avg. loss: 0.2665 Accuracy: 9243/10000 (92%)\n"
     ]
    }
   ],
   "source": [
    "model = LogReg(28 * 28, 10).to(device)\n",
    "\n",
    "lg = Logger()\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "    train_loader=train_mnist_loader,\n",
    "    val_loader=val_mnist_loader,\n",
    "    n_epochs=20,\n",
    "    logger=lg,\n",
    "    logging_interval=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x1c81cfcfc50>]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lg.val_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T09:19:36.992550500Z",
     "start_time": "2024-03-09T09:19:36.906558800Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите лучшие веса модели и подсчитайте аккураси на валидации, чтобы подтвердить что веса сохранились корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-28T20:57:53.271093100Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите сохранённое состояние оптимизатора и сделайте две эпохи обучения, чтобы подтвердить, что состояния оптимизатора были сохранены корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-28T20:57:53.272092700Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Задание 4. Логирование с помощью tensorboard (1 балл)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле это дурной тон логировать обучение нейросети так, как это сделали вы выше. Дело в том, что весь функционал уже реализован в [`tensorboard`](https://pytorch.org/docs/stable/tensorboard.html), а вы написали велосипед."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T12:30:18.089493100Z",
     "start_time": "2024-03-10T12:30:15.821922300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in c:\\python_venv\\lib\\site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\python_venv\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\python_venv\\lib\\site-packages (from tensorboard) (1.62.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python_venv\\lib\\site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\python_venv\\lib\\site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\python_venv\\lib\\site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\python_venv\\lib\\site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\python_venv\\lib\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python_venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python_venv\\lib\\site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python_venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас есть доступ к терминалу и браузеру, то удобнее всего ввести команду\n",
    "```bash\n",
    "tensorboard --logdir .\n",
    "```\n",
    "и открыть UI в выделенном локалхосте. \n",
    "\n",
    "Если вы работаете в гугл колабе, то можете запустить UI как виджет в Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-28T20:57:53.274092600Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как только вы начнете логировать в объект типа `SummaryWriter`, в UI начнут строиться кривые обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скопируйте и перепишите функции обучения таким образом, чтобы логирование происходило в `tensorboard`.\n",
    "\n",
    "*Замечание.* Вам пригодится метод `add_scalar`, чтобы сохранять лосс и аккураси, и метод `add_hparams`, чтобы сохранить важные гиперпараметры (например, кодовое имя архитектуры сети) и финальное достигнутое качество."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from itertools import count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T12:30:18.527113Z",
     "start_time": "2024-03-10T12:30:18.518952800Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T12:30:18.714584600Z",
     "start_time": "2024-03-10T12:30:18.707070300Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    writer: SummaryWriter,\n",
    "    logging_interval: int = 1,\n",
    "    logger_step_cntr: count = None,\n",
    "):\n",
    "    \"\"\"train `network` with `optimizer` for one epoch with data from `train_loader` to minimize `criterion`\"\"\"\n",
    "    \n",
    "    network.train()  # switch network submodules to train mode, e.g. it influences on batch-norm, dropout\n",
    "    logger_step_cntr = count() if logger_step_cntr is None else logger_step_cntr\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)   # send data to device\n",
    "        optimizer.zero_grad()   # zero out grads, collected from previous batch\n",
    "        logits = network(images)  # forward pass\n",
    "        loss = criterion(logits, labels, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logger_step = next(logger_step_cntr)\n",
    "        \n",
    "        if logger_step % logging_interval == 0:\n",
    "            pred = logits.argmax(dim=1)\n",
    "            train_accuracy = torch.sum(pred == labels).item() / len(labels)\n",
    "            writer.add_scalar('Loss/train', loss.item(), logger_step // logging_interval)\n",
    "            writer.add_scalar('Accuracy/train', train_accuracy, logger_step // logging_interval)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_epoch(\n",
    "    network,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    writer: SummaryWriter,\n",
    "    epoch_no: int\n",
    "):\n",
    "    \"\"\"calculate loss and accuracy on validation data\"\"\"\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    network.eval()  # switch network submodules to test mode\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = network(images)\n",
    "        val_loss += criterion(logits, labels, reduction='sum').item()\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(pred == labels).item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accur = correct / len(val_loader.dataset)\n",
    "\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch_no)\n",
    "    writer.add_scalar('Accuracy/val', val_accur, epoch_no)\n",
    "    # logger.log_validation(-1, val_loss, val_accur, None, None)\n",
    "\n",
    "    print(\n",
    "        f'Test set: Avg. loss: {val_loss:.4f}',\n",
    "        f'Accuracy: {correct}/{len(val_loader.dataset)}',\n",
    "        f'({100. * val_accur:.0f}%)',\n",
    "    )\n",
    "    \n",
    "    return val_loss, val_accur"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T12:30:18.891532800Z",
     "start_time": "2024-03-10T12:30:18.880914300Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_val(\n",
    "    network,\n",
    "    n_epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    writer: SummaryWriter,\n",
    "    logging_interval: int = 5,\n",
    "    model_name: str = None,\n",
    "):\n",
    "    \"\"\"full cycle of neural network training\"\"\"\n",
    "\n",
    "    if model_name is not None:\n",
    "        writer.add_hparams(run_name=model_name, hparam_dict={}, metric_dict={})\n",
    "\n",
    "    logger_step_cntr = count(0)\n",
    "    val_loss, val_accur = val_epoch(network, val_loader, criterion, writer, epoch_no=0)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_epoch(network, train_loader, criterion, optimizer, writer, logging_interval, logger_step_cntr)\n",
    "        val_loss, val_accur = val_epoch(network, val_loader, criterion, writer, epoch_no=epoch)\n",
    "    \n",
    "    # if model_name is not None:\n",
    "    #     writer.add_hparams(run_name=model_name, hparam_dict=network.state_dict(), metric_dict={'val_loss': val_loss, 'val_accur': val_accur})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T12:30:19.355580500Z",
     "start_time": "2024-03-10T12:30:19.338067100Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Avg. loss: 2.4057 Accuracy: 1434/10000 (14%)\n",
      "Test set: Avg. loss: 0.4585 Accuracy: 8858/10000 (89%)\n",
      "Test set: Avg. loss: 0.3596 Accuracy: 9047/10000 (90%)\n",
      "Test set: Avg. loss: 0.3259 Accuracy: 9109/10000 (91%)\n",
      "Test set: Avg. loss: 0.3066 Accuracy: 9140/10000 (91%)\n",
      "Test set: Avg. loss: 0.2975 Accuracy: 9155/10000 (92%)\n",
      "Test set: Avg. loss: 0.2900 Accuracy: 9182/10000 (92%)\n",
      "Test set: Avg. loss: 0.2854 Accuracy: 9176/10000 (92%)\n",
      "Test set: Avg. loss: 0.2809 Accuracy: 9193/10000 (92%)\n",
      "Test set: Avg. loss: 0.2794 Accuracy: 9208/10000 (92%)\n",
      "Test set: Avg. loss: 0.2774 Accuracy: 9213/10000 (92%)\n",
      "Test set: Avg. loss: 0.2739 Accuracy: 9216/10000 (92%)\n",
      "Test set: Avg. loss: 0.2742 Accuracy: 9209/10000 (92%)\n",
      "Test set: Avg. loss: 0.2722 Accuracy: 9230/10000 (92%)\n",
      "Test set: Avg. loss: 0.2710 Accuracy: 9227/10000 (92%)\n",
      "Test set: Avg. loss: 0.2716 Accuracy: 9224/10000 (92%)\n",
      "Test set: Avg. loss: 0.2700 Accuracy: 9235/10000 (92%)\n",
      "Test set: Avg. loss: 0.2681 Accuracy: 9245/10000 (92%)\n",
      "Test set: Avg. loss: 0.2690 Accuracy: 9240/10000 (92%)\n",
      "Test set: Avg. loss: 0.2676 Accuracy: 9240/10000 (92%)\n",
      "Test set: Avg. loss: 0.2684 Accuracy: 9238/10000 (92%)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "hparam_dict and metric_dict should be dictionary.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m LogReg(\u001B[38;5;241m28\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m10\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter()\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain_val\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_mnist_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_mnist_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwriter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmy_nn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[0;32m     15\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[29], line 21\u001B[0m, in \u001B[0;36mtrain_val\u001B[1;34m(network, n_epochs, criterion, optimizer, train_loader, val_loader, writer, logging_interval, model_name)\u001B[0m\n\u001B[0;32m     18\u001B[0m     val_loss, val_accur \u001B[38;5;241m=\u001B[39m val_epoch(network, val_loader, criterion, writer, epoch_no\u001B[38;5;241m=\u001B[39mepoch)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 21\u001B[0m     \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_hparams\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhparam_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_loss\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_accur\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_accur\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:335\u001B[0m, in \u001B[0;36mSummaryWriter.add_hparams\u001B[1;34m(self, hparam_dict, metric_dict, hparam_domain_discrete, run_name)\u001B[0m\n\u001B[0;32m    333\u001B[0m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_log_api_usage_once(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorboard.logging.add_hparams\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(hparam_dict) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mdict\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(metric_dict) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mdict\u001B[39m:\n\u001B[1;32m--> 335\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhparam_dict and metric_dict should be dictionary.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    336\u001B[0m exp, ssi, sei \u001B[38;5;241m=\u001B[39m hparams(hparam_dict, metric_dict, hparam_domain_discrete)\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m run_name:\n",
      "\u001B[1;31mTypeError\u001B[0m: hparam_dict and metric_dict should be dictionary."
     ]
    }
   ],
   "source": [
    "model = LogReg(28 * 28, 10).to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "    train_loader=train_mnist_loader,\n",
    "    val_loader=val_mnist_loader,\n",
    "    n_epochs=20,\n",
    "    writer=writer,\n",
    "    logging_interval=5,\n",
    "    model_name='my_nn'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T09:36:29.877349200Z",
     "start_time": "2024-03-09T09:34:13.230323700Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T09:29:34.683731700Z",
     "start_time": "2024-03-09T09:29:34.675012400Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Задание 5. Побейте бейзлайн (2+2 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На датасете [`CIFAR-10`](https://pytorch.org/vision/0.8/datasets.html#torchvision.datasets.CIFAR10) обучите модель, которая выдает аккураси `>=0.7` (2 балла) и аккураси `>=0.8` (ещё 2 балла).\n",
    "- Можете использовать любые модули `pytorch`, любые оптимизаторы и шедулеры, можете использовать аугментации ([например](https://pytorch.org/vision/stable/transforms.html))\n",
    "- Для отслеживания экспериментов используйте логирование с `tensorboard`, не забывайте давать осмысленные имена эспериментам и логировать нужные гиперпараметры, сохранять веса сети и состояние оптимизатора."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:35.946382600Z",
     "start_time": "2024-03-12T19:12:35.733894200Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    writer: SummaryWriter,\n",
    "    logging_interval: int = 1,\n",
    "    logger_step_cntr: count = None,\n",
    "):\n",
    "    \"\"\"train `network` with `optimizer` for one epoch with data from `train_loader` to minimize `criterion`\"\"\"\n",
    "    \n",
    "    network.train()  # switch network submodules to train mode, e.g. it influences on batch-norm, dropout\n",
    "    logger_step_cntr = count() if logger_step_cntr is None else logger_step_cntr\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)   # send data to device\n",
    "        optimizer.zero_grad()   # zero out grads, collected from previous batch\n",
    "        logits = network(images)  # forward pass\n",
    "        loss = criterion(logits, labels, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logger_step = next(logger_step_cntr)\n",
    "        \n",
    "        if logger_step % logging_interval == 0:\n",
    "            pred = logits.argmax(dim=1)\n",
    "            train_accuracy = torch.sum(pred == labels).item() / len(labels)\n",
    "            writer.add_scalar('Loss/train', loss.item(), logger_step // logging_interval)\n",
    "            writer.add_scalar('Accuracy/train', train_accuracy, logger_step // logging_interval)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:35.956695700Z",
     "start_time": "2024-03-12T19:12:35.949379100Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_epoch(\n",
    "    network,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    writer: SummaryWriter,\n",
    "    epoch_no: int\n",
    "):\n",
    "    \"\"\"calculate loss and accuracy on validation data\"\"\"\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    network.eval()  # switch network submodules to test mode\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = network(images)\n",
    "        val_loss += criterion(logits, labels, reduction='sum').item()\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(pred == labels).item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accur = correct / len(val_loader.dataset)\n",
    "\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch_no)\n",
    "    writer.add_scalar('Accuracy/val', val_accur, epoch_no)\n",
    "    # logger.log_validation(-1, val_loss, val_accur, None, None)\n",
    "\n",
    "    print(\n",
    "        f'Test set: Avg. loss: {val_loss:.4f}',\n",
    "        f'Accuracy: {correct}/{len(val_loader.dataset)}',\n",
    "        f'({100. * val_accur:.0f}%)',\n",
    "    )\n",
    "    \n",
    "    return val_loss, val_accur"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:36.047846700Z",
     "start_time": "2024-03-12T19:12:36.039769900Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_val(\n",
    "    network,\n",
    "    n_epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    writer: SummaryWriter,\n",
    "    logging_interval: int = 5,\n",
    "    run_name: str = None,\n",
    "    model_description_dict: dict = None\n",
    "):\n",
    "    \"\"\"full cycle of neural network training\"\"\"\n",
    "\n",
    "    # best_accuracy = -1\n",
    "    # best_loss = float('inf')\n",
    "    # \n",
    "    # if (model_description_dict is not None) and (run_name is not None):\n",
    "    #     writer.add_hparams(run_name=run_name, hparam_dict=model_description_dict, metric_dict={'best_accuracy': best_accuracy, 'best_loss': best_loss})\n",
    "\n",
    "    logger_step_cntr = count(0)\n",
    "    val_loss, val_accur = val_epoch(network, val_loader, criterion, writer, epoch_no=0)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        writer.add_scalar('Params/LR', optimizer.state_dict()['param_groups'][0]['lr'], epoch)\n",
    "        train_epoch(network, train_loader, criterion, optimizer, writer, logging_interval, logger_step_cntr)\n",
    "        val_loss, val_accur = val_epoch(network, val_loader, criterion, writer, epoch_no=epoch)\n",
    "        lr_scheduler.step(val_loss)\n",
    "    \n",
    "    # if model_name is not None:\n",
    "    #     writer.add_hparams(run_name=model_name, hparam_dict=network.state_dict(), metric_dict={'val_loss': val_loss, 'val_accur': val_accur})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:16:09.616795900Z",
     "start_time": "2024-03-12T19:16:09.588397Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:36.853395700Z",
     "start_time": "2024-03-12T19:12:36.818463200Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_cifar10(train: bool, transform = None):\n",
    "    if transform is None:\n",
    "        transform = T.Compose([T.ToTensor()])\n",
    "    return torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = load_cifar10(True)\n",
    "# cifar10_val = load_cifar10(False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:37.596881700Z",
     "start_time": "2024-03-12T19:12:37.013644300Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 32, 32])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_train[3][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:37.604240700Z",
     "start_time": "2024-03-12T19:12:37.597883200Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x255b6d4ac50>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cifar10_train[1][0].transpose(0, 1).transpose(1, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:37.708917600Z",
     "start_time": "2024-03-12T19:12:37.603241400Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.Size([50000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.stack([cifar10_train[i][0] for i in range(len(cifar10_train))])\n",
    "\n",
    "print(X_train.dtype, X_train.shape)\n",
    "\n",
    "mean =  torch.mean(X_train, dim=[0, 2, 3])\n",
    "std =  torch.std(X_train, dim=[0, 2, 3])\n",
    "# print(f\"mean = {mean:.2f}, std = {std:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:44.151128700Z",
     "start_time": "2024-03-12T19:12:37.879595900Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:44.156357600Z",
     "start_time": "2024-03-12T19:12:44.152127Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Аугментация позаимствована из [ноутбука](https://www.kaggle.com/code/vikasbhadoria/cifar10-high-accuracy-model-build-on-pytorch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.5), \n",
    "    T.RandomRotation(10),\n",
    "    T.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    # T.GaussianBlur(kernel_size=(3, 3), sigma=(0.05, 0.5)),\n",
    "    # T.RandomEqualize(p=0.2),\n",
    "    # T.RandomGrayscale(p=0.1),\n",
    "    T.ToTensor(), \n",
    "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "cifar10_train = load_cifar10(True, transform = transform_train)\n",
    "cifar10_val = load_cifar10(False, transform = transform_val)\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 512\n",
    "\n",
    "train_cifar10_loader = DataLoader(\n",
    "    dataset=cifar10_train,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=9\n",
    ")\n",
    "\n",
    "val_cifar10_loader = torch.utils.data.DataLoader(\n",
    "    dataset=cifar10_val,\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=6\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:16:14.602849700Z",
     "start_time": "2024-03-12T19:16:13.471493900Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:16:15.357130900Z",
     "start_time": "2024-03-12T19:16:15.334349300Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogReg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mLogReg\u001B[49m(\u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m10\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter()\n\u001B[0;32m      5\u001B[0m train_val(\n\u001B[0;32m      6\u001B[0m     network\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m      7\u001B[0m     criterion\u001B[38;5;241m=\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mcross_entropy,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m     model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcifar10_logreg\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     15\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'LogReg' is not defined"
     ]
    }
   ],
   "source": [
    "model = LogReg(32 * 32 * 3, 10).to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "    train_loader=train_cifar10_loader,\n",
    "    val_loader=val_cifar10_loader,\n",
    "    n_epochs=20,\n",
    "    writer=writer,\n",
    "    logging_interval=5,\n",
    "    model_name='cifar10_logreg'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T18:35:18.826317100Z",
     "start_time": "2024-03-10T18:35:18.381961100Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "AlexNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_cifar10_loader))\n",
    "\n",
    "print(images.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T09:05:33.833450500Z",
     "start_time": "2024-03-11T09:05:19.767389200Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 256, 4, 4])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(images).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T09:09:00.445071500Z",
     "start_time": "2024-03-11T09:09:00.273774200Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channels: int, pairs_cnt: int):\n",
    "        super().__init__()\n",
    "        self.conv_pairs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(channels),\n",
    "            )\n",
    "            for _ in range(pairs_cnt)\n",
    "        ])\n",
    "        \n",
    "        self.relu_blocks = nn.ModuleList([nn.ReLU() for _ in range(pairs_cnt)])\n",
    "        \n",
    "        self.pairs_cnt = pairs_cnt\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.pairs_cnt):\n",
    "            x = self.relu_blocks[i](x + self.conv_pairs[i](x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ConvDownsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * 2, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm2d(in_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels * 2, in_channels * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels * 2),\n",
    "        )\n",
    "        \n",
    "        self.downsample_id = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * 2, kernel_size=1, stride=2),\n",
    "            nn.BatchNorm2d(in_channels * 2),\n",
    "        )\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x) + self.downsample_id(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # 32 --> 16\n",
    "        )\n",
    "        \n",
    "        self.middle = nn.Sequential(\n",
    "            ConvBlock(32, 2),\n",
    "            \n",
    "            ConvDownsample(32), # 16 --> 8\n",
    "            ConvBlock(64, 1),\n",
    "            \n",
    "            ConvDownsample(64), # 8 --> 4\n",
    "            ConvBlock(128, 1),\n",
    "        )\n",
    "        \n",
    "        self.tail = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=2), # 4 --> 2\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(128 * 4),\n",
    "            nn.Linear(128 * 4, 10, bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.Linear(128, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.tail(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:45.391492600Z",
     "start_time": "2024-03-12T19:12:45.320324500Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "726250"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyResNet()\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:12:45.932570900Z",
     "start_time": "2024-03-12T19:12:45.920655900Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.001"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['lr']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T19:15:44.131203Z",
     "start_time": "2024-03-12T19:15:44.110931400Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Avg. loss: 2.3040 Accuracy: 1000/10000 (10%)\n",
      "Test set: Avg. loss: 1.3219 Accuracy: 5296/10000 (53%)\n",
      "Test set: Avg. loss: 0.9826 Accuracy: 6481/10000 (65%)\n",
      "Test set: Avg. loss: 0.9005 Accuracy: 6944/10000 (69%)\n",
      "Test set: Avg. loss: 0.8709 Accuracy: 7014/10000 (70%)\n",
      "Test set: Avg. loss: 0.7895 Accuracy: 7315/10000 (73%)\n",
      "Test set: Avg. loss: 0.6704 Accuracy: 7772/10000 (78%)\n",
      "Test set: Avg. loss: 0.6486 Accuracy: 7777/10000 (78%)\n",
      "Test set: Avg. loss: 0.6833 Accuracy: 7710/10000 (77%)\n",
      "Test set: Avg. loss: 0.6233 Accuracy: 7837/10000 (78%)\n",
      "Test set: Avg. loss: 0.5639 Accuracy: 8080/10000 (81%)\n",
      "Test set: Avg. loss: 0.5633 Accuracy: 8109/10000 (81%)\n",
      "Test set: Avg. loss: 0.5500 Accuracy: 8144/10000 (81%)\n",
      "Test set: Avg. loss: 0.4989 Accuracy: 8262/10000 (83%)\n",
      "Test set: Avg. loss: 0.5178 Accuracy: 8218/10000 (82%)\n",
      "Test set: Avg. loss: 0.4843 Accuracy: 8357/10000 (84%)\n",
      "Test set: Avg. loss: 0.5358 Accuracy: 8189/10000 (82%)\n",
      "Test set: Avg. loss: 0.4801 Accuracy: 8379/10000 (84%)\n",
      "Test set: Avg. loss: 0.4967 Accuracy: 8303/10000 (83%)\n",
      "Test set: Avg. loss: 0.4521 Accuracy: 8453/10000 (85%)\n",
      "Test set: Avg. loss: 0.4446 Accuracy: 8501/10000 (85%)\n",
      "Test set: Avg. loss: 0.4307 Accuracy: 8524/10000 (85%)\n",
      "Test set: Avg. loss: 0.4746 Accuracy: 8387/10000 (84%)\n",
      "Test set: Avg. loss: 0.4291 Accuracy: 8594/10000 (86%)\n",
      "Test set: Avg. loss: 0.4144 Accuracy: 8582/10000 (86%)\n",
      "Test set: Avg. loss: 0.4056 Accuracy: 8618/10000 (86%)\n",
      "Test set: Avg. loss: 0.4037 Accuracy: 8645/10000 (86%)\n",
      "Test set: Avg. loss: 0.4041 Accuracy: 8644/10000 (86%)\n",
      "Test set: Avg. loss: 0.3966 Accuracy: 8639/10000 (86%)\n",
      "Test set: Avg. loss: 0.3909 Accuracy: 8714/10000 (87%)\n",
      "Test set: Avg. loss: 0.4034 Accuracy: 8635/10000 (86%)\n",
      "Test set: Avg. loss: 0.3852 Accuracy: 8703/10000 (87%)\n",
      "Test set: Avg. loss: 0.3907 Accuracy: 8699/10000 (87%)\n",
      "Test set: Avg. loss: 0.3842 Accuracy: 8738/10000 (87%)\n",
      "Test set: Avg. loss: 0.3764 Accuracy: 8735/10000 (87%)\n",
      "Test set: Avg. loss: 0.3849 Accuracy: 8749/10000 (87%)\n",
      "Test set: Avg. loss: 0.3689 Accuracy: 8778/10000 (88%)\n",
      "Test set: Avg. loss: 0.3820 Accuracy: 8746/10000 (87%)\n",
      "Test set: Avg. loss: 0.3558 Accuracy: 8810/10000 (88%)\n",
      "Test set: Avg. loss: 0.3623 Accuracy: 8777/10000 (88%)\n",
      "Test set: Avg. loss: 0.4074 Accuracy: 8676/10000 (87%)\n",
      "Test set: Avg. loss: 0.3988 Accuracy: 8694/10000 (87%)\n",
      "Test set: Avg. loss: 0.3697 Accuracy: 8767/10000 (88%)\n",
      "Test set: Avg. loss: 0.3655 Accuracy: 8824/10000 (88%)\n",
      "Test set: Avg. loss: 0.3395 Accuracy: 8881/10000 (89%)\n",
      "Test set: Avg. loss: 0.3397 Accuracy: 8899/10000 (89%)\n",
      "Test set: Avg. loss: 0.3358 Accuracy: 8922/10000 (89%)\n",
      "Test set: Avg. loss: 0.3340 Accuracy: 8944/10000 (89%)\n",
      "Test set: Avg. loss: 0.3374 Accuracy: 8925/10000 (89%)\n",
      "Test set: Avg. loss: 0.3386 Accuracy: 8897/10000 (89%)\n",
      "Test set: Avg. loss: 0.3360 Accuracy: 8928/10000 (89%)\n",
      "Test set: Avg. loss: 0.3435 Accuracy: 8898/10000 (89%)\n",
      "Test set: Avg. loss: 0.3415 Accuracy: 8888/10000 (89%)\n",
      "Test set: Avg. loss: 0.3219 Accuracy: 8989/10000 (90%)\n",
      "Test set: Avg. loss: 0.3193 Accuracy: 9008/10000 (90%)\n",
      "Test set: Avg. loss: 0.3222 Accuracy: 8971/10000 (90%)\n",
      "Test set: Avg. loss: 0.3171 Accuracy: 9005/10000 (90%)\n",
      "Test set: Avg. loss: 0.3148 Accuracy: 9014/10000 (90%)\n",
      "Test set: Avg. loss: 0.3231 Accuracy: 8990/10000 (90%)\n",
      "Test set: Avg. loss: 0.3188 Accuracy: 9014/10000 (90%)\n",
      "Test set: Avg. loss: 0.3210 Accuracy: 8997/10000 (90%)\n",
      "Test set: Avg. loss: 0.3189 Accuracy: 8998/10000 (90%)\n",
      "Test set: Avg. loss: 0.3161 Accuracy: 9002/10000 (90%)\n",
      "Test set: Avg. loss: 0.3119 Accuracy: 9026/10000 (90%)\n",
      "Test set: Avg. loss: 0.3123 Accuracy: 9033/10000 (90%)\n",
      "Test set: Avg. loss: 0.3190 Accuracy: 9019/10000 (90%)\n",
      "Test set: Avg. loss: 0.3199 Accuracy: 9018/10000 (90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MyResNet().to(device)\n",
    "\n",
    "writer = SummaryWriter(\"MyResNet_mod_learning\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=4, min_lr=1e-6)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.75)\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_cifar10_loader,\n",
    "    val_loader=val_cifar10_loader,\n",
    "    n_epochs=350,\n",
    "    writer=writer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    logging_interval=100,\n",
    "    # run_name='cifar10_alex_dropout',\n",
    "    # model_description_dict={\n",
    "    #     'model_name': 'cifar10_alex_dropout',\n",
    "    #     'optimizer_initial_lr': 1e-4,\n",
    "    #     'lr_scheduler_type': 'StepLR',\n",
    "    #     'lr_scheduler_step_size': 15,\n",
    "    #     'lr_scheduler_gamma': 0.75,\n",
    "    # }\n",
    ")\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:01:13.388217200Z",
     "start_time": "2024-03-12T19:16:19.478695900Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pathlib\n",
    "path = pathlib.Path('./cifar10_own_resnet.bin')\n",
    "\n",
    "with open(path, \"wb\") as f:\n",
    "    torch.save(\n",
    "        obj=model.state_dict(),\n",
    "        f=f\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:01:32.261767600Z",
     "start_time": "2024-03-12T20:01:32.224161900Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ld = MyResNet()\n",
    "model_ld.load_state_dict(torch.load(path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:04:34.962950700Z",
     "start_time": "2024-03-12T20:04:34.901174100Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Avg. loss: 0.3135 Accuracy: 9032/10000 (90%)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.3134802177429199, 0.9032)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_epoch(\n",
    "    model_ld.to(device),\n",
    "    val_cifar10_loader,\n",
    "    nn.functional.cross_entropy,\n",
    "    writer=SummaryWriter(\"test\"),\n",
    "    epoch_no=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:07:49.277480900Z",
     "start_time": "2024-03-12T20:07:37.424630600Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "DENSE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Avg. loss: 2.3053 Accuracy: 1000/10000 (10%)\n",
      "Test set: Avg. loss: 1.6131 Accuracy: 4338/10000 (43%)\n",
      "Test set: Avg. loss: 1.4802 Accuracy: 4754/10000 (48%)\n",
      "Test set: Avg. loss: 1.4342 Accuracy: 4930/10000 (49%)\n",
      "Test set: Avg. loss: 1.3913 Accuracy: 5102/10000 (51%)\n",
      "Test set: Avg. loss: 1.3489 Accuracy: 5240/10000 (52%)\n",
      "Test set: Avg. loss: 1.3470 Accuracy: 5242/10000 (52%)\n",
      "Test set: Avg. loss: 1.3223 Accuracy: 5276/10000 (53%)\n",
      "Test set: Avg. loss: 1.3230 Accuracy: 5442/10000 (54%)\n",
      "Test set: Avg. loss: 1.3163 Accuracy: 5447/10000 (54%)\n",
      "Test set: Avg. loss: 1.3460 Accuracy: 5389/10000 (54%)\n",
      "Test set: Avg. loss: 1.3456 Accuracy: 5460/10000 (55%)\n",
      "Test set: Avg. loss: 1.3573 Accuracy: 5382/10000 (54%)\n",
      "Test set: Avg. loss: 1.3862 Accuracy: 5435/10000 (54%)\n",
      "Test set: Avg. loss: 1.3697 Accuracy: 5499/10000 (55%)\n",
      "Test set: Avg. loss: 1.4276 Accuracy: 5399/10000 (54%)\n",
      "Test set: Avg. loss: 1.4370 Accuracy: 5405/10000 (54%)\n",
      "Test set: Avg. loss: 1.4720 Accuracy: 5420/10000 (54%)\n",
      "Test set: Avg. loss: 1.5142 Accuracy: 5411/10000 (54%)\n",
      "Test set: Avg. loss: 1.5132 Accuracy: 5459/10000 (55%)\n",
      "Test set: Avg. loss: 1.5665 Accuracy: 5376/10000 (54%)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 32 * 3, 32 * 32),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32 * 32),\n",
    "    nn.Linear(32 * 32, 16 * 16),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16 * 16),\n",
    "    nn.Linear(16*16, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 10),\n",
    ").to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "    train_loader=train_cifar10_loader,\n",
    "    val_loader=val_cifar10_loader,\n",
    "    n_epochs=20,\n",
    "    writer=writer,\n",
    "    logging_interval=5,\n",
    "    model_name='cifar10_dense_deep'\n",
    ")\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T10:36:21.951166900Z",
     "start_time": "2024-03-09T10:28:47.937880300Z"
    }
   },
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomHorizontalFlip(p=0.2), \n",
    "    T.RandomVerticalFlip(p=0.2),\n",
    "    T.RandomRotation(30),\n",
    "    T.RandomGrayscale(p=0.1)]\n",
    ")\n",
    "\n",
    "transform_val = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "\n",
    "cifar10_train = load_cifar10(True, transform = transform_train)\n",
    "cifar10_val = load_cifar10(False, transform = transform_val)\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_cifar10_loader = DataLoader(\n",
    "    dataset=cifar10_train,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "val_cifar10_loader = torch.utils.data.DataLoader(\n",
    "    dataset=cifar10_val,\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T20:30:22.417276200Z",
     "start_time": "2024-03-09T20:30:21.142008100Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 26\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m      2\u001B[0m     nn\u001B[38;5;241m.\u001B[39mFlatten(),\n\u001B[0;32m      3\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m10\u001B[39m),\n\u001B[0;32m     22\u001B[0m )\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     24\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter()\n\u001B[1;32m---> 26\u001B[0m \u001B[43mtrain_val\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_cifar10_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_cifar10_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwriter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcifar10_dense_deep\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[0;32m     36\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m writer\u001B[38;5;241m.\u001B[39mflush()\n",
      "Cell \u001B[1;32mIn[7], line 18\u001B[0m, in \u001B[0;36mtrain_val\u001B[1;34m(network, n_epochs, criterion, optimizer, train_loader, val_loader, writer, logging_interval, model_name)\u001B[0m\n\u001B[0;32m     15\u001B[0m     writer\u001B[38;5;241m.\u001B[39madd_hparams(run_name\u001B[38;5;241m=\u001B[39mmodel_name, hparam_dict\u001B[38;5;241m=\u001B[39m{}, metric_dict\u001B[38;5;241m=\u001B[39m{})\n\u001B[0;32m     17\u001B[0m logger_step_cntr \u001B[38;5;241m=\u001B[39m count(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m val_loss, val_accur \u001B[38;5;241m=\u001B[39m \u001B[43mval_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_no\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     20\u001B[0m     train_epoch(network, train_loader, criterion, optimizer, writer, logging_interval, logger_step_cntr)\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 16\u001B[0m, in \u001B[0;36mval_epoch\u001B[1;34m(network, val_loader, criterion, writer, epoch_no)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m val_loader:\n\u001B[0;32m     15\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 16\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     val_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m criterion(logits, labels, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     18\u001B[0m     pred \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 32 * 3, 32 * 32),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32 * 32),\n",
    "    nn.Linear(32 * 32, 16 * 16),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16 * 16),\n",
    "    nn.Linear(16*16, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 10),\n",
    ").to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "    train_loader=train_cifar10_loader,\n",
    "    val_loader=val_cifar10_loader,\n",
    "    n_epochs=20,\n",
    "    writer=writer,\n",
    "    logging_interval=5,\n",
    ")\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T20:30:00.814770300Z",
     "start_time": "2024-03-09T20:29:49.822697200Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Test set: Avg. loss: 2.3065 Accuracy: 1000/10000 (10%)\n",
      "Test set: Avg. loss: 2.1761 Accuracy: 2035/10000 (20%)\n",
      "Test set: Avg. loss: 2.0076 Accuracy: 2804/10000 (28%)\n",
      "Test set: Avg. loss: 1.8989 Accuracy: 3203/10000 (32%)\n",
      "Test set: Avg. loss: 1.8230 Accuracy: 3552/10000 (36%)\n",
      "Test set: Avg. loss: 1.7662 Accuracy: 3739/10000 (37%)\n",
      "Test set: Avg. loss: 1.7322 Accuracy: 3899/10000 (39%)\n",
      "Test set: Avg. loss: 1.6866 Accuracy: 4015/10000 (40%)\n",
      "Test set: Avg. loss: 1.6646 Accuracy: 4141/10000 (41%)\n",
      "Test set: Avg. loss: 1.6416 Accuracy: 4224/10000 (42%)\n",
      "Test set: Avg. loss: 1.6187 Accuracy: 4329/10000 (43%)\n",
      "Test set: Avg. loss: 1.5924 Accuracy: 4399/10000 (44%)\n",
      "Test set: Avg. loss: 1.5797 Accuracy: 4461/10000 (45%)\n",
      "Test set: Avg. loss: 1.5501 Accuracy: 4494/10000 (45%)\n",
      "Test set: Avg. loss: 1.5502 Accuracy: 4555/10000 (46%)\n",
      "Test set: Avg. loss: 1.5299 Accuracy: 4635/10000 (46%)\n",
      "Test set: Avg. loss: 1.5179 Accuracy: 4668/10000 (47%)\n",
      "Test set: Avg. loss: 1.5034 Accuracy: 4721/10000 (47%)\n",
      "Test set: Avg. loss: 1.4945 Accuracy: 4754/10000 (48%)\n",
      "Test set: Avg. loss: 1.4887 Accuracy: 4739/10000 (47%)\n",
      "Test set: Avg. loss: 1.4716 Accuracy: 4847/10000 (48%)\n",
      "Test set: Avg. loss: 1.4590 Accuracy: 4882/10000 (49%)\n",
      "Test set: Avg. loss: 1.4431 Accuracy: 4922/10000 (49%)\n",
      "Test set: Avg. loss: 1.4425 Accuracy: 4879/10000 (49%)\n",
      "Test set: Avg. loss: 1.4287 Accuracy: 4917/10000 (49%)\n",
      "Test set: Avg. loss: 1.4092 Accuracy: 5041/10000 (50%)\n",
      "Test set: Avg. loss: 1.4011 Accuracy: 5030/10000 (50%)\n",
      "Test set: Avg. loss: 1.4026 Accuracy: 5052/10000 (51%)\n",
      "Test set: Avg. loss: 1.3856 Accuracy: 5139/10000 (51%)\n",
      "Test set: Avg. loss: 1.3897 Accuracy: 5061/10000 (51%)\n",
      "Test set: Avg. loss: 1.3892 Accuracy: 5134/10000 (51%)\n",
      "Test set: Avg. loss: 1.3793 Accuracy: 5183/10000 (52%)\n",
      "Test set: Avg. loss: 1.3720 Accuracy: 5143/10000 (51%)\n",
      "Test set: Avg. loss: 1.3732 Accuracy: 5178/10000 (52%)\n",
      "Test set: Avg. loss: 1.3613 Accuracy: 5198/10000 (52%)\n",
      "Test set: Avg. loss: 1.3535 Accuracy: 5216/10000 (52%)\n",
      "Test set: Avg. loss: 1.3486 Accuracy: 5240/10000 (52%)\n",
      "Test set: Avg. loss: 1.3429 Accuracy: 5273/10000 (53%)\n",
      "Test set: Avg. loss: 1.3428 Accuracy: 5255/10000 (53%)\n",
      "Test set: Avg. loss: 1.3406 Accuracy: 5262/10000 (53%)\n",
      "Test set: Avg. loss: 1.3358 Accuracy: 5241/10000 (52%)\n",
      "Test set: Avg. loss: 1.3300 Accuracy: 5300/10000 (53%)\n",
      "Test set: Avg. loss: 1.3353 Accuracy: 5302/10000 (53%)\n",
      "Test set: Avg. loss: 1.3211 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.3275 Accuracy: 5308/10000 (53%)\n",
      "Test set: Avg. loss: 1.3194 Accuracy: 5341/10000 (53%)\n",
      "Test set: Avg. loss: 1.3083 Accuracy: 5374/10000 (54%)\n",
      "Test set: Avg. loss: 1.3045 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.3083 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.3075 Accuracy: 5395/10000 (54%)\n",
      "Test set: Avg. loss: 1.3042 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.2939 Accuracy: 5447/10000 (54%)\n",
      "Test set: Avg. loss: 1.3065 Accuracy: 5407/10000 (54%)\n",
      "Test set: Avg. loss: 1.2920 Accuracy: 5400/10000 (54%)\n",
      "Test set: Avg. loss: 1.2914 Accuracy: 5430/10000 (54%)\n",
      "Test set: Avg. loss: 1.2828 Accuracy: 5447/10000 (54%)\n",
      "Test set: Avg. loss: 1.2918 Accuracy: 5447/10000 (54%)\n",
      "Test set: Avg. loss: 1.2818 Accuracy: 5446/10000 (54%)\n",
      "Test set: Avg. loss: 1.2823 Accuracy: 5454/10000 (55%)\n",
      "Test set: Avg. loss: 1.2754 Accuracy: 5461/10000 (55%)\n",
      "Test set: Avg. loss: 1.2856 Accuracy: 5409/10000 (54%)\n",
      "Test set: Avg. loss: 1.2712 Accuracy: 5537/10000 (55%)\n",
      "Test set: Avg. loss: 1.2700 Accuracy: 5449/10000 (54%)\n",
      "Test set: Avg. loss: 1.2709 Accuracy: 5494/10000 (55%)\n",
      "Test set: Avg. loss: 1.2726 Accuracy: 5482/10000 (55%)\n",
      "Test set: Avg. loss: 1.2660 Accuracy: 5481/10000 (55%)\n",
      "Test set: Avg. loss: 1.2681 Accuracy: 5448/10000 (54%)\n",
      "Test set: Avg. loss: 1.2686 Accuracy: 5506/10000 (55%)\n",
      "Test set: Avg. loss: 1.2687 Accuracy: 5514/10000 (55%)\n",
      "Test set: Avg. loss: 1.2596 Accuracy: 5550/10000 (56%)\n",
      "Test set: Avg. loss: 1.2529 Accuracy: 5537/10000 (55%)\n",
      "Test set: Avg. loss: 1.2576 Accuracy: 5512/10000 (55%)\n",
      "Test set: Avg. loss: 1.2538 Accuracy: 5559/10000 (56%)\n",
      "Test set: Avg. loss: 1.2523 Accuracy: 5578/10000 (56%)\n",
      "Test set: Avg. loss: 1.2618 Accuracy: 5561/10000 (56%)\n",
      "Test set: Avg. loss: 1.2436 Accuracy: 5599/10000 (56%)\n",
      "Test set: Avg. loss: 1.2526 Accuracy: 5568/10000 (56%)\n",
      "Test set: Avg. loss: 1.2516 Accuracy: 5597/10000 (56%)\n",
      "Test set: Avg. loss: 1.2418 Accuracy: 5618/10000 (56%)\n",
      "Test set: Avg. loss: 1.2487 Accuracy: 5597/10000 (56%)\n",
      "Test set: Avg. loss: 1.2459 Accuracy: 5590/10000 (56%)\n",
      "Test set: Avg. loss: 1.2428 Accuracy: 5617/10000 (56%)\n",
      "Test set: Avg. loss: 1.2404 Accuracy: 5598/10000 (56%)\n",
      "Test set: Avg. loss: 1.2368 Accuracy: 5637/10000 (56%)\n",
      "Test set: Avg. loss: 1.2401 Accuracy: 5616/10000 (56%)\n",
      "Test set: Avg. loss: 1.2307 Accuracy: 5622/10000 (56%)\n",
      "Test set: Avg. loss: 1.2284 Accuracy: 5647/10000 (56%)\n",
      "Test set: Avg. loss: 1.2381 Accuracy: 5643/10000 (56%)\n",
      "Test set: Avg. loss: 1.2380 Accuracy: 5616/10000 (56%)\n",
      "Test set: Avg. loss: 1.2386 Accuracy: 5585/10000 (56%)\n",
      "Test set: Avg. loss: 1.2328 Accuracy: 5606/10000 (56%)\n",
      "Test set: Avg. loss: 1.2324 Accuracy: 5709/10000 (57%)\n",
      "Test set: Avg. loss: 1.2374 Accuracy: 5630/10000 (56%)\n",
      "Test set: Avg. loss: 1.2329 Accuracy: 5665/10000 (57%)\n",
      "Test set: Avg. loss: 1.2335 Accuracy: 5636/10000 (56%)\n",
      "Test set: Avg. loss: 1.2268 Accuracy: 5657/10000 (57%)\n",
      "Test set: Avg. loss: 1.2216 Accuracy: 5693/10000 (57%)\n",
      "Test set: Avg. loss: 1.2202 Accuracy: 5718/10000 (57%)\n",
      "Test set: Avg. loss: 1.2311 Accuracy: 5678/10000 (57%)\n",
      "Test set: Avg. loss: 1.2265 Accuracy: 5681/10000 (57%)\n",
      "Test set: Avg. loss: 1.2317 Accuracy: 5625/10000 (56%)\n",
      "Test set: Avg. loss: 1.2311 Accuracy: 5628/10000 (56%)\n",
      "Test set: Avg. loss: 1.2231 Accuracy: 5670/10000 (57%)\n",
      "Test set: Avg. loss: 1.2201 Accuracy: 5651/10000 (57%)\n",
      "Test set: Avg. loss: 1.2068 Accuracy: 5721/10000 (57%)\n",
      "Test set: Avg. loss: 1.2134 Accuracy: 5692/10000 (57%)\n",
      "Test set: Avg. loss: 1.2119 Accuracy: 5713/10000 (57%)\n",
      "Test set: Avg. loss: 1.2110 Accuracy: 5718/10000 (57%)\n",
      "Test set: Avg. loss: 1.2109 Accuracy: 5703/10000 (57%)\n",
      "Test set: Avg. loss: 1.2203 Accuracy: 5674/10000 (57%)\n",
      "Test set: Avg. loss: 1.2121 Accuracy: 5659/10000 (57%)\n",
      "Test set: Avg. loss: 1.2133 Accuracy: 5622/10000 (56%)\n",
      "Test set: Avg. loss: 1.2071 Accuracy: 5679/10000 (57%)\n",
      "Test set: Avg. loss: 1.1986 Accuracy: 5755/10000 (58%)\n",
      "Test set: Avg. loss: 1.2044 Accuracy: 5714/10000 (57%)\n",
      "Test set: Avg. loss: 1.2071 Accuracy: 5720/10000 (57%)\n",
      "Test set: Avg. loss: 1.2070 Accuracy: 5747/10000 (57%)\n",
      "Test set: Avg. loss: 1.2079 Accuracy: 5709/10000 (57%)\n",
      "Test set: Avg. loss: 1.2119 Accuracy: 5703/10000 (57%)\n",
      "Test set: Avg. loss: 1.2049 Accuracy: 5751/10000 (58%)\n",
      "Test set: Avg. loss: 1.1996 Accuracy: 5742/10000 (57%)\n",
      "Test set: Avg. loss: 1.2127 Accuracy: 5714/10000 (57%)\n",
      "Test set: Avg. loss: 1.2043 Accuracy: 5721/10000 (57%)\n",
      "Test set: Avg. loss: 1.2068 Accuracy: 5728/10000 (57%)\n",
      "Test set: Avg. loss: 1.2054 Accuracy: 5748/10000 (57%)\n",
      "Test set: Avg. loss: 1.1909 Accuracy: 5781/10000 (58%)\n",
      "Test set: Avg. loss: 1.2007 Accuracy: 5785/10000 (58%)\n",
      "Test set: Avg. loss: 1.2030 Accuracy: 5727/10000 (57%)\n",
      "Test set: Avg. loss: 1.2004 Accuracy: 5788/10000 (58%)\n",
      "Test set: Avg. loss: 1.2033 Accuracy: 5751/10000 (58%)\n",
      "Test set: Avg. loss: 1.2027 Accuracy: 5725/10000 (57%)\n",
      "Test set: Avg. loss: 1.2084 Accuracy: 5740/10000 (57%)\n",
      "Test set: Avg. loss: 1.2034 Accuracy: 5756/10000 (58%)\n",
      "Test set: Avg. loss: 1.1962 Accuracy: 5795/10000 (58%)\n",
      "Test set: Avg. loss: 1.1938 Accuracy: 5810/10000 (58%)\n",
      "Test set: Avg. loss: 1.2073 Accuracy: 5742/10000 (57%)\n",
      "Test set: Avg. loss: 1.2031 Accuracy: 5733/10000 (57%)\n",
      "Test set: Avg. loss: 1.2026 Accuracy: 5741/10000 (57%)\n",
      "Test set: Avg. loss: 1.1950 Accuracy: 5771/10000 (58%)\n",
      "Test set: Avg. loss: 1.2038 Accuracy: 5744/10000 (57%)\n",
      "Test set: Avg. loss: 1.1909 Accuracy: 5797/10000 (58%)\n",
      "Test set: Avg. loss: 1.2040 Accuracy: 5771/10000 (58%)\n",
      "Test set: Avg. loss: 1.1876 Accuracy: 5772/10000 (58%)\n",
      "Test set: Avg. loss: 1.1920 Accuracy: 5820/10000 (58%)\n",
      "Test set: Avg. loss: 1.1975 Accuracy: 5789/10000 (58%)\n",
      "Test set: Avg. loss: 1.1939 Accuracy: 5726/10000 (57%)\n",
      "Test set: Avg. loss: 1.1938 Accuracy: 5775/10000 (58%)\n",
      "Test set: Avg. loss: 1.1871 Accuracy: 5826/10000 (58%)\n",
      "Test set: Avg. loss: 1.1971 Accuracy: 5792/10000 (58%)\n",
      "Test set: Avg. loss: 1.1896 Accuracy: 5792/10000 (58%)\n",
      "Test set: Avg. loss: 1.1955 Accuracy: 5758/10000 (58%)\n",
      "Test set: Avg. loss: 1.1935 Accuracy: 5777/10000 (58%)\n",
      "Test set: Avg. loss: 1.1986 Accuracy: 5775/10000 (58%)\n",
      "Test set: Avg. loss: 1.1933 Accuracy: 5736/10000 (57%)\n",
      "Test set: Avg. loss: 1.1952 Accuracy: 5807/10000 (58%)\n",
      "Test set: Avg. loss: 1.1945 Accuracy: 5793/10000 (58%)\n",
      "Test set: Avg. loss: 1.1984 Accuracy: 5746/10000 (57%)\n",
      "Test set: Avg. loss: 1.1958 Accuracy: 5759/10000 (58%)\n",
      "Test set: Avg. loss: 1.1938 Accuracy: 5760/10000 (58%)\n",
      "Test set: Avg. loss: 1.1868 Accuracy: 5760/10000 (58%)\n",
      "Test set: Avg. loss: 1.1957 Accuracy: 5778/10000 (58%)\n",
      "Test set: Avg. loss: 1.1906 Accuracy: 5810/10000 (58%)\n",
      "Test set: Avg. loss: 1.1897 Accuracy: 5755/10000 (58%)\n",
      "Test set: Avg. loss: 1.1791 Accuracy: 5877/10000 (59%)\n",
      "Test set: Avg. loss: 1.1873 Accuracy: 5812/10000 (58%)\n",
      "Test set: Avg. loss: 1.1960 Accuracy: 5789/10000 (58%)\n",
      "Test set: Avg. loss: 1.1884 Accuracy: 5825/10000 (58%)\n",
      "Test set: Avg. loss: 1.1916 Accuracy: 5814/10000 (58%)\n",
      "Test set: Avg. loss: 1.1840 Accuracy: 5860/10000 (59%)\n",
      "Test set: Avg. loss: 1.2061 Accuracy: 5801/10000 (58%)\n",
      "Test set: Avg. loss: 1.1802 Accuracy: 5888/10000 (59%)\n",
      "Test set: Avg. loss: 1.1861 Accuracy: 5832/10000 (58%)\n",
      "Test set: Avg. loss: 1.1840 Accuracy: 5830/10000 (58%)\n",
      "Test set: Avg. loss: 1.1885 Accuracy: 5878/10000 (59%)\n",
      "Test set: Avg. loss: 1.1867 Accuracy: 5875/10000 (59%)\n",
      "Test set: Avg. loss: 1.1933 Accuracy: 5821/10000 (58%)\n",
      "Test set: Avg. loss: 1.1911 Accuracy: 5836/10000 (58%)\n",
      "Test set: Avg. loss: 1.1845 Accuracy: 5837/10000 (58%)\n",
      "Test set: Avg. loss: 1.1831 Accuracy: 5850/10000 (58%)\n",
      "Test set: Avg. loss: 1.1861 Accuracy: 5832/10000 (58%)\n",
      "Test set: Avg. loss: 1.1866 Accuracy: 5834/10000 (58%)\n",
      "Test set: Avg. loss: 1.1924 Accuracy: 5873/10000 (59%)\n",
      "Test set: Avg. loss: 1.1856 Accuracy: 5826/10000 (58%)\n",
      "Test set: Avg. loss: 1.1979 Accuracy: 5797/10000 (58%)\n",
      "Test set: Avg. loss: 1.1847 Accuracy: 5833/10000 (58%)\n",
      "Test set: Avg. loss: 1.1809 Accuracy: 5888/10000 (59%)\n",
      "Test set: Avg. loss: 1.1948 Accuracy: 5847/10000 (58%)\n",
      "Test set: Avg. loss: 1.1842 Accuracy: 5843/10000 (58%)\n",
      "Test set: Avg. loss: 1.1779 Accuracy: 5892/10000 (59%)\n",
      "Test set: Avg. loss: 1.1875 Accuracy: 5825/10000 (58%)\n",
      "Test set: Avg. loss: 1.1954 Accuracy: 5822/10000 (58%)\n",
      "Test set: Avg. loss: 1.1982 Accuracy: 5839/10000 (58%)\n",
      "Test set: Avg. loss: 1.1901 Accuracy: 5851/10000 (59%)\n",
      "Test set: Avg. loss: 1.1895 Accuracy: 5856/10000 (59%)\n",
      "Test set: Avg. loss: 1.1973 Accuracy: 5825/10000 (58%)\n",
      "Test set: Avg. loss: 1.1892 Accuracy: 5854/10000 (59%)\n",
      "Test set: Avg. loss: 1.1844 Accuracy: 5877/10000 (59%)\n",
      "Test set: Avg. loss: 1.1961 Accuracy: 5880/10000 (59%)\n",
      "Test set: Avg. loss: 1.1828 Accuracy: 5857/10000 (59%)\n",
      "Test set: Avg. loss: 1.1979 Accuracy: 5832/10000 (58%)\n",
      "Test set: Avg. loss: 1.1871 Accuracy: 5854/10000 (59%)\n",
      "Test set: Avg. loss: 1.1833 Accuracy: 5900/10000 (59%)\n",
      "Test set: Avg. loss: 1.1858 Accuracy: 5879/10000 (59%)\n",
      "Test set: Avg. loss: 1.1769 Accuracy: 5948/10000 (59%)\n",
      "Test set: Avg. loss: 1.2044 Accuracy: 5817/10000 (58%)\n",
      "Test set: Avg. loss: 1.1985 Accuracy: 5858/10000 (59%)\n",
      "Test set: Avg. loss: 1.1994 Accuracy: 5844/10000 (58%)\n",
      "Test set: Avg. loss: 1.1859 Accuracy: 5866/10000 (59%)\n",
      "Test set: Avg. loss: 1.1791 Accuracy: 5919/10000 (59%)\n",
      "Test set: Avg. loss: 1.1750 Accuracy: 5914/10000 (59%)\n",
      "Test set: Avg. loss: 1.1811 Accuracy: 5888/10000 (59%)\n",
      "Test set: Avg. loss: 1.1895 Accuracy: 5915/10000 (59%)\n",
      "Test set: Avg. loss: 1.1957 Accuracy: 5839/10000 (58%)\n",
      "Test set: Avg. loss: 1.1974 Accuracy: 5853/10000 (59%)\n",
      "Test set: Avg. loss: 1.1866 Accuracy: 5905/10000 (59%)\n",
      "Test set: Avg. loss: 1.1773 Accuracy: 5953/10000 (60%)\n",
      "Test set: Avg. loss: 1.1890 Accuracy: 5857/10000 (59%)\n",
      "Test set: Avg. loss: 1.1843 Accuracy: 5912/10000 (59%)\n",
      "Test set: Avg. loss: 1.1885 Accuracy: 5853/10000 (59%)\n",
      "Test set: Avg. loss: 1.1930 Accuracy: 5815/10000 (58%)\n",
      "Test set: Avg. loss: 1.1846 Accuracy: 5888/10000 (59%)\n",
      "Test set: Avg. loss: 1.1924 Accuracy: 5856/10000 (59%)\n",
      "Test set: Avg. loss: 1.1899 Accuracy: 5851/10000 (59%)\n",
      "Test set: Avg. loss: 1.1854 Accuracy: 5819/10000 (58%)\n",
      "Test set: Avg. loss: 1.2028 Accuracy: 5830/10000 (58%)\n",
      "Test set: Avg. loss: 1.1965 Accuracy: 5834/10000 (58%)\n",
      "Test set: Avg. loss: 1.1849 Accuracy: 5888/10000 (59%)\n",
      "Test set: Avg. loss: 1.1842 Accuracy: 5900/10000 (59%)\n",
      "Test set: Avg. loss: 1.1998 Accuracy: 5853/10000 (59%)\n",
      "Test set: Avg. loss: 1.1839 Accuracy: 5842/10000 (58%)\n",
      "Test set: Avg. loss: 1.1863 Accuracy: 5878/10000 (59%)\n",
      "Test set: Avg. loss: 1.1780 Accuracy: 5866/10000 (59%)\n",
      "Test set: Avg. loss: 1.1862 Accuracy: 5877/10000 (59%)\n",
      "Test set: Avg. loss: 1.1880 Accuracy: 5851/10000 (59%)\n",
      "Test set: Avg. loss: 1.1867 Accuracy: 5865/10000 (59%)\n",
      "Test set: Avg. loss: 1.1967 Accuracy: 5854/10000 (59%)\n",
      "Test set: Avg. loss: 1.1841 Accuracy: 5893/10000 (59%)\n",
      "Test set: Avg. loss: 1.1903 Accuracy: 5878/10000 (59%)\n",
      "Test set: Avg. loss: 1.1984 Accuracy: 5831/10000 (58%)\n",
      "Test set: Avg. loss: 1.1783 Accuracy: 5870/10000 (59%)\n",
      "Test set: Avg. loss: 1.1840 Accuracy: 5901/10000 (59%)\n",
      "Test set: Avg. loss: 1.1912 Accuracy: 5850/10000 (58%)\n",
      "Test set: Avg. loss: 1.1894 Accuracy: 5865/10000 (59%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 42\u001B[0m\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m      3\u001B[0m     nn\u001B[38;5;241m.\u001B[39mFlatten(),\n\u001B[0;32m      4\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m32\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m10\u001B[39m),\n\u001B[0;32m     38\u001B[0m )\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     40\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter()\n\u001B[1;32m---> 42\u001B[0m \u001B[43mtrain_val\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     44\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_cifar10_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_cifar10_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m250\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwriter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcifar10_dense_deep\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[0;32m     52\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m writer\u001B[38;5;241m.\u001B[39mflush()\n",
      "Cell \u001B[1;32mIn[7], line 21\u001B[0m, in \u001B[0;36mtrain_val\u001B[1;34m(network, n_epochs, criterion, optimizer, train_loader, val_loader, writer, logging_interval, model_name)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     20\u001B[0m     train_epoch(network, train_loader, criterion, optimizer, writer, logging_interval, logger_step_cntr)\n\u001B[1;32m---> 21\u001B[0m     val_loss, val_accur \u001B[38;5;241m=\u001B[39m \u001B[43mval_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_no\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 14\u001B[0m, in \u001B[0;36mval_epoch\u001B[1;34m(network, val_loader, criterion, writer, epoch_no)\u001B[0m\n\u001B[0;32m     11\u001B[0m correct \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     13\u001B[0m network\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# switch network submodules to test mode\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m val_loader:\n\u001B[0;32m     15\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     16\u001B[0m     logits \u001B[38;5;241m=\u001B[39m network(images)\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 441\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[1;32m--> 388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m   1035\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[1;32m-> 1042\u001B[0m \u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[0;32m   1044\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[0;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    120\u001B[0m _cleanup()\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\context.py:336\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_win32\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     93\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(prep_data, to_child)\n\u001B[1;32m---> 94\u001B[0m     \u001B[43mreduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_child\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     96\u001B[0m     set_spawning_popen(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001B[0m, in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdump\u001B[39m(obj, file, protocol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m     \u001B[43mForkingPickler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 32 * 3, 32 * 32),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32 * 32),\n",
    "    nn.Linear(32 * 32, 16 * 16),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16 * 16),\n",
    "    nn.Linear(16*16, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 10),\n",
    ").to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.25 * 1e-4),\n",
    "    train_loader=train_cifar10_loader,\n",
    "    val_loader=val_cifar10_loader,\n",
    "    n_epochs=250,\n",
    "    writer=writer,\n",
    "    logging_interval=20,\n",
    "    model_name='cifar10_dense_deep'\n",
    ")\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T22:07:53.653581700Z",
     "start_time": "2024-03-09T20:30:25.319246600Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size_train = 1024\n",
    "batch_size_test = 10_000\n",
    "\n",
    "train_cifar10_loader = DataLoader(\n",
    "    dataset=cifar10_train,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "val_cifar10_loader = torch.utils.data.DataLoader(\n",
    "    dataset=cifar10_val,\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T22:10:06.182500Z",
     "start_time": "2024-03-09T22:10:06.178986900Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Test set: Avg. loss: 2.3063 Accuracy: 1000/10000 (10%)\n",
      "Test set: Avg. loss: 2.3831 Accuracy: 1159/10000 (12%)\n",
      "Test set: Avg. loss: 2.3601 Accuracy: 1292/10000 (13%)\n",
      "Test set: Avg. loss: 2.3314 Accuracy: 1320/10000 (13%)\n",
      "Test set: Avg. loss: 2.3104 Accuracy: 1460/10000 (15%)\n",
      "Test set: Avg. loss: 2.2940 Accuracy: 1594/10000 (16%)\n",
      "Test set: Avg. loss: 2.2749 Accuracy: 1633/10000 (16%)\n",
      "Test set: Avg. loss: 2.2607 Accuracy: 1751/10000 (18%)\n",
      "Test set: Avg. loss: 2.2372 Accuracy: 1809/10000 (18%)\n",
      "Test set: Avg. loss: 2.2232 Accuracy: 1948/10000 (19%)\n",
      "Test set: Avg. loss: 2.2032 Accuracy: 2064/10000 (21%)\n",
      "Test set: Avg. loss: 2.1863 Accuracy: 2108/10000 (21%)\n",
      "Test set: Avg. loss: 2.1681 Accuracy: 2218/10000 (22%)\n",
      "Test set: Avg. loss: 2.1479 Accuracy: 2277/10000 (23%)\n",
      "Test set: Avg. loss: 2.1293 Accuracy: 2398/10000 (24%)\n",
      "Test set: Avg. loss: 2.1170 Accuracy: 2438/10000 (24%)\n",
      "Test set: Avg. loss: 2.0994 Accuracy: 2492/10000 (25%)\n",
      "Test set: Avg. loss: 2.0822 Accuracy: 2614/10000 (26%)\n",
      "Test set: Avg. loss: 2.0637 Accuracy: 2692/10000 (27%)\n",
      "Test set: Avg. loss: 2.0533 Accuracy: 2705/10000 (27%)\n",
      "Test set: Avg. loss: 2.0463 Accuracy: 2724/10000 (27%)\n",
      "Test set: Avg. loss: 2.0290 Accuracy: 2789/10000 (28%)\n",
      "Test set: Avg. loss: 2.0208 Accuracy: 2885/10000 (29%)\n",
      "Test set: Avg. loss: 2.0103 Accuracy: 2935/10000 (29%)\n",
      "Test set: Avg. loss: 1.9965 Accuracy: 3002/10000 (30%)\n",
      "Test set: Avg. loss: 1.9795 Accuracy: 3039/10000 (30%)\n",
      "Test set: Avg. loss: 1.9644 Accuracy: 3069/10000 (31%)\n",
      "Test set: Avg. loss: 1.9520 Accuracy: 3152/10000 (32%)\n",
      "Test set: Avg. loss: 1.9490 Accuracy: 3185/10000 (32%)\n",
      "Test set: Avg. loss: 1.9339 Accuracy: 3218/10000 (32%)\n",
      "Test set: Avg. loss: 1.9201 Accuracy: 3341/10000 (33%)\n",
      "Test set: Avg. loss: 1.9095 Accuracy: 3347/10000 (33%)\n",
      "Test set: Avg. loss: 1.9029 Accuracy: 3315/10000 (33%)\n",
      "Test set: Avg. loss: 1.8900 Accuracy: 3412/10000 (34%)\n",
      "Test set: Avg. loss: 1.8795 Accuracy: 3422/10000 (34%)\n",
      "Test set: Avg. loss: 1.8718 Accuracy: 3432/10000 (34%)\n",
      "Test set: Avg. loss: 1.8644 Accuracy: 3482/10000 (35%)\n",
      "Test set: Avg. loss: 1.8508 Accuracy: 3521/10000 (35%)\n",
      "Test set: Avg. loss: 1.8424 Accuracy: 3528/10000 (35%)\n",
      "Test set: Avg. loss: 1.8295 Accuracy: 3592/10000 (36%)\n",
      "Test set: Avg. loss: 1.8229 Accuracy: 3658/10000 (37%)\n",
      "Test set: Avg. loss: 1.8143 Accuracy: 3685/10000 (37%)\n",
      "Test set: Avg. loss: 1.8097 Accuracy: 3720/10000 (37%)\n",
      "Test set: Avg. loss: 1.8034 Accuracy: 3741/10000 (37%)\n",
      "Test set: Avg. loss: 1.7937 Accuracy: 3761/10000 (38%)\n",
      "Test set: Avg. loss: 1.7892 Accuracy: 3747/10000 (37%)\n",
      "Test set: Avg. loss: 1.7792 Accuracy: 3778/10000 (38%)\n",
      "Test set: Avg. loss: 1.7700 Accuracy: 3851/10000 (39%)\n",
      "Test set: Avg. loss: 1.7669 Accuracy: 3868/10000 (39%)\n",
      "Test set: Avg. loss: 1.7531 Accuracy: 3846/10000 (38%)\n",
      "Test set: Avg. loss: 1.7503 Accuracy: 3890/10000 (39%)\n",
      "Test set: Avg. loss: 1.7476 Accuracy: 3877/10000 (39%)\n",
      "Test set: Avg. loss: 1.7458 Accuracy: 3928/10000 (39%)\n",
      "Test set: Avg. loss: 1.7361 Accuracy: 3958/10000 (40%)\n",
      "Test set: Avg. loss: 1.7293 Accuracy: 3984/10000 (40%)\n",
      "Test set: Avg. loss: 1.7272 Accuracy: 4001/10000 (40%)\n",
      "Test set: Avg. loss: 1.7164 Accuracy: 4031/10000 (40%)\n",
      "Test set: Avg. loss: 1.7144 Accuracy: 4023/10000 (40%)\n",
      "Test set: Avg. loss: 1.7076 Accuracy: 4053/10000 (41%)\n",
      "Test set: Avg. loss: 1.6980 Accuracy: 4093/10000 (41%)\n",
      "Test set: Avg. loss: 1.6945 Accuracy: 4129/10000 (41%)\n",
      "Test set: Avg. loss: 1.6914 Accuracy: 4128/10000 (41%)\n",
      "Test set: Avg. loss: 1.6854 Accuracy: 4140/10000 (41%)\n",
      "Test set: Avg. loss: 1.6837 Accuracy: 4146/10000 (41%)\n",
      "Test set: Avg. loss: 1.6832 Accuracy: 4204/10000 (42%)\n",
      "Test set: Avg. loss: 1.6733 Accuracy: 4181/10000 (42%)\n",
      "Test set: Avg. loss: 1.6713 Accuracy: 4165/10000 (42%)\n",
      "Test set: Avg. loss: 1.6666 Accuracy: 4187/10000 (42%)\n",
      "Test set: Avg. loss: 1.6645 Accuracy: 4224/10000 (42%)\n",
      "Test set: Avg. loss: 1.6505 Accuracy: 4283/10000 (43%)\n",
      "Test set: Avg. loss: 1.6582 Accuracy: 4268/10000 (43%)\n",
      "Test set: Avg. loss: 1.6568 Accuracy: 4272/10000 (43%)\n",
      "Test set: Avg. loss: 1.6547 Accuracy: 4256/10000 (43%)\n",
      "Test set: Avg. loss: 1.6411 Accuracy: 4301/10000 (43%)\n",
      "Test set: Avg. loss: 1.6463 Accuracy: 4305/10000 (43%)\n",
      "Test set: Avg. loss: 1.6366 Accuracy: 4325/10000 (43%)\n",
      "Test set: Avg. loss: 1.6381 Accuracy: 4341/10000 (43%)\n",
      "Test set: Avg. loss: 1.6284 Accuracy: 4340/10000 (43%)\n",
      "Test set: Avg. loss: 1.6232 Accuracy: 4361/10000 (44%)\n",
      "Test set: Avg. loss: 1.6232 Accuracy: 4348/10000 (43%)\n",
      "Test set: Avg. loss: 1.6317 Accuracy: 4312/10000 (43%)\n",
      "Test set: Avg. loss: 1.6145 Accuracy: 4363/10000 (44%)\n",
      "Test set: Avg. loss: 1.6233 Accuracy: 4363/10000 (44%)\n",
      "Test set: Avg. loss: 1.6143 Accuracy: 4369/10000 (44%)\n",
      "Test set: Avg. loss: 1.6087 Accuracy: 4357/10000 (44%)\n",
      "Test set: Avg. loss: 1.6055 Accuracy: 4385/10000 (44%)\n",
      "Test set: Avg. loss: 1.5997 Accuracy: 4410/10000 (44%)\n",
      "Test set: Avg. loss: 1.5942 Accuracy: 4454/10000 (45%)\n",
      "Test set: Avg. loss: 1.5995 Accuracy: 4407/10000 (44%)\n",
      "Test set: Avg. loss: 1.5987 Accuracy: 4415/10000 (44%)\n",
      "Test set: Avg. loss: 1.5998 Accuracy: 4432/10000 (44%)\n",
      "Test set: Avg. loss: 1.5796 Accuracy: 4474/10000 (45%)\n",
      "Test set: Avg. loss: 1.5898 Accuracy: 4427/10000 (44%)\n",
      "Test set: Avg. loss: 1.5892 Accuracy: 4448/10000 (44%)\n",
      "Test set: Avg. loss: 1.5868 Accuracy: 4429/10000 (44%)\n",
      "Test set: Avg. loss: 1.5826 Accuracy: 4471/10000 (45%)\n",
      "Test set: Avg. loss: 1.5922 Accuracy: 4451/10000 (45%)\n",
      "Test set: Avg. loss: 1.5768 Accuracy: 4482/10000 (45%)\n",
      "Test set: Avg. loss: 1.5726 Accuracy: 4501/10000 (45%)\n",
      "Test set: Avg. loss: 1.5773 Accuracy: 4461/10000 (45%)\n",
      "Test set: Avg. loss: 1.5696 Accuracy: 4462/10000 (45%)\n",
      "Test set: Avg. loss: 1.5781 Accuracy: 4539/10000 (45%)\n",
      "Test set: Avg. loss: 1.5719 Accuracy: 4488/10000 (45%)\n",
      "Test set: Avg. loss: 1.5621 Accuracy: 4538/10000 (45%)\n",
      "Test set: Avg. loss: 1.5592 Accuracy: 4538/10000 (45%)\n",
      "Test set: Avg. loss: 1.5605 Accuracy: 4548/10000 (45%)\n",
      "Test set: Avg. loss: 1.5497 Accuracy: 4580/10000 (46%)\n",
      "Test set: Avg. loss: 1.5511 Accuracy: 4560/10000 (46%)\n",
      "Test set: Avg. loss: 1.5414 Accuracy: 4605/10000 (46%)\n",
      "Test set: Avg. loss: 1.5396 Accuracy: 4637/10000 (46%)\n",
      "Test set: Avg. loss: 1.5485 Accuracy: 4609/10000 (46%)\n",
      "Test set: Avg. loss: 1.5380 Accuracy: 4628/10000 (46%)\n",
      "Test set: Avg. loss: 1.5281 Accuracy: 4679/10000 (47%)\n",
      "Test set: Avg. loss: 1.5439 Accuracy: 4625/10000 (46%)\n",
      "Test set: Avg. loss: 1.5367 Accuracy: 4635/10000 (46%)\n",
      "Test set: Avg. loss: 1.5322 Accuracy: 4657/10000 (47%)\n",
      "Test set: Avg. loss: 1.5325 Accuracy: 4673/10000 (47%)\n",
      "Test set: Avg. loss: 1.5289 Accuracy: 4659/10000 (47%)\n",
      "Test set: Avg. loss: 1.5403 Accuracy: 4627/10000 (46%)\n",
      "Test set: Avg. loss: 1.5196 Accuracy: 4709/10000 (47%)\n",
      "Test set: Avg. loss: 1.5134 Accuracy: 4727/10000 (47%)\n",
      "Test set: Avg. loss: 1.5275 Accuracy: 4710/10000 (47%)\n",
      "Test set: Avg. loss: 1.5221 Accuracy: 4687/10000 (47%)\n",
      "Test set: Avg. loss: 1.5259 Accuracy: 4721/10000 (47%)\n",
      "Test set: Avg. loss: 1.5199 Accuracy: 4677/10000 (47%)\n",
      "Test set: Avg. loss: 1.5142 Accuracy: 4689/10000 (47%)\n",
      "Test set: Avg. loss: 1.4995 Accuracy: 4733/10000 (47%)\n",
      "Test set: Avg. loss: 1.4987 Accuracy: 4745/10000 (47%)\n",
      "Test set: Avg. loss: 1.5063 Accuracy: 4747/10000 (47%)\n",
      "Test set: Avg. loss: 1.4997 Accuracy: 4713/10000 (47%)\n",
      "Test set: Avg. loss: 1.5134 Accuracy: 4760/10000 (48%)\n",
      "Test set: Avg. loss: 1.5032 Accuracy: 4746/10000 (47%)\n",
      "Test set: Avg. loss: 1.5133 Accuracy: 4723/10000 (47%)\n",
      "Test set: Avg. loss: 1.4967 Accuracy: 4774/10000 (48%)\n",
      "Test set: Avg. loss: 1.5001 Accuracy: 4824/10000 (48%)\n",
      "Test set: Avg. loss: 1.4891 Accuracy: 4809/10000 (48%)\n",
      "Test set: Avg. loss: 1.4853 Accuracy: 4849/10000 (48%)\n",
      "Test set: Avg. loss: 1.4914 Accuracy: 4783/10000 (48%)\n",
      "Test set: Avg. loss: 1.4881 Accuracy: 4797/10000 (48%)\n",
      "Test set: Avg. loss: 1.4834 Accuracy: 4830/10000 (48%)\n",
      "Test set: Avg. loss: 1.4909 Accuracy: 4853/10000 (49%)\n",
      "Test set: Avg. loss: 1.4806 Accuracy: 4866/10000 (49%)\n",
      "Test set: Avg. loss: 1.4910 Accuracy: 4855/10000 (49%)\n",
      "Test set: Avg. loss: 1.4817 Accuracy: 4886/10000 (49%)\n",
      "Test set: Avg. loss: 1.4921 Accuracy: 4854/10000 (49%)\n",
      "Test set: Avg. loss: 1.4745 Accuracy: 4844/10000 (48%)\n",
      "Test set: Avg. loss: 1.4831 Accuracy: 4859/10000 (49%)\n",
      "Test set: Avg. loss: 1.4824 Accuracy: 4859/10000 (49%)\n",
      "Test set: Avg. loss: 1.4755 Accuracy: 4871/10000 (49%)\n",
      "Test set: Avg. loss: 1.4715 Accuracy: 4895/10000 (49%)\n",
      "Test set: Avg. loss: 1.4765 Accuracy: 4889/10000 (49%)\n",
      "Test set: Avg. loss: 1.4748 Accuracy: 4848/10000 (48%)\n",
      "Test set: Avg. loss: 1.4772 Accuracy: 4871/10000 (49%)\n",
      "Test set: Avg. loss: 1.4700 Accuracy: 4866/10000 (49%)\n",
      "Test set: Avg. loss: 1.4785 Accuracy: 4856/10000 (49%)\n",
      "Test set: Avg. loss: 1.4738 Accuracy: 4838/10000 (48%)\n",
      "Test set: Avg. loss: 1.4680 Accuracy: 4883/10000 (49%)\n",
      "Test set: Avg. loss: 1.4626 Accuracy: 4929/10000 (49%)\n",
      "Test set: Avg. loss: 1.4648 Accuracy: 4908/10000 (49%)\n",
      "Test set: Avg. loss: 1.4499 Accuracy: 4935/10000 (49%)\n",
      "Test set: Avg. loss: 1.4553 Accuracy: 4921/10000 (49%)\n",
      "Test set: Avg. loss: 1.4488 Accuracy: 4957/10000 (50%)\n",
      "Test set: Avg. loss: 1.4487 Accuracy: 4969/10000 (50%)\n",
      "Test set: Avg. loss: 1.4559 Accuracy: 4926/10000 (49%)\n",
      "Test set: Avg. loss: 1.4568 Accuracy: 4952/10000 (50%)\n",
      "Test set: Avg. loss: 1.4467 Accuracy: 4945/10000 (49%)\n",
      "Test set: Avg. loss: 1.4672 Accuracy: 4946/10000 (49%)\n",
      "Test set: Avg. loss: 1.4405 Accuracy: 4975/10000 (50%)\n",
      "Test set: Avg. loss: 1.4443 Accuracy: 4968/10000 (50%)\n",
      "Test set: Avg. loss: 1.4440 Accuracy: 4966/10000 (50%)\n",
      "Test set: Avg. loss: 1.4595 Accuracy: 4941/10000 (49%)\n",
      "Test set: Avg. loss: 1.4479 Accuracy: 4903/10000 (49%)\n",
      "Test set: Avg. loss: 1.4305 Accuracy: 4986/10000 (50%)\n",
      "Test set: Avg. loss: 1.4354 Accuracy: 4980/10000 (50%)\n",
      "Test set: Avg. loss: 1.4507 Accuracy: 4940/10000 (49%)\n",
      "Test set: Avg. loss: 1.4412 Accuracy: 4963/10000 (50%)\n",
      "Test set: Avg. loss: 1.4519 Accuracy: 4942/10000 (49%)\n",
      "Test set: Avg. loss: 1.4452 Accuracy: 5007/10000 (50%)\n",
      "Test set: Avg. loss: 1.4617 Accuracy: 4908/10000 (49%)\n",
      "Test set: Avg. loss: 1.4424 Accuracy: 4971/10000 (50%)\n",
      "Test set: Avg. loss: 1.4512 Accuracy: 4952/10000 (50%)\n",
      "Test set: Avg. loss: 1.4494 Accuracy: 4961/10000 (50%)\n",
      "Test set: Avg. loss: 1.4479 Accuracy: 5010/10000 (50%)\n",
      "Test set: Avg. loss: 1.4417 Accuracy: 4975/10000 (50%)\n",
      "Test set: Avg. loss: 1.4369 Accuracy: 5028/10000 (50%)\n",
      "Test set: Avg. loss: 1.4446 Accuracy: 4949/10000 (49%)\n",
      "Test set: Avg. loss: 1.4324 Accuracy: 4990/10000 (50%)\n",
      "Test set: Avg. loss: 1.4433 Accuracy: 5012/10000 (50%)\n",
      "Test set: Avg. loss: 1.4395 Accuracy: 4983/10000 (50%)\n",
      "Test set: Avg. loss: 1.4406 Accuracy: 4982/10000 (50%)\n",
      "Test set: Avg. loss: 1.4420 Accuracy: 4984/10000 (50%)\n",
      "Test set: Avg. loss: 1.4344 Accuracy: 5017/10000 (50%)\n",
      "Test set: Avg. loss: 1.4396 Accuracy: 5002/10000 (50%)\n",
      "Test set: Avg. loss: 1.4446 Accuracy: 5007/10000 (50%)\n",
      "Test set: Avg. loss: 1.4163 Accuracy: 5069/10000 (51%)\n",
      "Test set: Avg. loss: 1.4288 Accuracy: 5040/10000 (50%)\n",
      "Test set: Avg. loss: 1.4391 Accuracy: 5023/10000 (50%)\n",
      "Test set: Avg. loss: 1.4224 Accuracy: 5049/10000 (50%)\n",
      "Test set: Avg. loss: 1.4267 Accuracy: 4995/10000 (50%)\n",
      "Test set: Avg. loss: 1.4246 Accuracy: 5111/10000 (51%)\n",
      "Test set: Avg. loss: 1.4311 Accuracy: 4992/10000 (50%)\n",
      "Test set: Avg. loss: 1.4221 Accuracy: 5092/10000 (51%)\n",
      "Test set: Avg. loss: 1.4406 Accuracy: 5030/10000 (50%)\n",
      "Test set: Avg. loss: 1.4355 Accuracy: 5028/10000 (50%)\n",
      "Test set: Avg. loss: 1.4248 Accuracy: 5010/10000 (50%)\n",
      "Test set: Avg. loss: 1.4180 Accuracy: 5067/10000 (51%)\n",
      "Test set: Avg. loss: 1.4139 Accuracy: 5115/10000 (51%)\n",
      "Test set: Avg. loss: 1.4244 Accuracy: 5039/10000 (50%)\n",
      "Test set: Avg. loss: 1.4310 Accuracy: 5068/10000 (51%)\n",
      "Test set: Avg. loss: 1.4365 Accuracy: 5014/10000 (50%)\n",
      "Test set: Avg. loss: 1.4163 Accuracy: 5083/10000 (51%)\n",
      "Test set: Avg. loss: 1.4135 Accuracy: 5089/10000 (51%)\n",
      "Test set: Avg. loss: 1.4202 Accuracy: 5057/10000 (51%)\n",
      "Test set: Avg. loss: 1.4227 Accuracy: 5039/10000 (50%)\n",
      "Test set: Avg. loss: 1.4126 Accuracy: 5132/10000 (51%)\n",
      "Test set: Avg. loss: 1.4211 Accuracy: 5076/10000 (51%)\n",
      "Test set: Avg. loss: 1.4155 Accuracy: 5072/10000 (51%)\n",
      "Test set: Avg. loss: 1.4284 Accuracy: 5047/10000 (50%)\n",
      "Test set: Avg. loss: 1.4173 Accuracy: 5074/10000 (51%)\n",
      "Test set: Avg. loss: 1.4256 Accuracy: 5084/10000 (51%)\n",
      "Test set: Avg. loss: 1.4107 Accuracy: 5104/10000 (51%)\n",
      "Test set: Avg. loss: 1.4150 Accuracy: 5113/10000 (51%)\n",
      "Test set: Avg. loss: 1.4052 Accuracy: 5133/10000 (51%)\n",
      "Test set: Avg. loss: 1.4118 Accuracy: 5133/10000 (51%)\n",
      "Test set: Avg. loss: 1.4158 Accuracy: 5093/10000 (51%)\n",
      "Test set: Avg. loss: 1.4062 Accuracy: 5118/10000 (51%)\n",
      "Test set: Avg. loss: 1.4164 Accuracy: 5093/10000 (51%)\n",
      "Test set: Avg. loss: 1.4091 Accuracy: 5096/10000 (51%)\n",
      "Test set: Avg. loss: 1.3972 Accuracy: 5125/10000 (51%)\n",
      "Test set: Avg. loss: 1.4056 Accuracy: 5089/10000 (51%)\n",
      "Test set: Avg. loss: 1.4267 Accuracy: 5109/10000 (51%)\n",
      "Test set: Avg. loss: 1.4130 Accuracy: 5132/10000 (51%)\n",
      "Test set: Avg. loss: 1.4001 Accuracy: 5104/10000 (51%)\n",
      "Test set: Avg. loss: 1.4083 Accuracy: 5139/10000 (51%)\n",
      "Test set: Avg. loss: 1.4084 Accuracy: 5150/10000 (52%)\n",
      "Test set: Avg. loss: 1.4138 Accuracy: 5094/10000 (51%)\n",
      "Test set: Avg. loss: 1.4142 Accuracy: 5101/10000 (51%)\n",
      "Test set: Avg. loss: 1.4022 Accuracy: 5164/10000 (52%)\n",
      "Test set: Avg. loss: 1.4034 Accuracy: 5145/10000 (51%)\n",
      "Test set: Avg. loss: 1.3968 Accuracy: 5206/10000 (52%)\n",
      "Test set: Avg. loss: 1.4037 Accuracy: 5133/10000 (51%)\n",
      "Test set: Avg. loss: 1.4153 Accuracy: 5075/10000 (51%)\n",
      "Test set: Avg. loss: 1.4153 Accuracy: 5113/10000 (51%)\n",
      "Test set: Avg. loss: 1.3958 Accuracy: 5122/10000 (51%)\n",
      "Test set: Avg. loss: 1.4010 Accuracy: 5085/10000 (51%)\n",
      "Test set: Avg. loss: 1.3892 Accuracy: 5123/10000 (51%)\n",
      "Test set: Avg. loss: 1.4130 Accuracy: 5119/10000 (51%)\n",
      "Test set: Avg. loss: 1.3900 Accuracy: 5115/10000 (51%)\n",
      "Test set: Avg. loss: 1.4190 Accuracy: 5084/10000 (51%)\n",
      "Test set: Avg. loss: 1.4004 Accuracy: 5129/10000 (51%)\n",
      "Test set: Avg. loss: 1.3961 Accuracy: 5140/10000 (51%)\n",
      "Test set: Avg. loss: 1.3957 Accuracy: 5140/10000 (51%)\n",
      "Test set: Avg. loss: 1.3940 Accuracy: 5175/10000 (52%)\n",
      "Test set: Avg. loss: 1.4078 Accuracy: 5128/10000 (51%)\n",
      "Test set: Avg. loss: 1.4186 Accuracy: 5114/10000 (51%)\n",
      "Test set: Avg. loss: 1.4002 Accuracy: 5142/10000 (51%)\n",
      "Test set: Avg. loss: 1.4087 Accuracy: 5086/10000 (51%)\n",
      "Test set: Avg. loss: 1.4135 Accuracy: 5135/10000 (51%)\n",
      "Test set: Avg. loss: 1.3988 Accuracy: 5112/10000 (51%)\n",
      "Test set: Avg. loss: 1.3916 Accuracy: 5140/10000 (51%)\n",
      "Test set: Avg. loss: 1.3891 Accuracy: 5174/10000 (52%)\n",
      "Test set: Avg. loss: 1.3908 Accuracy: 5171/10000 (52%)\n",
      "Test set: Avg. loss: 1.3946 Accuracy: 5196/10000 (52%)\n",
      "Test set: Avg. loss: 1.3925 Accuracy: 5170/10000 (52%)\n",
      "Test set: Avg. loss: 1.3906 Accuracy: 5160/10000 (52%)\n",
      "Test set: Avg. loss: 1.4105 Accuracy: 5141/10000 (51%)\n",
      "Test set: Avg. loss: 1.4009 Accuracy: 5180/10000 (52%)\n",
      "Test set: Avg. loss: 1.4125 Accuracy: 5145/10000 (51%)\n",
      "Test set: Avg. loss: 1.3995 Accuracy: 5168/10000 (52%)\n",
      "Test set: Avg. loss: 1.3913 Accuracy: 5163/10000 (52%)\n",
      "Test set: Avg. loss: 1.3982 Accuracy: 5186/10000 (52%)\n",
      "Test set: Avg. loss: 1.3978 Accuracy: 5177/10000 (52%)\n",
      "Test set: Avg. loss: 1.4138 Accuracy: 5152/10000 (52%)\n",
      "Test set: Avg. loss: 1.3912 Accuracy: 5186/10000 (52%)\n",
      "Test set: Avg. loss: 1.4069 Accuracy: 5167/10000 (52%)\n",
      "Test set: Avg. loss: 1.3910 Accuracy: 5211/10000 (52%)\n",
      "Test set: Avg. loss: 1.3902 Accuracy: 5196/10000 (52%)\n",
      "Test set: Avg. loss: 1.3942 Accuracy: 5210/10000 (52%)\n",
      "Test set: Avg. loss: 1.3976 Accuracy: 5172/10000 (52%)\n",
      "Test set: Avg. loss: 1.3929 Accuracy: 5157/10000 (52%)\n",
      "Test set: Avg. loss: 1.4106 Accuracy: 5165/10000 (52%)\n",
      "Test set: Avg. loss: 1.3970 Accuracy: 5179/10000 (52%)\n",
      "Test set: Avg. loss: 1.3883 Accuracy: 5167/10000 (52%)\n",
      "Test set: Avg. loss: 1.3882 Accuracy: 5211/10000 (52%)\n",
      "Test set: Avg. loss: 1.3991 Accuracy: 5152/10000 (52%)\n",
      "Test set: Avg. loss: 1.4008 Accuracy: 5159/10000 (52%)\n",
      "Test set: Avg. loss: 1.3921 Accuracy: 5184/10000 (52%)\n",
      "Test set: Avg. loss: 1.3980 Accuracy: 5158/10000 (52%)\n",
      "Test set: Avg. loss: 1.4004 Accuracy: 5174/10000 (52%)\n",
      "Test set: Avg. loss: 1.3996 Accuracy: 5159/10000 (52%)\n",
      "Test set: Avg. loss: 1.3885 Accuracy: 5225/10000 (52%)\n",
      "Test set: Avg. loss: 1.4004 Accuracy: 5167/10000 (52%)\n",
      "Test set: Avg. loss: 1.3935 Accuracy: 5245/10000 (52%)\n",
      "Test set: Avg. loss: 1.3860 Accuracy: 5202/10000 (52%)\n",
      "Test set: Avg. loss: 1.4013 Accuracy: 5165/10000 (52%)\n",
      "Test set: Avg. loss: 1.4125 Accuracy: 5164/10000 (52%)\n",
      "Test set: Avg. loss: 1.4002 Accuracy: 5189/10000 (52%)\n",
      "Test set: Avg. loss: 1.3900 Accuracy: 5209/10000 (52%)\n",
      "Test set: Avg. loss: 1.3983 Accuracy: 5150/10000 (52%)\n",
      "Test set: Avg. loss: 1.3907 Accuracy: 5201/10000 (52%)\n",
      "Test set: Avg. loss: 1.3983 Accuracy: 5204/10000 (52%)\n",
      "Test set: Avg. loss: 1.3896 Accuracy: 5189/10000 (52%)\n",
      "Test set: Avg. loss: 1.3931 Accuracy: 5190/10000 (52%)\n",
      "Test set: Avg. loss: 1.3823 Accuracy: 5216/10000 (52%)\n",
      "Test set: Avg. loss: 1.3997 Accuracy: 5181/10000 (52%)\n",
      "Test set: Avg. loss: 1.3900 Accuracy: 5219/10000 (52%)\n",
      "Test set: Avg. loss: 1.3791 Accuracy: 5250/10000 (52%)\n",
      "Test set: Avg. loss: 1.4046 Accuracy: 5176/10000 (52%)\n",
      "Test set: Avg. loss: 1.3890 Accuracy: 5205/10000 (52%)\n",
      "Test set: Avg. loss: 1.3962 Accuracy: 5198/10000 (52%)\n",
      "Test set: Avg. loss: 1.3912 Accuracy: 5201/10000 (52%)\n",
      "Test set: Avg. loss: 1.3837 Accuracy: 5256/10000 (53%)\n",
      "Test set: Avg. loss: 1.3910 Accuracy: 5257/10000 (53%)\n",
      "Test set: Avg. loss: 1.3801 Accuracy: 5222/10000 (52%)\n",
      "Test set: Avg. loss: 1.3892 Accuracy: 5194/10000 (52%)\n",
      "Test set: Avg. loss: 1.3755 Accuracy: 5232/10000 (52%)\n",
      "Test set: Avg. loss: 1.3788 Accuracy: 5215/10000 (52%)\n",
      "Test set: Avg. loss: 1.3794 Accuracy: 5238/10000 (52%)\n",
      "Test set: Avg. loss: 1.3751 Accuracy: 5236/10000 (52%)\n",
      "Test set: Avg. loss: 1.3968 Accuracy: 5203/10000 (52%)\n",
      "Test set: Avg. loss: 1.3904 Accuracy: 5159/10000 (52%)\n",
      "Test set: Avg. loss: 1.3799 Accuracy: 5207/10000 (52%)\n",
      "Test set: Avg. loss: 1.4044 Accuracy: 5179/10000 (52%)\n",
      "Test set: Avg. loss: 1.3937 Accuracy: 5179/10000 (52%)\n",
      "Test set: Avg. loss: 1.3878 Accuracy: 5216/10000 (52%)\n",
      "Test set: Avg. loss: 1.4040 Accuracy: 5203/10000 (52%)\n",
      "Test set: Avg. loss: 1.3837 Accuracy: 5271/10000 (53%)\n",
      "Test set: Avg. loss: 1.3822 Accuracy: 5248/10000 (52%)\n",
      "Test set: Avg. loss: 1.3814 Accuracy: 5251/10000 (53%)\n",
      "Test set: Avg. loss: 1.3792 Accuracy: 5258/10000 (53%)\n",
      "Test set: Avg. loss: 1.3793 Accuracy: 5297/10000 (53%)\n",
      "Test set: Avg. loss: 1.3863 Accuracy: 5234/10000 (52%)\n",
      "Test set: Avg. loss: 1.3965 Accuracy: 5209/10000 (52%)\n",
      "Test set: Avg. loss: 1.3876 Accuracy: 5216/10000 (52%)\n",
      "Test set: Avg. loss: 1.3878 Accuracy: 5234/10000 (52%)\n",
      "Test set: Avg. loss: 1.3935 Accuracy: 5222/10000 (52%)\n",
      "Test set: Avg. loss: 1.3726 Accuracy: 5252/10000 (53%)\n",
      "Test set: Avg. loss: 1.3841 Accuracy: 5258/10000 (53%)\n",
      "Test set: Avg. loss: 1.3848 Accuracy: 5267/10000 (53%)\n",
      "Test set: Avg. loss: 1.3884 Accuracy: 5255/10000 (53%)\n",
      "Test set: Avg. loss: 1.3823 Accuracy: 5242/10000 (52%)\n",
      "Test set: Avg. loss: 1.3770 Accuracy: 5284/10000 (53%)\n",
      "Test set: Avg. loss: 1.3888 Accuracy: 5266/10000 (53%)\n",
      "Test set: Avg. loss: 1.3934 Accuracy: 5217/10000 (52%)\n",
      "Test set: Avg. loss: 1.3818 Accuracy: 5216/10000 (52%)\n",
      "Test set: Avg. loss: 1.3903 Accuracy: 5227/10000 (52%)\n",
      "Test set: Avg. loss: 1.3922 Accuracy: 5220/10000 (52%)\n",
      "Test set: Avg. loss: 1.3826 Accuracy: 5268/10000 (53%)\n",
      "Test set: Avg. loss: 1.3860 Accuracy: 5226/10000 (52%)\n",
      "Test set: Avg. loss: 1.3761 Accuracy: 5254/10000 (53%)\n",
      "Test set: Avg. loss: 1.3933 Accuracy: 5214/10000 (52%)\n",
      "Test set: Avg. loss: 1.3829 Accuracy: 5235/10000 (52%)\n",
      "Test set: Avg. loss: 1.3815 Accuracy: 5240/10000 (52%)\n",
      "Test set: Avg. loss: 1.3988 Accuracy: 5202/10000 (52%)\n",
      "Test set: Avg. loss: 1.3680 Accuracy: 5267/10000 (53%)\n",
      "Test set: Avg. loss: 1.3864 Accuracy: 5244/10000 (52%)\n",
      "Test set: Avg. loss: 1.3738 Accuracy: 5217/10000 (52%)\n",
      "Test set: Avg. loss: 1.3962 Accuracy: 5202/10000 (52%)\n",
      "Test set: Avg. loss: 1.3916 Accuracy: 5214/10000 (52%)\n",
      "Test set: Avg. loss: 1.3862 Accuracy: 5245/10000 (52%)\n",
      "Test set: Avg. loss: 1.3793 Accuracy: 5259/10000 (53%)\n",
      "Test set: Avg. loss: 1.3758 Accuracy: 5266/10000 (53%)\n",
      "Test set: Avg. loss: 1.3821 Accuracy: 5261/10000 (53%)\n",
      "Test set: Avg. loss: 1.3778 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.3782 Accuracy: 5249/10000 (52%)\n",
      "Test set: Avg. loss: 1.3943 Accuracy: 5222/10000 (52%)\n",
      "Test set: Avg. loss: 1.3926 Accuracy: 5207/10000 (52%)\n",
      "Test set: Avg. loss: 1.3785 Accuracy: 5219/10000 (52%)\n",
      "Test set: Avg. loss: 1.3800 Accuracy: 5297/10000 (53%)\n",
      "Test set: Avg. loss: 1.3841 Accuracy: 5264/10000 (53%)\n",
      "Test set: Avg. loss: 1.3694 Accuracy: 5287/10000 (53%)\n",
      "Test set: Avg. loss: 1.3774 Accuracy: 5280/10000 (53%)\n",
      "Test set: Avg. loss: 1.3744 Accuracy: 5275/10000 (53%)\n",
      "Test set: Avg. loss: 1.3925 Accuracy: 5218/10000 (52%)\n",
      "Test set: Avg. loss: 1.3777 Accuracy: 5225/10000 (52%)\n",
      "Test set: Avg. loss: 1.3721 Accuracy: 5319/10000 (53%)\n",
      "Test set: Avg. loss: 1.3962 Accuracy: 5205/10000 (52%)\n",
      "Test set: Avg. loss: 1.3885 Accuracy: 5239/10000 (52%)\n",
      "Test set: Avg. loss: 1.3848 Accuracy: 5221/10000 (52%)\n",
      "Test set: Avg. loss: 1.3964 Accuracy: 5224/10000 (52%)\n",
      "Test set: Avg. loss: 1.3930 Accuracy: 5220/10000 (52%)\n",
      "Test set: Avg. loss: 1.3901 Accuracy: 5281/10000 (53%)\n",
      "Test set: Avg. loss: 1.3765 Accuracy: 5316/10000 (53%)\n",
      "Test set: Avg. loss: 1.3926 Accuracy: 5287/10000 (53%)\n",
      "Test set: Avg. loss: 1.3845 Accuracy: 5281/10000 (53%)\n",
      "Test set: Avg. loss: 1.3895 Accuracy: 5262/10000 (53%)\n",
      "Test set: Avg. loss: 1.3834 Accuracy: 5267/10000 (53%)\n",
      "Test set: Avg. loss: 1.3922 Accuracy: 5226/10000 (52%)\n",
      "Test set: Avg. loss: 1.3800 Accuracy: 5266/10000 (53%)\n",
      "Test set: Avg. loss: 1.3774 Accuracy: 5250/10000 (52%)\n",
      "Test set: Avg. loss: 1.3802 Accuracy: 5280/10000 (53%)\n",
      "Test set: Avg. loss: 1.3863 Accuracy: 5243/10000 (52%)\n",
      "Test set: Avg. loss: 1.3882 Accuracy: 5274/10000 (53%)\n",
      "Test set: Avg. loss: 1.3863 Accuracy: 5280/10000 (53%)\n",
      "Test set: Avg. loss: 1.3798 Accuracy: 5305/10000 (53%)\n",
      "Test set: Avg. loss: 1.3950 Accuracy: 5261/10000 (53%)\n",
      "Test set: Avg. loss: 1.3734 Accuracy: 5312/10000 (53%)\n",
      "Test set: Avg. loss: 1.3702 Accuracy: 5308/10000 (53%)\n",
      "Test set: Avg. loss: 1.3919 Accuracy: 5276/10000 (53%)\n",
      "Test set: Avg. loss: 1.3905 Accuracy: 5225/10000 (52%)\n",
      "Test set: Avg. loss: 1.3871 Accuracy: 5249/10000 (52%)\n",
      "Test set: Avg. loss: 1.3764 Accuracy: 5281/10000 (53%)\n",
      "Test set: Avg. loss: 1.3853 Accuracy: 5238/10000 (52%)\n",
      "Test set: Avg. loss: 1.3695 Accuracy: 5279/10000 (53%)\n",
      "Test set: Avg. loss: 1.3831 Accuracy: 5267/10000 (53%)\n",
      "Test set: Avg. loss: 1.3943 Accuracy: 5228/10000 (52%)\n",
      "Test set: Avg. loss: 1.3866 Accuracy: 5235/10000 (52%)\n",
      "Test set: Avg. loss: 1.3889 Accuracy: 5247/10000 (52%)\n",
      "Test set: Avg. loss: 1.3747 Accuracy: 5318/10000 (53%)\n",
      "Test set: Avg. loss: 1.3745 Accuracy: 5278/10000 (53%)\n",
      "Test set: Avg. loss: 1.3745 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.3910 Accuracy: 5277/10000 (53%)\n",
      "Test set: Avg. loss: 1.3858 Accuracy: 5239/10000 (52%)\n",
      "Test set: Avg. loss: 1.3922 Accuracy: 5285/10000 (53%)\n",
      "Test set: Avg. loss: 1.3868 Accuracy: 5277/10000 (53%)\n",
      "Test set: Avg. loss: 1.3991 Accuracy: 5245/10000 (52%)\n",
      "Test set: Avg. loss: 1.3904 Accuracy: 5255/10000 (53%)\n",
      "Test set: Avg. loss: 1.3738 Accuracy: 5277/10000 (53%)\n",
      "Test set: Avg. loss: 1.3917 Accuracy: 5257/10000 (53%)\n",
      "Test set: Avg. loss: 1.3807 Accuracy: 5285/10000 (53%)\n",
      "Test set: Avg. loss: 1.3800 Accuracy: 5300/10000 (53%)\n",
      "Test set: Avg. loss: 1.3867 Accuracy: 5295/10000 (53%)\n",
      "Test set: Avg. loss: 1.3896 Accuracy: 5300/10000 (53%)\n",
      "Test set: Avg. loss: 1.3780 Accuracy: 5287/10000 (53%)\n",
      "Test set: Avg. loss: 1.3833 Accuracy: 5268/10000 (53%)\n",
      "Test set: Avg. loss: 1.3899 Accuracy: 5295/10000 (53%)\n",
      "Test set: Avg. loss: 1.3810 Accuracy: 5289/10000 (53%)\n",
      "Test set: Avg. loss: 1.4014 Accuracy: 5273/10000 (53%)\n",
      "Test set: Avg. loss: 1.3872 Accuracy: 5271/10000 (53%)\n",
      "Test set: Avg. loss: 1.3816 Accuracy: 5291/10000 (53%)\n",
      "Test set: Avg. loss: 1.3891 Accuracy: 5295/10000 (53%)\n",
      "Test set: Avg. loss: 1.4019 Accuracy: 5234/10000 (52%)\n",
      "Test set: Avg. loss: 1.3890 Accuracy: 5262/10000 (53%)\n",
      "Test set: Avg. loss: 1.3754 Accuracy: 5299/10000 (53%)\n",
      "Test set: Avg. loss: 1.3961 Accuracy: 5229/10000 (52%)\n",
      "Test set: Avg. loss: 1.3785 Accuracy: 5326/10000 (53%)\n",
      "Test set: Avg. loss: 1.3860 Accuracy: 5276/10000 (53%)\n",
      "Test set: Avg. loss: 1.3794 Accuracy: 5318/10000 (53%)\n",
      "Test set: Avg. loss: 1.3820 Accuracy: 5306/10000 (53%)\n",
      "Test set: Avg. loss: 1.3759 Accuracy: 5346/10000 (53%)\n",
      "Test set: Avg. loss: 1.3768 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.3816 Accuracy: 5300/10000 (53%)\n",
      "Test set: Avg. loss: 1.3970 Accuracy: 5299/10000 (53%)\n",
      "Test set: Avg. loss: 1.3914 Accuracy: 5292/10000 (53%)\n",
      "Test set: Avg. loss: 1.3967 Accuracy: 5306/10000 (53%)\n",
      "Test set: Avg. loss: 1.3808 Accuracy: 5296/10000 (53%)\n",
      "Test set: Avg. loss: 1.3790 Accuracy: 5355/10000 (54%)\n",
      "Test set: Avg. loss: 1.3812 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.3797 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.3911 Accuracy: 5309/10000 (53%)\n",
      "Test set: Avg. loss: 1.4021 Accuracy: 5287/10000 (53%)\n",
      "Test set: Avg. loss: 1.3992 Accuracy: 5270/10000 (53%)\n",
      "Test set: Avg. loss: 1.3790 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.3944 Accuracy: 5324/10000 (53%)\n",
      "Test set: Avg. loss: 1.3785 Accuracy: 5317/10000 (53%)\n",
      "Test set: Avg. loss: 1.3879 Accuracy: 5288/10000 (53%)\n",
      "Test set: Avg. loss: 1.3807 Accuracy: 5311/10000 (53%)\n",
      "Test set: Avg. loss: 1.3897 Accuracy: 5278/10000 (53%)\n",
      "Test set: Avg. loss: 1.3742 Accuracy: 5334/10000 (53%)\n",
      "Test set: Avg. loss: 1.3963 Accuracy: 5285/10000 (53%)\n",
      "Test set: Avg. loss: 1.3816 Accuracy: 5301/10000 (53%)\n",
      "Test set: Avg. loss: 1.4045 Accuracy: 5276/10000 (53%)\n",
      "Test set: Avg. loss: 1.3783 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.3847 Accuracy: 5296/10000 (53%)\n",
      "Test set: Avg. loss: 1.3980 Accuracy: 5290/10000 (53%)\n",
      "Test set: Avg. loss: 1.3936 Accuracy: 5278/10000 (53%)\n",
      "Test set: Avg. loss: 1.3989 Accuracy: 5307/10000 (53%)\n",
      "Test set: Avg. loss: 1.3856 Accuracy: 5334/10000 (53%)\n",
      "Test set: Avg. loss: 1.3904 Accuracy: 5288/10000 (53%)\n",
      "Test set: Avg. loss: 1.3969 Accuracy: 5298/10000 (53%)\n",
      "Test set: Avg. loss: 1.3955 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.3917 Accuracy: 5285/10000 (53%)\n",
      "Test set: Avg. loss: 1.4014 Accuracy: 5279/10000 (53%)\n",
      "Test set: Avg. loss: 1.3897 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.3904 Accuracy: 5306/10000 (53%)\n",
      "Test set: Avg. loss: 1.3866 Accuracy: 5329/10000 (53%)\n",
      "Test set: Avg. loss: 1.3739 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.3831 Accuracy: 5345/10000 (53%)\n",
      "Test set: Avg. loss: 1.3878 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.3846 Accuracy: 5326/10000 (53%)\n",
      "Test set: Avg. loss: 1.3980 Accuracy: 5304/10000 (53%)\n",
      "Test set: Avg. loss: 1.4004 Accuracy: 5274/10000 (53%)\n",
      "Test set: Avg. loss: 1.3804 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.3870 Accuracy: 5308/10000 (53%)\n",
      "Test set: Avg. loss: 1.3940 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.3815 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.3911 Accuracy: 5284/10000 (53%)\n",
      "Test set: Avg. loss: 1.3874 Accuracy: 5334/10000 (53%)\n",
      "Test set: Avg. loss: 1.3875 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.3957 Accuracy: 5294/10000 (53%)\n",
      "Test set: Avg. loss: 1.3914 Accuracy: 5330/10000 (53%)\n",
      "Test set: Avg. loss: 1.3967 Accuracy: 5307/10000 (53%)\n",
      "Test set: Avg. loss: 1.3978 Accuracy: 5253/10000 (53%)\n",
      "Test set: Avg. loss: 1.3902 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.3952 Accuracy: 5318/10000 (53%)\n",
      "Test set: Avg. loss: 1.3881 Accuracy: 5320/10000 (53%)\n",
      "Test set: Avg. loss: 1.3816 Accuracy: 5319/10000 (53%)\n",
      "Test set: Avg. loss: 1.3809 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.3920 Accuracy: 5301/10000 (53%)\n",
      "Test set: Avg. loss: 1.3862 Accuracy: 5356/10000 (54%)\n",
      "Test set: Avg. loss: 1.3860 Accuracy: 5309/10000 (53%)\n",
      "Test set: Avg. loss: 1.3983 Accuracy: 5298/10000 (53%)\n",
      "Test set: Avg. loss: 1.3981 Accuracy: 5309/10000 (53%)\n",
      "Test set: Avg. loss: 1.3858 Accuracy: 5317/10000 (53%)\n",
      "Test set: Avg. loss: 1.4109 Accuracy: 5263/10000 (53%)\n",
      "Test set: Avg. loss: 1.3853 Accuracy: 5378/10000 (54%)\n",
      "Test set: Avg. loss: 1.3876 Accuracy: 5337/10000 (53%)\n",
      "Test set: Avg. loss: 1.3934 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.3850 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.3927 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.3864 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.4002 Accuracy: 5318/10000 (53%)\n",
      "Test set: Avg. loss: 1.3911 Accuracy: 5330/10000 (53%)\n",
      "Test set: Avg. loss: 1.3916 Accuracy: 5330/10000 (53%)\n",
      "Test set: Avg. loss: 1.3851 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.3969 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.4011 Accuracy: 5291/10000 (53%)\n",
      "Test set: Avg. loss: 1.3846 Accuracy: 5329/10000 (53%)\n",
      "Test set: Avg. loss: 1.3999 Accuracy: 5303/10000 (53%)\n",
      "Test set: Avg. loss: 1.3904 Accuracy: 5312/10000 (53%)\n",
      "Test set: Avg. loss: 1.3998 Accuracy: 5303/10000 (53%)\n",
      "Test set: Avg. loss: 1.4078 Accuracy: 5299/10000 (53%)\n",
      "Test set: Avg. loss: 1.3908 Accuracy: 5309/10000 (53%)\n",
      "Test set: Avg. loss: 1.3886 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.3953 Accuracy: 5296/10000 (53%)\n",
      "Test set: Avg. loss: 1.3886 Accuracy: 5309/10000 (53%)\n",
      "Test set: Avg. loss: 1.3919 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.3919 Accuracy: 5362/10000 (54%)\n",
      "Test set: Avg. loss: 1.3877 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.3904 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.3921 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.3934 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.3970 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4190 Accuracy: 5280/10000 (53%)\n",
      "Test set: Avg. loss: 1.3927 Accuracy: 5362/10000 (54%)\n",
      "Test set: Avg. loss: 1.3986 Accuracy: 5316/10000 (53%)\n",
      "Test set: Avg. loss: 1.4034 Accuracy: 5288/10000 (53%)\n",
      "Test set: Avg. loss: 1.3981 Accuracy: 5287/10000 (53%)\n",
      "Test set: Avg. loss: 1.4051 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.3939 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.3850 Accuracy: 5368/10000 (54%)\n",
      "Test set: Avg. loss: 1.3889 Accuracy: 5363/10000 (54%)\n",
      "Test set: Avg. loss: 1.3973 Accuracy: 5343/10000 (53%)\n",
      "Test set: Avg. loss: 1.3850 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.3780 Accuracy: 5378/10000 (54%)\n",
      "Test set: Avg. loss: 1.3965 Accuracy: 5341/10000 (53%)\n",
      "Test set: Avg. loss: 1.3960 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.4022 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.3843 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.3904 Accuracy: 5325/10000 (53%)\n",
      "Test set: Avg. loss: 1.4092 Accuracy: 5329/10000 (53%)\n",
      "Test set: Avg. loss: 1.3907 Accuracy: 5344/10000 (53%)\n",
      "Test set: Avg. loss: 1.4001 Accuracy: 5340/10000 (53%)\n",
      "Test set: Avg. loss: 1.3961 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.4049 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4033 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4035 Accuracy: 5329/10000 (53%)\n",
      "Test set: Avg. loss: 1.3973 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4063 Accuracy: 5290/10000 (53%)\n",
      "Test set: Avg. loss: 1.3954 Accuracy: 5300/10000 (53%)\n",
      "Test set: Avg. loss: 1.4008 Accuracy: 5311/10000 (53%)\n",
      "Test set: Avg. loss: 1.4013 Accuracy: 5300/10000 (53%)\n",
      "Test set: Avg. loss: 1.4007 Accuracy: 5328/10000 (53%)\n",
      "Test set: Avg. loss: 1.4266 Accuracy: 5310/10000 (53%)\n",
      "Test set: Avg. loss: 1.4112 Accuracy: 5359/10000 (54%)\n",
      "Test set: Avg. loss: 1.4092 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.3959 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4058 Accuracy: 5394/10000 (54%)\n",
      "Test set: Avg. loss: 1.4009 Accuracy: 5350/10000 (54%)\n",
      "Test set: Avg. loss: 1.4019 Accuracy: 5303/10000 (53%)\n",
      "Test set: Avg. loss: 1.4037 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4072 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4076 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.4084 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4193 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.4088 Accuracy: 5347/10000 (53%)\n",
      "Test set: Avg. loss: 1.4052 Accuracy: 5321/10000 (53%)\n",
      "Test set: Avg. loss: 1.4007 Accuracy: 5326/10000 (53%)\n",
      "Test set: Avg. loss: 1.4115 Accuracy: 5318/10000 (53%)\n",
      "Test set: Avg. loss: 1.3975 Accuracy: 5385/10000 (54%)\n",
      "Test set: Avg. loss: 1.4139 Accuracy: 5294/10000 (53%)\n",
      "Test set: Avg. loss: 1.3998 Accuracy: 5382/10000 (54%)\n",
      "Test set: Avg. loss: 1.4190 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.3893 Accuracy: 5380/10000 (54%)\n",
      "Test set: Avg. loss: 1.3905 Accuracy: 5325/10000 (53%)\n",
      "Test set: Avg. loss: 1.4142 Accuracy: 5289/10000 (53%)\n",
      "Test set: Avg. loss: 1.3946 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.3893 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4001 Accuracy: 5312/10000 (53%)\n",
      "Test set: Avg. loss: 1.3891 Accuracy: 5355/10000 (54%)\n",
      "Test set: Avg. loss: 1.3990 Accuracy: 5344/10000 (53%)\n",
      "Test set: Avg. loss: 1.4008 Accuracy: 5340/10000 (53%)\n",
      "Test set: Avg. loss: 1.3997 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4060 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4054 Accuracy: 5304/10000 (53%)\n",
      "Test set: Avg. loss: 1.4075 Accuracy: 5282/10000 (53%)\n",
      "Test set: Avg. loss: 1.3959 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.4138 Accuracy: 5261/10000 (53%)\n",
      "Test set: Avg. loss: 1.4149 Accuracy: 5287/10000 (53%)\n",
      "Test set: Avg. loss: 1.4011 Accuracy: 5367/10000 (54%)\n",
      "Test set: Avg. loss: 1.4138 Accuracy: 5324/10000 (53%)\n",
      "Test set: Avg. loss: 1.4119 Accuracy: 5315/10000 (53%)\n",
      "Test set: Avg. loss: 1.4067 Accuracy: 5365/10000 (54%)\n",
      "Test set: Avg. loss: 1.4020 Accuracy: 5378/10000 (54%)\n",
      "Test set: Avg. loss: 1.4083 Accuracy: 5313/10000 (53%)\n",
      "Test set: Avg. loss: 1.4146 Accuracy: 5312/10000 (53%)\n",
      "Test set: Avg. loss: 1.4107 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4129 Accuracy: 5264/10000 (53%)\n",
      "Test set: Avg. loss: 1.3959 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4126 Accuracy: 5350/10000 (54%)\n",
      "Test set: Avg. loss: 1.3992 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.3971 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.4105 Accuracy: 5341/10000 (53%)\n",
      "Test set: Avg. loss: 1.3971 Accuracy: 5387/10000 (54%)\n",
      "Test set: Avg. loss: 1.3999 Accuracy: 5287/10000 (53%)\n",
      "Test set: Avg. loss: 1.4165 Accuracy: 5294/10000 (53%)\n",
      "Test set: Avg. loss: 1.3923 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.3956 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.3981 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.4009 Accuracy: 5371/10000 (54%)\n",
      "Test set: Avg. loss: 1.4062 Accuracy: 5326/10000 (53%)\n",
      "Test set: Avg. loss: 1.4005 Accuracy: 5356/10000 (54%)\n",
      "Test set: Avg. loss: 1.3992 Accuracy: 5363/10000 (54%)\n",
      "Test set: Avg. loss: 1.4302 Accuracy: 5269/10000 (53%)\n",
      "Test set: Avg. loss: 1.4128 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4029 Accuracy: 5372/10000 (54%)\n",
      "Test set: Avg. loss: 1.4154 Accuracy: 5356/10000 (54%)\n",
      "Test set: Avg. loss: 1.4084 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.4157 Accuracy: 5301/10000 (53%)\n",
      "Test set: Avg. loss: 1.4010 Accuracy: 5347/10000 (53%)\n",
      "Test set: Avg. loss: 1.3995 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.3989 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.4057 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.4092 Accuracy: 5336/10000 (53%)\n",
      "Test set: Avg. loss: 1.4155 Accuracy: 5353/10000 (54%)\n",
      "Test set: Avg. loss: 1.4076 Accuracy: 5320/10000 (53%)\n",
      "Test set: Avg. loss: 1.4188 Accuracy: 5290/10000 (53%)\n",
      "Test set: Avg. loss: 1.4107 Accuracy: 5324/10000 (53%)\n",
      "Test set: Avg. loss: 1.3944 Accuracy: 5361/10000 (54%)\n",
      "Test set: Avg. loss: 1.3897 Accuracy: 5375/10000 (54%)\n",
      "Test set: Avg. loss: 1.4238 Accuracy: 5291/10000 (53%)\n",
      "Test set: Avg. loss: 1.4055 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.3999 Accuracy: 5346/10000 (53%)\n",
      "Test set: Avg. loss: 1.4034 Accuracy: 5368/10000 (54%)\n",
      "Test set: Avg. loss: 1.4110 Accuracy: 5316/10000 (53%)\n",
      "Test set: Avg. loss: 1.4110 Accuracy: 5319/10000 (53%)\n",
      "Test set: Avg. loss: 1.4148 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4095 Accuracy: 5375/10000 (54%)\n",
      "Test set: Avg. loss: 1.4140 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4157 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.4087 Accuracy: 5309/10000 (53%)\n",
      "Test set: Avg. loss: 1.4054 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.4121 Accuracy: 5379/10000 (54%)\n",
      "Test set: Avg. loss: 1.4070 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4095 Accuracy: 5347/10000 (53%)\n",
      "Test set: Avg. loss: 1.4206 Accuracy: 5309/10000 (53%)\n",
      "Test set: Avg. loss: 1.4269 Accuracy: 5319/10000 (53%)\n",
      "Test set: Avg. loss: 1.4127 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4167 Accuracy: 5310/10000 (53%)\n",
      "Test set: Avg. loss: 1.4191 Accuracy: 5317/10000 (53%)\n",
      "Test set: Avg. loss: 1.4040 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4250 Accuracy: 5279/10000 (53%)\n",
      "Test set: Avg. loss: 1.4098 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.4143 Accuracy: 5385/10000 (54%)\n",
      "Test set: Avg. loss: 1.4022 Accuracy: 5389/10000 (54%)\n",
      "Test set: Avg. loss: 1.4151 Accuracy: 5346/10000 (53%)\n",
      "Test set: Avg. loss: 1.3996 Accuracy: 5385/10000 (54%)\n",
      "Test set: Avg. loss: 1.4032 Accuracy: 5356/10000 (54%)\n",
      "Test set: Avg. loss: 1.4107 Accuracy: 5372/10000 (54%)\n",
      "Test set: Avg. loss: 1.4147 Accuracy: 5347/10000 (53%)\n",
      "Test set: Avg. loss: 1.4285 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.4102 Accuracy: 5368/10000 (54%)\n",
      "Test set: Avg. loss: 1.4018 Accuracy: 5392/10000 (54%)\n",
      "Test set: Avg. loss: 1.4018 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.4280 Accuracy: 5307/10000 (53%)\n",
      "Test set: Avg. loss: 1.4127 Accuracy: 5324/10000 (53%)\n",
      "Test set: Avg. loss: 1.4180 Accuracy: 5321/10000 (53%)\n",
      "Test set: Avg. loss: 1.4144 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.4191 Accuracy: 5350/10000 (54%)\n",
      "Test set: Avg. loss: 1.4063 Accuracy: 5299/10000 (53%)\n",
      "Test set: Avg. loss: 1.4181 Accuracy: 5286/10000 (53%)\n",
      "Test set: Avg. loss: 1.4181 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4109 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.4230 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.4091 Accuracy: 5378/10000 (54%)\n",
      "Test set: Avg. loss: 1.4207 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4228 Accuracy: 5292/10000 (53%)\n",
      "Test set: Avg. loss: 1.4094 Accuracy: 5353/10000 (54%)\n",
      "Test set: Avg. loss: 1.3957 Accuracy: 5387/10000 (54%)\n",
      "Test set: Avg. loss: 1.4170 Accuracy: 5301/10000 (53%)\n",
      "Test set: Avg. loss: 1.4170 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4238 Accuracy: 5277/10000 (53%)\n",
      "Test set: Avg. loss: 1.4104 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4094 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4006 Accuracy: 5399/10000 (54%)\n",
      "Test set: Avg. loss: 1.3929 Accuracy: 5386/10000 (54%)\n",
      "Test set: Avg. loss: 1.4030 Accuracy: 5392/10000 (54%)\n",
      "Test set: Avg. loss: 1.4007 Accuracy: 5400/10000 (54%)\n",
      "Test set: Avg. loss: 1.4112 Accuracy: 5373/10000 (54%)\n",
      "Test set: Avg. loss: 1.4081 Accuracy: 5402/10000 (54%)\n",
      "Test set: Avg. loss: 1.4193 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.4271 Accuracy: 5298/10000 (53%)\n",
      "Test set: Avg. loss: 1.4134 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.4255 Accuracy: 5302/10000 (53%)\n",
      "Test set: Avg. loss: 1.4126 Accuracy: 5389/10000 (54%)\n",
      "Test set: Avg. loss: 1.4237 Accuracy: 5385/10000 (54%)\n",
      "Test set: Avg. loss: 1.4079 Accuracy: 5383/10000 (54%)\n",
      "Test set: Avg. loss: 1.4262 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.4292 Accuracy: 5346/10000 (53%)\n",
      "Test set: Avg. loss: 1.4321 Accuracy: 5363/10000 (54%)\n",
      "Test set: Avg. loss: 1.4179 Accuracy: 5377/10000 (54%)\n",
      "Test set: Avg. loss: 1.3972 Accuracy: 5401/10000 (54%)\n",
      "Test set: Avg. loss: 1.4359 Accuracy: 5315/10000 (53%)\n",
      "Test set: Avg. loss: 1.4054 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4022 Accuracy: 5404/10000 (54%)\n",
      "Test set: Avg. loss: 1.4105 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4136 Accuracy: 5406/10000 (54%)\n",
      "Test set: Avg. loss: 1.4159 Accuracy: 5356/10000 (54%)\n",
      "Test set: Avg. loss: 1.4069 Accuracy: 5367/10000 (54%)\n",
      "Test set: Avg. loss: 1.4061 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.4171 Accuracy: 5378/10000 (54%)\n",
      "Test set: Avg. loss: 1.4009 Accuracy: 5419/10000 (54%)\n",
      "Test set: Avg. loss: 1.4020 Accuracy: 5343/10000 (53%)\n",
      "Test set: Avg. loss: 1.4123 Accuracy: 5347/10000 (53%)\n",
      "Test set: Avg. loss: 1.4231 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4153 Accuracy: 5364/10000 (54%)\n",
      "Test set: Avg. loss: 1.4205 Accuracy: 5362/10000 (54%)\n",
      "Test set: Avg. loss: 1.4086 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4237 Accuracy: 5319/10000 (53%)\n",
      "Test set: Avg. loss: 1.4099 Accuracy: 5391/10000 (54%)\n",
      "Test set: Avg. loss: 1.4254 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4210 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4222 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4129 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4106 Accuracy: 5402/10000 (54%)\n",
      "Test set: Avg. loss: 1.4168 Accuracy: 5383/10000 (54%)\n",
      "Test set: Avg. loss: 1.4147 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4218 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.4170 Accuracy: 5369/10000 (54%)\n",
      "Test set: Avg. loss: 1.4105 Accuracy: 5391/10000 (54%)\n",
      "Test set: Avg. loss: 1.4346 Accuracy: 5308/10000 (53%)\n",
      "Test set: Avg. loss: 1.4093 Accuracy: 5362/10000 (54%)\n",
      "Test set: Avg. loss: 1.4167 Accuracy: 5374/10000 (54%)\n",
      "Test set: Avg. loss: 1.4114 Accuracy: 5416/10000 (54%)\n",
      "Test set: Avg. loss: 1.4178 Accuracy: 5401/10000 (54%)\n",
      "Test set: Avg. loss: 1.4320 Accuracy: 5394/10000 (54%)\n",
      "Test set: Avg. loss: 1.4281 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4233 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.4125 Accuracy: 5406/10000 (54%)\n",
      "Test set: Avg. loss: 1.4356 Accuracy: 5367/10000 (54%)\n",
      "Test set: Avg. loss: 1.4325 Accuracy: 5369/10000 (54%)\n",
      "Test set: Avg. loss: 1.4268 Accuracy: 5391/10000 (54%)\n",
      "Test set: Avg. loss: 1.4338 Accuracy: 5387/10000 (54%)\n",
      "Test set: Avg. loss: 1.4561 Accuracy: 5328/10000 (53%)\n",
      "Test set: Avg. loss: 1.4332 Accuracy: 5335/10000 (53%)\n",
      "Test set: Avg. loss: 1.4250 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.4223 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4262 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4148 Accuracy: 5390/10000 (54%)\n",
      "Test set: Avg. loss: 1.4246 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.4387 Accuracy: 5337/10000 (53%)\n",
      "Test set: Avg. loss: 1.4347 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4301 Accuracy: 5340/10000 (53%)\n",
      "Test set: Avg. loss: 1.4240 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4299 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.4236 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.4157 Accuracy: 5388/10000 (54%)\n",
      "Test set: Avg. loss: 1.4252 Accuracy: 5377/10000 (54%)\n",
      "Test set: Avg. loss: 1.4109 Accuracy: 5415/10000 (54%)\n",
      "Test set: Avg. loss: 1.4183 Accuracy: 5402/10000 (54%)\n",
      "Test set: Avg. loss: 1.4264 Accuracy: 5375/10000 (54%)\n",
      "Test set: Avg. loss: 1.4176 Accuracy: 5405/10000 (54%)\n",
      "Test set: Avg. loss: 1.4335 Accuracy: 5373/10000 (54%)\n",
      "Test set: Avg. loss: 1.4362 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.4092 Accuracy: 5411/10000 (54%)\n",
      "Test set: Avg. loss: 1.4229 Accuracy: 5363/10000 (54%)\n",
      "Test set: Avg. loss: 1.4176 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4224 Accuracy: 5420/10000 (54%)\n",
      "Test set: Avg. loss: 1.4158 Accuracy: 5408/10000 (54%)\n",
      "Test set: Avg. loss: 1.4249 Accuracy: 5350/10000 (54%)\n",
      "Test set: Avg. loss: 1.4220 Accuracy: 5367/10000 (54%)\n",
      "Test set: Avg. loss: 1.4277 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4231 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4318 Accuracy: 5313/10000 (53%)\n",
      "Test set: Avg. loss: 1.4213 Accuracy: 5369/10000 (54%)\n",
      "Test set: Avg. loss: 1.4373 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.4396 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.4300 Accuracy: 5341/10000 (53%)\n",
      "Test set: Avg. loss: 1.4321 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.4110 Accuracy: 5385/10000 (54%)\n",
      "Test set: Avg. loss: 1.4192 Accuracy: 5392/10000 (54%)\n",
      "Test set: Avg. loss: 1.4297 Accuracy: 5373/10000 (54%)\n",
      "Test set: Avg. loss: 1.4282 Accuracy: 5328/10000 (53%)\n",
      "Test set: Avg. loss: 1.4251 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.4311 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.4253 Accuracy: 5347/10000 (53%)\n",
      "Test set: Avg. loss: 1.4382 Accuracy: 5365/10000 (54%)\n",
      "Test set: Avg. loss: 1.4324 Accuracy: 5337/10000 (53%)\n",
      "Test set: Avg. loss: 1.4523 Accuracy: 5301/10000 (53%)\n",
      "Test set: Avg. loss: 1.4226 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4261 Accuracy: 5359/10000 (54%)\n",
      "Test set: Avg. loss: 1.4432 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4190 Accuracy: 5375/10000 (54%)\n",
      "Test set: Avg. loss: 1.4287 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4202 Accuracy: 5384/10000 (54%)\n",
      "Test set: Avg. loss: 1.4310 Accuracy: 5336/10000 (53%)\n",
      "Test set: Avg. loss: 1.4255 Accuracy: 5365/10000 (54%)\n",
      "Test set: Avg. loss: 1.4393 Accuracy: 5316/10000 (53%)\n",
      "Test set: Avg. loss: 1.4218 Accuracy: 5350/10000 (54%)\n",
      "Test set: Avg. loss: 1.4270 Accuracy: 5363/10000 (54%)\n",
      "Test set: Avg. loss: 1.4330 Accuracy: 5387/10000 (54%)\n",
      "Test set: Avg. loss: 1.4352 Accuracy: 5345/10000 (53%)\n",
      "Test set: Avg. loss: 1.4326 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.4396 Accuracy: 5355/10000 (54%)\n",
      "Test set: Avg. loss: 1.4312 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4184 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4255 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4290 Accuracy: 5322/10000 (53%)\n",
      "Test set: Avg. loss: 1.4337 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.4307 Accuracy: 5349/10000 (53%)\n",
      "Test set: Avg. loss: 1.4187 Accuracy: 5361/10000 (54%)\n",
      "Test set: Avg. loss: 1.4239 Accuracy: 5352/10000 (54%)\n",
      "Test set: Avg. loss: 1.4130 Accuracy: 5371/10000 (54%)\n",
      "Test set: Avg. loss: 1.4160 Accuracy: 5386/10000 (54%)\n",
      "Test set: Avg. loss: 1.4225 Accuracy: 5392/10000 (54%)\n",
      "Test set: Avg. loss: 1.4299 Accuracy: 5362/10000 (54%)\n",
      "Test set: Avg. loss: 1.4310 Accuracy: 5364/10000 (54%)\n",
      "Test set: Avg. loss: 1.4189 Accuracy: 5402/10000 (54%)\n",
      "Test set: Avg. loss: 1.4337 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.4374 Accuracy: 5368/10000 (54%)\n",
      "Test set: Avg. loss: 1.4215 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4234 Accuracy: 5381/10000 (54%)\n",
      "Test set: Avg. loss: 1.4377 Accuracy: 5353/10000 (54%)\n",
      "Test set: Avg. loss: 1.4432 Accuracy: 5399/10000 (54%)\n",
      "Test set: Avg. loss: 1.4349 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.4514 Accuracy: 5313/10000 (53%)\n",
      "Test set: Avg. loss: 1.4338 Accuracy: 5343/10000 (53%)\n",
      "Test set: Avg. loss: 1.4176 Accuracy: 5408/10000 (54%)\n",
      "Test set: Avg. loss: 1.4209 Accuracy: 5355/10000 (54%)\n",
      "Test set: Avg. loss: 1.4536 Accuracy: 5316/10000 (53%)\n",
      "Test set: Avg. loss: 1.4457 Accuracy: 5340/10000 (53%)\n",
      "Test set: Avg. loss: 1.4297 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.4431 Accuracy: 5328/10000 (53%)\n",
      "Test set: Avg. loss: 1.4315 Accuracy: 5383/10000 (54%)\n",
      "Test set: Avg. loss: 1.4284 Accuracy: 5374/10000 (54%)\n",
      "Test set: Avg. loss: 1.4394 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.4258 Accuracy: 5374/10000 (54%)\n",
      "Test set: Avg. loss: 1.4314 Accuracy: 5373/10000 (54%)\n",
      "Test set: Avg. loss: 1.4364 Accuracy: 5349/10000 (53%)\n",
      "Test set: Avg. loss: 1.4323 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4467 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4294 Accuracy: 5367/10000 (54%)\n",
      "Test set: Avg. loss: 1.4376 Accuracy: 5344/10000 (53%)\n",
      "Test set: Avg. loss: 1.4498 Accuracy: 5371/10000 (54%)\n",
      "Test set: Avg. loss: 1.4418 Accuracy: 5378/10000 (54%)\n",
      "Test set: Avg. loss: 1.4406 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4349 Accuracy: 5328/10000 (53%)\n",
      "Test set: Avg. loss: 1.4350 Accuracy: 5383/10000 (54%)\n",
      "Test set: Avg. loss: 1.4470 Accuracy: 5345/10000 (53%)\n",
      "Test set: Avg. loss: 1.4353 Accuracy: 5380/10000 (54%)\n",
      "Test set: Avg. loss: 1.4476 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4467 Accuracy: 5368/10000 (54%)\n",
      "Test set: Avg. loss: 1.4307 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4276 Accuracy: 5403/10000 (54%)\n",
      "Test set: Avg. loss: 1.4558 Accuracy: 5304/10000 (53%)\n",
      "Test set: Avg. loss: 1.4372 Accuracy: 5363/10000 (54%)\n",
      "Test set: Avg. loss: 1.4347 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4422 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4309 Accuracy: 5337/10000 (53%)\n",
      "Test set: Avg. loss: 1.4330 Accuracy: 5349/10000 (53%)\n",
      "Test set: Avg. loss: 1.4404 Accuracy: 5362/10000 (54%)\n",
      "Test set: Avg. loss: 1.4500 Accuracy: 5314/10000 (53%)\n",
      "Test set: Avg. loss: 1.4351 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4355 Accuracy: 5385/10000 (54%)\n",
      "Test set: Avg. loss: 1.4479 Accuracy: 5339/10000 (53%)\n",
      "Test set: Avg. loss: 1.4420 Accuracy: 5306/10000 (53%)\n",
      "Test set: Avg. loss: 1.4317 Accuracy: 5350/10000 (54%)\n",
      "Test set: Avg. loss: 1.4386 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.4303 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.4576 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4337 Accuracy: 5364/10000 (54%)\n",
      "Test set: Avg. loss: 1.4452 Accuracy: 5326/10000 (53%)\n",
      "Test set: Avg. loss: 1.4405 Accuracy: 5374/10000 (54%)\n",
      "Test set: Avg. loss: 1.4442 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.4397 Accuracy: 5359/10000 (54%)\n",
      "Test set: Avg. loss: 1.4423 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4427 Accuracy: 5318/10000 (53%)\n",
      "Test set: Avg. loss: 1.4354 Accuracy: 5366/10000 (54%)\n",
      "Test set: Avg. loss: 1.4516 Accuracy: 5306/10000 (53%)\n",
      "Test set: Avg. loss: 1.4530 Accuracy: 5353/10000 (54%)\n",
      "Test set: Avg. loss: 1.4561 Accuracy: 5290/10000 (53%)\n",
      "Test set: Avg. loss: 1.4581 Accuracy: 5286/10000 (53%)\n",
      "Test set: Avg. loss: 1.4366 Accuracy: 5343/10000 (53%)\n",
      "Test set: Avg. loss: 1.4612 Accuracy: 5330/10000 (53%)\n",
      "Test set: Avg. loss: 1.4539 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4412 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4672 Accuracy: 5294/10000 (53%)\n",
      "Test set: Avg. loss: 1.4568 Accuracy: 5285/10000 (53%)\n",
      "Test set: Avg. loss: 1.4441 Accuracy: 5329/10000 (53%)\n",
      "Test set: Avg. loss: 1.4374 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4615 Accuracy: 5336/10000 (53%)\n",
      "Test set: Avg. loss: 1.4649 Accuracy: 5330/10000 (53%)\n",
      "Test set: Avg. loss: 1.4513 Accuracy: 5312/10000 (53%)\n",
      "Test set: Avg. loss: 1.4411 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.4624 Accuracy: 5304/10000 (53%)\n",
      "Test set: Avg. loss: 1.4667 Accuracy: 5281/10000 (53%)\n",
      "Test set: Avg. loss: 1.4479 Accuracy: 5298/10000 (53%)\n",
      "Test set: Avg. loss: 1.4523 Accuracy: 5325/10000 (53%)\n",
      "Test set: Avg. loss: 1.4587 Accuracy: 5286/10000 (53%)\n",
      "Test set: Avg. loss: 1.4594 Accuracy: 5330/10000 (53%)\n",
      "Test set: Avg. loss: 1.4543 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.4442 Accuracy: 5349/10000 (53%)\n",
      "Test set: Avg. loss: 1.4666 Accuracy: 5293/10000 (53%)\n",
      "Test set: Avg. loss: 1.4654 Accuracy: 5343/10000 (53%)\n",
      "Test set: Avg. loss: 1.4451 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4536 Accuracy: 5332/10000 (53%)\n",
      "Test set: Avg. loss: 1.4402 Accuracy: 5331/10000 (53%)\n",
      "Test set: Avg. loss: 1.4518 Accuracy: 5347/10000 (53%)\n",
      "Test set: Avg. loss: 1.4519 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4461 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4398 Accuracy: 5345/10000 (53%)\n",
      "Test set: Avg. loss: 1.4480 Accuracy: 5405/10000 (54%)\n",
      "Test set: Avg. loss: 1.4604 Accuracy: 5329/10000 (53%)\n",
      "Test set: Avg. loss: 1.4533 Accuracy: 5315/10000 (53%)\n",
      "Test set: Avg. loss: 1.4733 Accuracy: 5304/10000 (53%)\n",
      "Test set: Avg. loss: 1.4417 Accuracy: 5320/10000 (53%)\n",
      "Test set: Avg. loss: 1.4445 Accuracy: 5342/10000 (53%)\n",
      "Test set: Avg. loss: 1.4561 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4649 Accuracy: 5264/10000 (53%)\n",
      "Test set: Avg. loss: 1.4654 Accuracy: 5297/10000 (53%)\n",
      "Test set: Avg. loss: 1.4461 Accuracy: 5371/10000 (54%)\n",
      "Test set: Avg. loss: 1.4499 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.4472 Accuracy: 5358/10000 (54%)\n",
      "Test set: Avg. loss: 1.4602 Accuracy: 5359/10000 (54%)\n",
      "Test set: Avg. loss: 1.4670 Accuracy: 5313/10000 (53%)\n",
      "Test set: Avg. loss: 1.4421 Accuracy: 5368/10000 (54%)\n",
      "Test set: Avg. loss: 1.4438 Accuracy: 5380/10000 (54%)\n",
      "Test set: Avg. loss: 1.4519 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4446 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4548 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4539 Accuracy: 5353/10000 (54%)\n",
      "Test set: Avg. loss: 1.4486 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.4410 Accuracy: 5361/10000 (54%)\n",
      "Test set: Avg. loss: 1.4492 Accuracy: 5369/10000 (54%)\n",
      "Test set: Avg. loss: 1.4598 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4540 Accuracy: 5328/10000 (53%)\n",
      "Test set: Avg. loss: 1.4524 Accuracy: 5316/10000 (53%)\n",
      "Test set: Avg. loss: 1.4543 Accuracy: 5325/10000 (53%)\n",
      "Test set: Avg. loss: 1.4616 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.4501 Accuracy: 5356/10000 (54%)\n",
      "Test set: Avg. loss: 1.4568 Accuracy: 5353/10000 (54%)\n",
      "Test set: Avg. loss: 1.4452 Accuracy: 5379/10000 (54%)\n",
      "Test set: Avg. loss: 1.4497 Accuracy: 5372/10000 (54%)\n",
      "Test set: Avg. loss: 1.4506 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4595 Accuracy: 5330/10000 (53%)\n",
      "Test set: Avg. loss: 1.4543 Accuracy: 5322/10000 (53%)\n",
      "Test set: Avg. loss: 1.4484 Accuracy: 5373/10000 (54%)\n",
      "Test set: Avg. loss: 1.4631 Accuracy: 5353/10000 (54%)\n",
      "Test set: Avg. loss: 1.4439 Accuracy: 5381/10000 (54%)\n",
      "Test set: Avg. loss: 1.4580 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4542 Accuracy: 5348/10000 (53%)\n",
      "Test set: Avg. loss: 1.4386 Accuracy: 5367/10000 (54%)\n",
      "Test set: Avg. loss: 1.4527 Accuracy: 5369/10000 (54%)\n",
      "Test set: Avg. loss: 1.4674 Accuracy: 5338/10000 (53%)\n",
      "Test set: Avg. loss: 1.4452 Accuracy: 5337/10000 (53%)\n",
      "Test set: Avg. loss: 1.4542 Accuracy: 5367/10000 (54%)\n",
      "Test set: Avg. loss: 1.4497 Accuracy: 5376/10000 (54%)\n",
      "Test set: Avg. loss: 1.4614 Accuracy: 5336/10000 (53%)\n",
      "Test set: Avg. loss: 1.4616 Accuracy: 5355/10000 (54%)\n",
      "Test set: Avg. loss: 1.4475 Accuracy: 5344/10000 (53%)\n",
      "Test set: Avg. loss: 1.4562 Accuracy: 5364/10000 (54%)\n",
      "Test set: Avg. loss: 1.4636 Accuracy: 5357/10000 (54%)\n",
      "Test set: Avg. loss: 1.4672 Accuracy: 5323/10000 (53%)\n",
      "Test set: Avg. loss: 1.4626 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4639 Accuracy: 5383/10000 (54%)\n",
      "Test set: Avg. loss: 1.4669 Accuracy: 5344/10000 (53%)\n",
      "Test set: Avg. loss: 1.4531 Accuracy: 5383/10000 (54%)\n",
      "Test set: Avg. loss: 1.4662 Accuracy: 5355/10000 (54%)\n",
      "Test set: Avg. loss: 1.4582 Accuracy: 5360/10000 (54%)\n",
      "Test set: Avg. loss: 1.4585 Accuracy: 5307/10000 (53%)\n",
      "Test set: Avg. loss: 1.4627 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4566 Accuracy: 5370/10000 (54%)\n",
      "Test set: Avg. loss: 1.4440 Accuracy: 5377/10000 (54%)\n",
      "Test set: Avg. loss: 1.4565 Accuracy: 5349/10000 (53%)\n",
      "Test set: Avg. loss: 1.4538 Accuracy: 5368/10000 (54%)\n",
      "Test set: Avg. loss: 1.4566 Accuracy: 5301/10000 (53%)\n",
      "Test set: Avg. loss: 1.4612 Accuracy: 5307/10000 (53%)\n",
      "Test set: Avg. loss: 1.4679 Accuracy: 5333/10000 (53%)\n",
      "Test set: Avg. loss: 1.4727 Accuracy: 5324/10000 (53%)\n",
      "Test set: Avg. loss: 1.4765 Accuracy: 5312/10000 (53%)\n",
      "Test set: Avg. loss: 1.4605 Accuracy: 5315/10000 (53%)\n",
      "Test set: Avg. loss: 1.4496 Accuracy: 5354/10000 (54%)\n",
      "Test set: Avg. loss: 1.4498 Accuracy: 5363/10000 (54%)\n",
      "Test set: Avg. loss: 1.4548 Accuracy: 5336/10000 (53%)\n",
      "Test set: Avg. loss: 1.4549 Accuracy: 5351/10000 (54%)\n",
      "Test set: Avg. loss: 1.4730 Accuracy: 5301/10000 (53%)\n",
      "Test set: Avg. loss: 1.4481 Accuracy: 5365/10000 (54%)\n",
      "Test set: Avg. loss: 1.4541 Accuracy: 5359/10000 (54%)\n",
      "Test set: Avg. loss: 1.4512 Accuracy: 5365/10000 (54%)\n",
      "Test set: Avg. loss: 1.4759 Accuracy: 5302/10000 (53%)\n",
      "Test set: Avg. loss: 1.4546 Accuracy: 5345/10000 (53%)\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "deep_dense = nn.Sequential(*([nn.Linear(64, 64), nn.ReLU(), nn.BatchNorm1d(64)] * 10)\n",
    ")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 32 * 3, 32 * 32),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32 * 32),\n",
    "    nn.Linear(32 * 32, 16 * 16),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16 * 16),\n",
    "    nn.Linear(16*16, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    deep_dense,\n",
    "    nn.Linear(64, 10),\n",
    ").to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_val(\n",
    "    network=model,\n",
    "    criterion=nn.functional.cross_entropy,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.25 * 1e-4),\n",
    "    train_loader=train_cifar10_loader,\n",
    "    val_loader=val_cifar10_loader,\n",
    "    n_epochs=1_000,\n",
    "    writer=writer,\n",
    "    logging_interval=20,\n",
    "    model_name='cifar10_dense_deep'\n",
    ")\n",
    "\n",
    "writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T05:01:06.924947900Z",
     "start_time": "2024-03-09T22:10:06.322175200Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Бонус. Побейте бейзлайн (3 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На датасете `CIFAR-10` обучите модель, которая выдает аккураси `>=0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-28T20:57:53.276092900Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
