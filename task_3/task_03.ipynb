{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "804px",
    "left": "148px",
    "top": "50px",
    "width": "555.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "313px",
    "left": "926px",
    "right": "27px",
    "top": "120px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30683,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# `Практикум по программированию на языке Python`\n",
    "\n",
    "## `Задание 03. Рекуррентные Нейронные Сети. Dropout. LM`\n",
    "\n",
    "#### Фамилия, имя: Богачев Владимир\n",
    "\n",
    "Дата выдачи: <span style=\"color:red\">__30 марта 23:59__</span>.\n",
    "\n",
    "Мягкий дедлайн: <span style=\"color:red\">__13 апреля 23:59__</span>.\n",
    "\n",
    "Стоимость: __10 баллов__ (основная часть заданий) + __7 баллов__ (дополнительные задания).\n",
    "\n",
    "<span style=\"color:red\">__В ноутбуке все клетки должны выполняться без ошибок при последовательном их выполнении.__</span>\n",
    "\n",
    "#### `Москва, 2024`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Данное задание будет состоять из двух частей:\n",
    "1. Применение рекуррентной сети для решения задачи классификации текста. Более конкретно -- предсказания рейтинга отзыва фильма.\n",
    "2. Простейшая лингвистическая модель для генерации текста на основе LSTM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "При выполнении задания вы обучите LSTM с разным уровнем \"коробочности\", а также познакомитесь с различными способами применения DropOut к рекуррентным архитектурам. В рекуррентных архитектурах вариантов, куда можно наложить бинарную маску шума, гораздо больше, чем в нейросетях прямого прохода.\n",
    "\n",
    "Во второй части вы попробуете реализовать простейший рекуррентный декодер для генерации текстов.\n",
    "\n",
    "Задание сделано так, чтобы его можно было выполнять на CPU, однако RNN - это ресурсоёмкая вещь, поэтому на GPU с ними работать приятнее. Можете попробовать использовать [https://colab.research.google.com](https://colab.research.google.com) - бесплатное облако с GPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для корректного отображения картинок, вам может понадобится сделать ноутбук доверенным (Trusted) в правом верхнем углу**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 0. Загрузка и предобработка данных (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Рекомендуемые гиперпараметры`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "max_length = 200\n",
    "top_n_words = 5000\n",
    "\n",
    "hidden_dim = 128\n",
    "embedding_dim = 32\n",
    "\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:22:46.468794Z",
     "iopub.execute_input": "2024-04-11T17:22:46.469179Z",
     "iopub.status.idle": "2024-04-11T17:22:46.476062Z",
     "shell.execute_reply.started": "2024-04-11T17:22:46.469149Z",
     "shell.execute_reply": "2024-04-11T17:22:46.474329Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:54:32.808717600Z",
     "start_time": "2024-04-15T18:54:32.770724Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первое, что нужно сделать — скачать, предобработать данные и организовать их таким образом, чтобы их можно было подавать в нейронную сеть.\n",
    "\n",
    "Для обеих частей задания мы будем использовать [**Large Movie Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Загрузка и предобработка данных`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузите данные по ссылке выше. (**tip**: используйте `wget`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:19.592935Z",
     "iopub.execute_input": "2024-04-11T17:25:19.593392Z",
     "iopub.status.idle": "2024-04-11T17:25:38.191124Z",
     "shell.execute_reply.started": "2024-04-11T17:25:19.593355Z",
     "shell.execute_reply": "2024-04-11T17:25:38.189556Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:54:32.878717600Z",
     "start_time": "2024-04-15T18:54:32.774843500Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Распакуйте скачанные данные в папку `aclImdb` (**tip:** используйте `tar`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!tar -xzf aclImdb_v1.tar.gz"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:38.194362Z",
     "iopub.execute_input": "2024-04-11T17:25:38.194831Z",
     "iopub.status.idle": "2024-04-11T17:25:49.331609Z",
     "shell.execute_reply.started": "2024-04-11T17:25:38.194788Z",
     "shell.execute_reply": "2024-04-11T17:25:49.330257Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:20.540608400Z",
     "start_time": "2024-04-15T18:54:32.792717600Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрите в файле `./aclImdb/README` как организованы данные:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!cat ./aclImdb/train/pos/10003_8.txt"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:49.333046Z",
     "iopub.execute_input": "2024-04-11T17:25:49.333428Z",
     "iopub.status.idle": "2024-04-11T17:25:50.441878Z",
     "shell.execute_reply.started": "2024-04-11T17:25:49.333392Z",
     "shell.execute_reply": "2024-04-11T17:25:50.440493Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:20.562558100Z",
     "start_time": "2024-04-15T18:55:20.542608200Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_data_path = './aclImdb/test/'\n",
    "train_data_path = './aclImdb/train/'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:50.446497Z",
     "iopub.execute_input": "2024-04-11T17:25:50.447293Z",
     "iopub.status.idle": "2024-04-11T17:25:50.452552Z",
     "shell.execute_reply.started": "2024-04-11T17:25:50.447247Z",
     "shell.execute_reply": "2024-04-11T17:25:50.451165Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:20.619063800Z",
     "start_time": "2024-04-15T18:55:20.562037400Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:50.454104Z",
     "iopub.execute_input": "2024-04-11T17:25:50.455185Z",
     "iopub.status.idle": "2024-04-11T17:25:57.205948Z",
     "shell.execute_reply.started": "2024-04-11T17:25:50.455150Z",
     "shell.execute_reply": "2024-04-11T17:25:57.204408Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.301159400Z",
     "start_time": "2024-04-15T18:55:20.563552400Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060]\n",
      "[nltk_data]     Попытка установить соединение была безуспешной, т.к.\n",
      "[nltk_data]     от другого компьютера за требуемое время не получен\n",
      "[nltk_data]     нужный отклик, или было разорвано уже установленное\n",
      "[nltk_data]     соединение из-за неверного отклика уже подключенного\n",
      "[nltk_data]     компьютера>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Стандартной предобработкой данных является токенизация текстов. Полученные токены можно будет закодировать и затем подавать на вход нейронной сети. Ключевым моментом, который влияет на скорость работы нейросети и её размер в памяти — размер словаря, используемого при токенизации. Для задачи классификации мы можем убрать часть слов (стоп слова, редкие слова), ускорив обучение без потери в качестве."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:57.207709Z",
     "iopub.execute_input": "2024-04-11T17:25:57.208275Z",
     "iopub.status.idle": "2024-04-11T17:25:57.223033Z",
     "shell.execute_reply.started": "2024-04-11T17:25:57.208243Z",
     "shell.execute_reply": "2024-04-11T17:25:57.221565Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.382137100Z",
     "start_time": "2024-04-15T18:55:42.261671100Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:57.225050Z",
     "iopub.execute_input": "2024-04-11T17:25:57.225517Z",
     "iopub.status.idle": "2024-04-11T17:25:58.312143Z",
     "shell.execute_reply.started": "2024-04-11T17:25:57.225470Z",
     "shell.execute_reply": "2024-04-11T17:25:58.311031Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.386136300Z",
     "start_time": "2024-04-15T18:55:42.286644600Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуйте функцию для токенизации текста. Выполнять токенизацию можно по-разному, но в данном задании предлагается это делать следующим образом:\n",
    "1. Привести текст к нижнему регистру\n",
    "2. Убрать html разметку из текстов (`<br />`, ...)\n",
    "3. Убрать все символы кроме латинских букв\n",
    "4. Разбить строку по пробелам\n",
    "5. Убрать стоп слова"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.313717Z",
     "iopub.execute_input": "2024-04-11T17:25:58.314345Z",
     "iopub.status.idle": "2024-04-11T17:25:58.696667Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.314303Z",
     "shell.execute_reply": "2024-04-11T17:25:58.695352Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.432138500Z",
     "start_time": "2024-04-15T18:55:42.292157800Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param str text: Input text \n",
    "    :return List[str]: List of words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    text = text.translate(translator)\n",
    "    text = word_tokenize(text, language='english')\n",
    "    text = list(filter(lambda w: w not in STOPWORDS, text))\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.698220Z",
     "iopub.execute_input": "2024-04-11T17:25:58.698862Z",
     "iopub.status.idle": "2024-04-11T17:25:58.708538Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.698824Z",
     "shell.execute_reply": "2024-04-11T17:25:58.706709Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.435151200Z",
     "start_time": "2024-04-15T18:55:42.338403800Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenize('1. Hello <br /> words!! <br />')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.714426Z",
     "iopub.execute_input": "2024-04-11T17:25:58.715170Z",
     "iopub.status.idle": "2024-04-11T17:25:58.748140Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.715128Z",
     "shell.execute_reply": "2024-04-11T17:25:58.746737Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.450152100Z",
     "start_time": "2024-04-15T18:55:42.340579700Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['hello', 'words']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь мы можем создать словарь, с помощью которого мы будем численно кодировать токены из текста и наоборот.\n",
    "\n",
    "Удобной обёрткой для создания словарей является класс `torchtext.vocab.Vocab` и фабрика для создания таких классов `torchtext.vocab.vocab`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torchtext.vocab.vocab??"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.749880Z",
     "iopub.execute_input": "2024-04-11T17:25:58.750793Z",
     "iopub.status.idle": "2024-04-11T17:25:58.849637Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.750745Z",
     "shell.execute_reply": "2024-04-11T17:25:58.847692Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.570669800Z",
     "start_time": "2024-04-15T18:55:42.348621400Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы создать такой словарь, сначала нужно создать словарь со всеми токенами в тексте и их частотами встречаемости:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:51:55.300753Z",
     "start_time": "2021-04-01T19:51:55.275188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "counter = defaultdict(int)\n",
    "\n",
    "for path in ['./aclImdb/test/neg', './aclImdb/test/pos', './aclImdb/train/neg', './aclImdb/train/pos']:\n",
    "    for file_path in os.listdir(path):\n",
    "        text = open(os.path.join(path, file_path), 'r', encoding='utf-8', errors='ignore').read().strip()\n",
    "        for token in tokenize(text):\n",
    "            counter[token] += 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.852596Z",
     "iopub.execute_input": "2024-04-11T17:25:58.853845Z",
     "iopub.status.idle": "2024-04-11T17:28:46.997932Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.853790Z",
     "shell.execute_reply": "2024-04-11T17:28:46.996330Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.239438200Z",
     "start_time": "2024-04-15T18:55:42.380136300Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir\\AppData\\Local\\Temp\\ipykernel_12668\\3422549362.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text).get_text()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для работы с текстами нам необходимо зарезервировать два специальных токена:\n",
    "1. `<pad>` для токена означающего паддинг\n",
    "2. `<unk>` для токенов, которые отсутствуют в словаре"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# specials = ['<pad>', '<unk>']\n",
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for special in specials:\n",
    "    counter[special] = 0"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:46.999768Z",
     "iopub.execute_input": "2024-04-11T17:28:47.000149Z",
     "iopub.status.idle": "2024-04-11T17:28:47.005985Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.000119Z",
     "shell.execute_reply": "2024-04-11T17:28:47.004988Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.249047600Z",
     "start_time": "2024-04-15T18:56:27.239438200Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте словарь из словаря частот `counter`. Наименьшие *id* отдайте под специальные токены. \n",
    "\n",
    "Отбросьте низкочастотные слова, оставив только `top_n_words` слов. Можете использовать любой способ реализации этого условия, например:\n",
    "1. Оставить в словаре `counter` нужное число слов\n",
    "2. Подобрать параметр `min_freq`, чтобы оставшееся число слов было близко к необходимому порогу"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torchtext.__version__"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.007190Z",
     "iopub.execute_input": "2024-04-11T17:28:47.008227Z",
     "iopub.status.idle": "2024-04-11T17:28:47.028116Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.008192Z",
     "shell.execute_reply": "2024-04-11T17:28:47.026746Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.261046100Z",
     "start_time": "2024-04-15T18:56:27.242694400Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.16.0+cpu'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab = torchtext.vocab.vocab(\n",
    "    counter,\n",
    "    min_freq=145,\n",
    "    specials=specials,\n",
    ")\n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.029902Z",
     "iopub.execute_input": "2024-04-11T17:28:47.030270Z",
     "iopub.status.idle": "2024-04-11T17:28:47.199189Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.030230Z",
     "shell.execute_reply": "2024-04-11T17:28:47.197799Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.347669300Z",
     "start_time": "2024-04-15T18:56:27.247047100Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab, len(vocab)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.200582Z",
     "iopub.execute_input": "2024-04-11T17:28:47.201012Z",
     "iopub.status.idle": "2024-04-11T17:28:47.208352Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.200984Z",
     "shell.execute_reply": "2024-04-11T17:28:47.206947Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.368671300Z",
     "start_time": "2024-04-15T18:56:27.282679900Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(Vocab(), 5037)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab.lookup_indices(['<pad>', '<unk>'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.210021Z",
     "iopub.execute_input": "2024-04-11T17:28:47.210648Z",
     "iopub.status.idle": "2024-04-11T17:28:47.223755Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.210601Z",
     "shell.execute_reply": "2024-04-11T17:28:47.222380Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.372669800Z",
     "start_time": "2024-04-15T18:56:27.286902700Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab.lookup_indices(['this', 'film', 'was', 'awful'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.225104Z",
     "iopub.execute_input": "2024-04-11T17:28:47.225510Z",
     "iopub.status.idle": "2024-04-11T17:28:47.238847Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.225476Z",
     "shell.execute_reply": "2024-04-11T17:28:47.237637Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.389669900Z",
     "start_time": "2024-04-15T18:56:27.289902600Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 98, 1, 422]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь мы готовы создать обёртку-датасет для наших данных. \n",
    "\n",
    "Необходимо добавить несколько опции, которые понадобятся во второй части задания:\n",
    "1. Ограничение на максимальную длину текста в токенах. Если текст оказывается длиннее, то последние токены отбрасываются\n",
    "2. Возможность добавить в специальные токены `<sos>`, `<eos>` в начало и конец токенизированного текста\n",
    "    \n",
    "**tips:**\n",
    "1. Обратите особое внимание, что у длинных текстов не должен обрезаться паддинг\n",
    "2. В исходных данных рейтинг закодирован в названии файла в виде числа от $1$ до $10$. Для удобства, вычтите $1$, чтобы рейтинг был от $0$ до $9$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import re"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.241391Z",
     "iopub.execute_input": "2024-04-11T17:28:47.242132Z",
     "iopub.status.idle": "2024-04-11T17:28:47.253250Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.242077Z",
     "shell.execute_reply": "2024-04-11T17:28:47.251777Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.393669600Z",
     "start_time": "2024-04-15T18:56:27.293663300Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "re_rating = re.compile('_[0-9]*')\n",
    "re.search(re_rating, './aclImdb/train/pos/10003_10.txt').group(0)[1:]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.255370Z",
     "iopub.execute_input": "2024-04-11T17:28:47.256186Z",
     "iopub.status.idle": "2024-04-11T17:28:47.271859Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.256139Z",
     "shell.execute_reply": "2024-04-11T17:28:47.270616Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.405670700Z",
     "start_time": "2024-04-15T18:56:27.296475100Z"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'10'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_raiting(path: str, re_rating = None):\n",
    "    re_rating = '_[0-9]*' if re_rating is None else re_rating\n",
    "    \n",
    "    rating = re.search(re_rating, path).group(0)[1:]\n",
    "    return int(rating) - 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.273661Z",
     "iopub.execute_input": "2024-04-11T17:28:47.274645Z",
     "iopub.status.idle": "2024-04-11T17:28:47.284077Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.274598Z",
     "shell.execute_reply": "2024-04-11T17:28:47.282140Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.408669600Z",
     "start_time": "2024-04-15T18:56:27.301888600Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class LargeMovieReviewDataset(Dataset):\n",
    "    def __init__(self, data_path, vocab, max_len, pad_sos=False, pad_eos=False):\n",
    "        \"\"\"\n",
    "        :param str data_path: Path to folder with one of the data splits (train or test)\n",
    "        :param torchtext.vocab.Vocab vocab: dictionary with lookup_indices method\n",
    "        :param int max_len: Maximum length of tokenized text\n",
    "        :param bool pad_sos: If True pad sequence at the beginning with <sos> \n",
    "        :param bool pad_eos: If True pad sequence at the end with <eos>         \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pad_sos = pad_sos\n",
    "        if self.pad_sos:\n",
    "            self.sos_id = vocab.lookup_indices(['<sos>'])[0]\n",
    "        self.pad_eos = pad_eos\n",
    "        if self.pad_eos:\n",
    "            self.eos_id = vocab.lookup_indices(['<eos>'])[0]\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.data_path = data_path\n",
    "        self.negative_path = os.path.join(data_path, 'neg')\n",
    "        self.positive_path = os.path.join(data_path, 'pos')\n",
    "        \n",
    "        self.negative_paths = []\n",
    "        self.positive_paths = []\n",
    "\n",
    "        for file_path in os.listdir(self.negative_path):\n",
    "            self.negative_paths.append(os.path.join(self.negative_path, file_path))\n",
    "\n",
    "        for file_path in os.listdir(self.positive_path):\n",
    "            self.positive_paths.append(os.path.join(self.positive_path, file_path))\n",
    "        \n",
    "        self.texts = []\n",
    "        self.tokens = []\n",
    "        self.ratings = []\n",
    "        self.labels = [0] * len(self.negative_paths) + [1] * len(self.positive_paths)\n",
    "        \n",
    "        # Read each file in data_path, tokenize it, get tokens ids, its rating and store\n",
    "        re_rating = re.compile('_[0-9]*')\n",
    "        for path in self.negative_paths + self.positive_paths:\n",
    "            # YOUR CODE HERE\n",
    "            with open(path, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read().strip()\n",
    "            self.texts.append(text)\n",
    "            \n",
    "            txt_tokens = vocab.lookup_indices(tokenize(text))[0:self.max_len]\n",
    "            if self.pad_sos:\n",
    "                txt_tokens.insert(0, vocab['<sos>'])\n",
    "            if self.pad_eos:\n",
    "                txt_tokens.append(vocab['<eos>'])\n",
    "            \n",
    "            self.tokens.append(txt_tokens)\n",
    "            self.ratings.append(get_raiting(path, re_rating=re_rating))\n",
    "        \n",
    "        self.ratings = torch.LongTensor(self.ratings)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        self.tokens = [torch.LongTensor(tls) for tls in self.tokens]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :param int idx: index of object in dataset\n",
    "        :return dict: Dictionary with all useful object data \n",
    "            {\n",
    "                'text' str: unprocessed text,\n",
    "                'label' torch.Tensor(dtype=torch.long): sentiment of the text (0 for negative, 1 for positive)\n",
    "                'rating' torch.Tensor(dtype=torch.long): rating of the text\n",
    "                'tokens' torch.Tensor(dtype=torch.long): tensor of tokens ids for the text\n",
    "                'tokens_len' torch.Tensor(dtype=torch.long): number of tokens\n",
    "            }\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        res = {\n",
    "            'text': self.texts[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'rating': self.ratings[idx],\n",
    "            'tokens': self.tokens[idx],\n",
    "            'tokens_len': self.tokens[idx].shape[0]\n",
    "        }\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return int: number of objects in dataset \n",
    "        \"\"\"\n",
    "        return len(self.tokens)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.285804Z",
     "iopub.execute_input": "2024-04-11T17:28:47.286395Z",
     "iopub.status.idle": "2024-04-11T17:28:47.306007Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.286361Z",
     "shell.execute_reply": "2024-04-11T17:28:47.305049Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.412669600Z",
     "start_time": "2024-04-15T18:56:27.308889700Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте датасеты для тестовой и обучающей выборки. \n",
    "\n",
    "Обратите внимание, что для задачи классификации нам не потребуется паддинг с помощью `<sos>`, `<eos>`. \n",
    "\n",
    "Не забудьте обрезать длинные тексты, передав параметр `max_length`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pathlib"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.415670Z",
     "start_time": "2024-04-15T18:56:27.313108300Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "datasets_dump_path = pathlib.Path(\"./datasets_dump.bin\")\n",
    "\n",
    "if not datasets_dump_path.exists():\n",
    "    test_dataset = LargeMovieReviewDataset(test_data_path, vocab, max_len=max_length)\n",
    "    train_dataset = LargeMovieReviewDataset(train_data_path, vocab, max_len=max_length)\n",
    "    \n",
    "    with open(datasets_dump_path, \"wb\") as f:\n",
    "        torch.save(\n",
    "            obj=(test_dataset, train_dataset),\n",
    "            f=f\n",
    "        )\n",
    "else:\n",
    "    with open(datasets_dump_path, \"rb\") as f:\n",
    "        test_dataset, train_dataset = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:28.623920100Z",
     "start_time": "2024-04-15T18:56:27.315671900Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",\n 'label': tensor(0),\n 'rating': tensor(1),\n 'tokens': tensor([ 4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  1,\n         20, 21, 22, 23, 24, 25, 26, 15, 20, 27, 15,  1,  1,  1,  1, 28, 29, 30,\n         31, 32, 33, 34, 35, 36, 37, 38,  1, 19,  1, 39, 40,  1, 41, 42, 43, 44,\n         45, 46, 47,  1, 48, 17,  1, 18, 49,  1, 50, 51, 52,  1,  1, 53, 54, 55,\n         56, 57]),\n 'tokens_len': 74}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:24.848134700Z",
     "start_time": "2024-04-15T18:57:24.792770200Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим, как выглядит объект в датасете:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "for d in train_dataset:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\"\n",
    "    \n",
    "for d in test_dataset:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\""
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:33.858064Z",
     "iopub.execute_input": "2024-04-11T17:31:33.858477Z",
     "iopub.status.idle": "2024-04-11T17:31:34.788422Z",
     "shell.execute_reply.started": "2024-04-11T17:31:33.858426Z",
     "shell.execute_reply": "2024-04-11T17:31:34.787016Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:29.659055900Z",
     "start_time": "2024-04-15T18:57:29.092978500Z"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset[-2]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.808174Z",
     "iopub.execute_input": "2024-04-11T17:31:34.808645Z",
     "iopub.status.idle": "2024-04-11T17:31:34.828293Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.808610Z",
     "shell.execute_reply": "2024-04-11T17:31:34.826392Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:30.263105900Z",
     "start_time": "2024-04-15T18:57:30.181483400Z"
    }
   },
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"This movie, with all its complexity and subtlety, makes for one of the most thought-provoking short films I have ever seen. The topics it addresses are ugly, cynical, and at times, even macabre, but the film remains beautiful in its language, artful with its camera angles, and gorgeous in its style, skillfully recreating the short story of the same name written by a master of short stories, Tobias Wolff.<br /><br />Not wishing to spoil anything of the movie, I won't go into any details, other than to say that this movie is magnificent in and of itself. It takes pride in what it does, and does it well. It shows the most important memories of life, all of which can be topped by the single most elusive feeling: unexpected bliss. This movie, of its own volition, has created in me the same feelings the main character (Tom Noonan) felt when words transformed his very existence, and that is one impressive feat.\",\n 'label': tensor(1),\n 'rating': tensor(9),\n 'tokens': tensor([   6, 3680, 3314,   97,   79, 4861,  603,   61,  419,  111,    1,    1,\n          188, 1967,   74,  198,    1,   98,  760, 1733, 2546,    1,  128, 1951,\n         2747, 1365,    1,    1,  603,  301, 1457, 1533, 2836,  603,  819,    1,\n            1,  361, 1227,  665,    6,  252,   80, 2067,  542,    6, 4399, 1115,\n         3588,   42,   37, 1880, 3975, 1012,    1,  153,    1,  258, 4448,    1,\n            6,    1, 2124,  975,  232,   20,  950,    1,  578, 2419,    1, 2329,\n           79, 2513,    1]),\n 'tokens_len': 75}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь нам нужно создать `DataLoader` для наших данных. `DataLoader` умеет из коробки объединять список объектов из датасета в один батч, даже когда датасет возвращает словарь тензоров. Однако, это работает только в случае когда все эти тензоры имеют один и тот же размер во всех батчах. В нашем случае, это не так, так как разные тексты могут иметь разную длину.\n",
    "\n",
    "Чтобы обойти эту проблему у `DataLoader` есть параметр `collate_fn`, который позволяет задать функцию для объединения списка объектов в один батч."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы объединить несколько тензоров разной длины в один можно использовать функцию `torch.nn.utils.rnn.pad_sequence`\n",
    "\n",
    "Обратите внимание на её аргументы:\n",
    "1. `batch_first` определяет по какой оси \"складывать\" тензоры. Предпочтительнее использовать `batch_first=False` так как это может упростить выполнение задания в дальнейшем \n",
    "2. `padding_value` — число, которое будет использоваться в качестве паддинга, чтобы сделать все тензоры одинаковой длины"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.nn.utils.rnn.pad_sequence([\n",
    "    torch.tensor([1, 2, 3]),\n",
    "    torch.tensor([4, 5]),\n",
    "    torch.tensor([6, 7, 8, 9])\n",
    "], batch_first=False, padding_value=-1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.829776Z",
     "iopub.execute_input": "2024-04-11T17:31:34.830242Z",
     "iopub.status.idle": "2024-04-11T17:31:34.871668Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.830197Z",
     "shell.execute_reply": "2024-04-11T17:31:34.869604Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:31.817537400Z",
     "start_time": "2024-04-15T18:57:31.761618600Z"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  4,  6],\n        [ 2,  5,  7],\n        [ 3, -1,  8],\n        [-1, -1,  9]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch, padding_value, batch_first=False):\n",
    "    \"\"\"\n",
    "    :param List[Dict] batch: List of objects from dataset\n",
    "    :param int padding_value: Value that will be used to pad tokens\n",
    "    :param bool batch_first: If True resulting tensor with tokens must have shape [B, T] otherwise [T, B]\n",
    "    :return dict: Dictionary with all data collated\n",
    "        {\n",
    "            'ratings' torch.Tensor(dtype=torch.long): rating of the text for each object in batch\n",
    "            'labels' torch.Tensor(dtype=torch.long): sentiment of the text for each object in batch\n",
    "            \n",
    "            'texts' List[str]: All texts in one list\n",
    "            'tokens' torch.Tensor(dtype=torch.long): tensor of tokens ids padded with @padding_value\n",
    "            'tokens_lens' torch.Tensor(dtype=torch.long): number of tokens for each object in batch\n",
    "        }\n",
    "    \"\"\"\n",
    "    ratings = torch.LongTensor(size=(len(batch), ))\n",
    "    labels = torch.LongTensor(size=(len(batch), ))\n",
    "    texts = []\n",
    "    tokens = []\n",
    "    tokens_lens = torch.LongTensor(size=(len(batch), ))\n",
    "    \n",
    "    for i, batch_dir in enumerate(batch):\n",
    "        ratings[i] = batch_dir['rating']\n",
    "        labels[i] = batch_dir['label']\n",
    "        texts.append(batch_dir['text'])\n",
    "        tokens.append(batch_dir['tokens'])\n",
    "        tokens_lens[i] = batch_dir['tokens_len']\n",
    "    \n",
    "    tokens = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=batch_first, padding_value=padding_value)\n",
    "    \n",
    "    res = {\n",
    "            'texts': texts,\n",
    "            'labels': labels,\n",
    "            'ratings': ratings,\n",
    "            'tokens': tokens,\n",
    "            'tokens_lens': tokens_lens,\n",
    "        }\n",
    "    \n",
    "    return res"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.873549Z",
     "iopub.execute_input": "2024-04-11T17:31:34.873934Z",
     "iopub.status.idle": "2024-04-11T17:31:34.895676Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.873901Z",
     "shell.execute_reply": "2024-04-11T17:31:34.891939Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:32.538728900Z",
     "start_time": "2024-04-15T18:57:32.481744400Z"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте даталоадеры с использованием `collate_fn`.\n",
    "\n",
    "**tips**:\n",
    "1. Передать в `collate_fn` правильное значение паддинга можно, например, с помощью `functools.partial`\n",
    "2. Если вы работаете в Google Colab, то, возможно, вам будет необходимо установить `num_workers=0` во избежание падения ноутбука."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import functools"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.905939Z",
     "iopub.execute_input": "2024-04-11T17:31:34.907603Z",
     "iopub.status.idle": "2024-04-11T17:31:34.913490Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.907545Z",
     "shell.execute_reply": "2024-04-11T17:31:34.911716Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:33.534281200Z",
     "start_time": "2024-04-15T18:57:33.493291Z"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "collate_fn_ = functools.partial(collate_fn, padding_value=vocab['<pad>'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0, collate_fn=collate_fn_)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.917306Z",
     "iopub.execute_input": "2024-04-11T17:31:34.918343Z",
     "iopub.status.idle": "2024-04-11T17:31:34.932951Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.918275Z",
     "shell.execute_reply": "2024-04-11T17:31:34.931536Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:34.049316500Z",
     "start_time": "2024-04-15T18:57:34.019318900Z"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for d in train_dataloader:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\"\n",
    "\n",
    "for d in test_dataloader:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\""
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.935437Z",
     "iopub.execute_input": "2024-04-11T17:31:34.935969Z",
     "iopub.status.idle": "2024-04-11T17:31:36.315937Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.935933Z",
     "shell.execute_reply": "2024-04-11T17:31:36.314514Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:35.557648400Z",
     "start_time": "2024-04-15T18:57:34.584846Z"
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим на какой-нибудь батч:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "batch.keys(), batch['labels'], batch['ratings'], batch['tokens'], batch['tokens_lens']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.317864Z",
     "iopub.execute_input": "2024-04-11T17:31:36.318299Z",
     "iopub.status.idle": "2024-04-11T17:31:36.345228Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.318263Z",
     "shell.execute_reply": "2024-04-11T17:31:36.343273Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:36.365314Z",
     "start_time": "2024-04-15T18:57:36.282315800Z"
    }
   },
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(dict_keys(['texts', 'labels', 'ratings', 'tokens', 'tokens_lens']),\n tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n tensor([1, 3, 0, 2, 2, 1, 1, 1, 3, 3, 2, 2, 1, 0, 0, 3, 1, 3, 2, 0, 0, 0, 3, 0,\n         2, 3, 3, 2, 1, 2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 3, 0, 3, 1, 0, 0,\n         0, 1, 0, 3, 0, 0, 0, 3, 3, 1, 3, 3, 2, 0, 0, 0]),\n tensor([[   4,   58,  136,  ..., 1984,  514,    1],\n         [   1,   59,  137,  ...,  284,    1,  274],\n         [   5,   60,  138,  ...,  349,    1, 2002],\n         ...,\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0]]),\n tensor([ 74, 128, 108, 168, 137,  52,  74,  74,  72,  98,  59, 143, 134,  52,\n         104, 112,  67, 116, 189,  47,  36,  96, 200, 200, 136, 111, 105, 200,\n         200, 144,  75,  82, 184,  99, 156, 132, 131,  56, 182, 106,  67,  61,\n          86, 200, 113,  66, 200,  55, 115,  77,  56, 145, 130,  29,  64,  60,\n         200,  16, 151,  87,  64,  71,  83,  87]))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 1. Классификация текстов (4 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Сборка и обучение RNN в pytorch (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим переменные для device-agnostic кода:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dtype, device, cuda_device_id = torch.float32, None, 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{0}'.format(str(cuda_device_id) if cuda_device_id is not None else '')\n",
    "if cuda_device_id is not None and torch.cuda.is_available():\n",
    "    device = 'cuda:{0:d}'.format(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}, dtype: {dtype}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.346749Z",
     "iopub.execute_input": "2024-04-11T17:31:36.347147Z",
     "iopub.status.idle": "2024-04-11T17:31:36.356022Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.347114Z",
     "shell.execute_reply": "2024-04-11T17:31:36.354771Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:39.472501800Z",
     "start_time": "2024-04-15T18:57:39.407469Z"
    }
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0, dtype: torch.float32\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Наша нейросеть будет обрабатывать входную последовательность по словам (word level). Мы будем использовать простую и стандартную рекуррентную архитектуру для классификации:\n",
    "1. Слой представлений, превращающий id токена в вектор-эмбеддинг этого слова\n",
    "2. Слой LSTM\n",
    "3. Полносвязный слой, предсказывающий выход по последнему скрытому состоянию\n",
    "\n",
    "Ниже дан код для сборки и обучения нашей нейросети."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите класс-обёртку над LSTM для задачи классификации. \n",
    "**Не используйте циклы.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:59:16.467178Z",
     "start_time": "2021-04-01T20:59:16.441112Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.357634Z",
     "iopub.execute_input": "2024-04-11T17:31:36.358609Z",
     "iopub.status.idle": "2024-04-11T17:31:36.371926Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.358572Z",
     "shell.execute_reply": "2024-04-11T17:31:36.370373Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:41.303459Z",
     "start_time": "2024-04-15T18:57:41.241463500Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "emb_dump = None\n",
    "inp_dump = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:41.779950100Z",
     "start_time": "2024-04-15T18:57:41.762437200Z"
    }
   },
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim, hidden_dim, output_size, vocab,\n",
    "        rec_layer=torch.nn.LSTM, dropout=None, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Create a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        #    Use torch.nn.Embedding. Do not forget specify padding_idx!\n",
    "        # YOUR CODE HERE\n",
    "        self.word_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=len(vocab),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab['<pad>'],\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        if dropout is not None:\n",
    "            self.rnn = rec_layer(\n",
    "                input_size=embedding_dim, \n",
    "                hidden_size=hidden_dim, \n",
    "                device=device,\n",
    "                dropout=dropout,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = rec_layer(\n",
    "                input_size=embedding_dim, \n",
    "                hidden_size=hidden_dim, \n",
    "                device=device,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        # Create linear layer for classification\n",
    "        # YOUR CODE HERE\n",
    "        self.output = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_size),\n",
    "        )\n",
    "        # self.output = nn.Sequential(\n",
    "        #     nn.Linear(hidden_dim, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm1d(64),\n",
    "        #     nn.Linear(64, output_size),\n",
    "        # )\n",
    "    \n",
    "    def forward(self, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor(dtype=torch.long) tokens: Batch of texts represented with tokens.\n",
    "        :param torch.Tensor(dtype=torch.long) tokens_lens: Number of non-padding tokens for each object in batch.\n",
    "        :return torch.Tensor(dtype=torch.long): Vector representation for each sequence in batch\n",
    "        \"\"\"\n",
    "        # Evaluate embeddings\n",
    "        # DEBUG: store last input in globals\n",
    "        global inp_dump\n",
    "        global emb_dump\n",
    "        \n",
    "        # DEBUG: store last input in globals\n",
    "        inp_dump = tokens.detach()\n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Tokens has NaNs\"\n",
    "        \n",
    "        x = self.word_embeddings(tokens)\n",
    "        # DEBUG: store last input in globals\n",
    "        emb_dump = x.detach()\n",
    "        \n",
    "        assert not torch.any(torch.isnan(x)), f\"Embeddings has NaNs\"\n",
    "        \n",
    "        # Make forward pass through recurrent network\n",
    "        # YOUR CODE HERE\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Pass output from rnn to linear layer \n",
    "        # Note: each object in batch has its own length \n",
    "        #     so we must take rnn hidden state after the last token for each text in batch        \n",
    "        x = x[tokens_lens - 1, torch.arange(0, x.shape[1]), :]\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.373584Z",
     "iopub.execute_input": "2024-04-11T17:31:36.374996Z",
     "iopub.status.idle": "2024-04-11T17:31:36.393418Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.374920Z",
     "shell.execute_reply": "2024-04-11T17:31:36.392097Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:42.261618900Z",
     "start_time": "2024-04-15T18:57:42.205620400Z"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Исходный код LSTM](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите функции для обучения и оценки модели:\n",
    "\n",
    "**tip:**\n",
    "1. В функции `evaluate` при подсчёте метрик учитывайте, что батчи могут иметь разный размер. (в частности последний батч)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.394775Z",
     "iopub.execute_input": "2024-04-11T17:31:36.395929Z",
     "iopub.status.idle": "2024-04-11T17:31:37.839947Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.395887Z",
     "shell.execute_reply": "2024-04-11T17:31:37.837984Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:44.711806300Z",
     "start_time": "2024-04-15T18:57:43.870900600Z"
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def set_global_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "#     torch.use_deterministic_algorithms(True) # если нужно гарантировать 1000% воспроизводимость\n",
    "\n",
    "    # Для Dataloader\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Для каждого woerker в Daaloader\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:37.842763Z",
     "iopub.execute_input": "2024-04-11T17:31:37.843189Z",
     "iopub.status.idle": "2024-04-11T17:31:37.852490Z",
     "shell.execute_reply.started": "2024-04-11T17:31:37.843154Z",
     "shell.execute_reply": "2024-04-11T17:31:37.850826Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:44.721698900Z",
     "start_time": "2024-04-15T18:57:44.714182400Z"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Make optimizer step\n",
    "        \n",
    "        # labels = data['labels'].to(device)\n",
    "        ratings = data['ratings'].to(device)\n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Eval epoch {idx} has NaNs\"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    for idx, data in enumerate(dataloader):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Evaluate accuracy\n",
    "        \n",
    "        # labels = data['labels'].to(device)\n",
    "        ratings = data['ratings'].to(device)\n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Eval epoch {idx} has NaNs\"\n",
    "        \n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, ratings)\n",
    "        total_loss += loss\n",
    "        \n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        total_accuracy += torch.sum(pred == ratings).item()\n",
    "        \n",
    "    return total_loss / len(dataloader.dataset), total_accuracy / len(dataloader.dataset)\n",
    "    \n",
    "\n",
    "def train(\n",
    "    train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs, name,\n",
    "):\n",
    "    wandb.init(project=\"MMP_prac_rnn_deb\", name=name)\n",
    "    \n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    rng = tqdm(range(num_epochs))\n",
    "    \n",
    "    for epoch in rng:\n",
    "        train_epoch(train_loader, model, loss_fn, optimizer, device)\n",
    "        \n",
    "        train_loss, train_acc = evaluate(train_loader, model, loss_fn, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss, test_acc = evaluate(test_loader, model, loss_fn, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        wandb.log({\"eval/loss\": test_loss, \"eval/accuracy\": test_acc}, step=epoch)\n",
    "        wandb.log({\"train/loss\": train_loss, \"train/accuracy\": train_acc}, step=epoch)\n",
    "        \n",
    "        print(\n",
    "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. Accuracy (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
    "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.177562Z",
     "iopub.execute_input": "2024-04-11T17:34:50.178008Z",
     "iopub.status.idle": "2024-04-11T17:34:50.203391Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.177974Z",
     "shell.execute_reply": "2024-04-11T17:34:50.201917Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:44.983495800Z",
     "start_time": "2024-04-15T18:57:44.927274600Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.486380Z",
     "iopub.execute_input": "2024-04-11T17:34:50.487487Z",
     "iopub.status.idle": "2024-04-11T17:34:50.498593Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.487434Z",
     "shell.execute_reply": "2024-04-11T17:34:50.497242Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:45.808183Z",
     "start_time": "2024-04-15T18:57:45.735402700Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=torch.nn.LSTM, dropout=None\n",
    ").to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.646098Z",
     "iopub.execute_input": "2024-04-11T17:34:50.647421Z",
     "iopub.status.idle": "2024-04-11T17:34:50.661479Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.647372Z",
     "shell.execute_reply": "2024-04-11T17:34:50.659595Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:46:24.923655200Z",
     "start_time": "2024-04-14T08:46:21.001667500Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим класс для подсчёта функции потерь и оптимизатор:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:52.169426Z",
     "iopub.execute_input": "2024-04-11T17:34:52.169838Z",
     "iopub.status.idle": "2024-04-11T17:34:52.177144Z",
     "shell.execute_reply.started": "2024-04-11T17:34:52.169807Z",
     "shell.execute_reply": "2024-04-11T17:34:52.175838Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:46:25.213590600Z",
     "start_time": "2024-04-14T08:46:24.923655200Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем обучить модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"Basic RNN ratings\",\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:57.048782Z",
     "iopub.execute_input": "2024-04-11T17:34:57.049265Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:47:55.873096900Z",
     "start_time": "2024-04-14T08:46:25.213590600Z"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbogachevv\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240414_114627-b5aij9ql</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">Basic RNN ratings</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd97d54cef9d46d79c1c3d93df5f018b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b9a35d153da41718eed73459b47ac6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.272/0.262\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc2d260de0dc4da3be41b341d1e44160"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.027/0.028. Accuracy (Train/Test): 0.358/0.332\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60e6ae419ca344d0bfcaa8aada6bb63e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.385/0.324\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9b7c1ffe2e346cca0bf2226604d2360"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.025/0.028. Accuracy (Train/Test): 0.392/0.335\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "465f4edcd9204f03bb3064a561ac95e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.023/0.027. Accuracy (Train/Test): 0.435/0.360\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec3c0addbc5546ef93534a869f7e2b3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.022/0.027. Accuracy (Train/Test): 0.467/0.366\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7283d55f8524e30ad62133f486a397f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.516/0.374\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5dd6bfd2ed04b6a912e18be6572dc12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.019/0.026. Accuracy (Train/Test): 0.552/0.353\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87f5f774cf4e44f4b817b4ff672949e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.017/0.027. Accuracy (Train/Test): 0.585/0.355\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2561f1bcdd24009bf88a5cd7b3508da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.016/0.028. Accuracy (Train/Test): 0.613/0.337\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e63eafcaab34dedaf7e03781951f526"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.015/0.029. Accuracy (Train/Test): 0.656/0.348\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c0204bb7d1e44ed918b60484bd4e71f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.013/0.031. Accuracy (Train/Test): 0.679/0.361\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d0310a8c9aa43a3977e317b02b01f5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.012/0.032. Accuracy (Train/Test): 0.746/0.322\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30e5ecbd9500487bbb97079864a6e101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.010/0.035. Accuracy (Train/Test): 0.776/0.343\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ec5ffd18b1e416597c22ac24a78d4c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.009/0.037. Accuracy (Train/Test): 0.817/0.317\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33ef01845d1b42d89b9e0fa03ca3d7ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▅▆▇▇█▇▇▆▆▇▅▆▄</td></tr><tr><td>eval/loss</td><td>▄▂▂▂▁▁▁▁▂▂▃▄▅▆█</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▃▃▄▅▅▅▆▆▇▇█</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▅▄▄▄▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.31728</td></tr><tr><td>eval/loss</td><td>0.03729</td></tr><tr><td>train/accuracy</td><td>0.81728</td></tr><tr><td>train/loss</td><td>0.00877</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">Basic RNN ratings</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240414_114627-b5aij9ql\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нерегуляризованные LSTM часто быстро переобучаются (и мы это видим по точности на контроле). Чтобы с этим бороться, часто используют *L2-регуляризацию* и *дропаут*.\n",
    "Однако способов накладывать дропаут на рекуррентный слой достаточно много, и далеко не все хорошо работают. По [ссылке](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b) доступен хороший обзор дропаутов для RNN.\n",
    "\n",
    "Мы реализуем два варианта DropOut для RNN (и третий дополнительно). Заодно увидим, что для реализации различных усовершенствований рекуррентной архитектуры приходится \"вскрывать\" слой до различной \"глубины\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Гала и Гарамани. Variational Dropout (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Начнем с дропаута, описанного в [статье Гала и Гарамани](https://arxiv.org/abs/1512.05287).\n",
    "Для этого нам потребуется перейти от использования слоя `torch.nn.LSTM`, полностью скрывающего от нас рекуррентную логику, к использованию слоя `torch.nn.LSTMCell`, обрабатывающего лишь один временной шаг нашей последовательности (а всю логику вокруг придется реализовать самостоятельно). \n",
    "\n",
    "Допишите класс `RNNLayer`. При `dropout=0` ваш класс должен работать как обычный слой LSTM, а при `dropout > 0` накладывать бинарную маску на входной и скрытый вектор на каждом временном шаге, причем эта маска должна быть одинаковой во все моменты времени.\n",
    "\n",
    "Дропаут Гала и Гарамани в виде формул (m обозначает маску дропаута):\n",
    "\n",
    "$$\n",
    "h_{t-1} = h_{t-1}*m_h, \\, x_t = x_t * m_x\n",
    "$$\n",
    "\n",
    "Далее обычный шаг рекуррентной архитектуры, например, LSTM:\n",
    "\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Union, Optional, Tuple, List"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:49.970640500Z",
     "start_time": "2024-04-15T18:57:49.945644100Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def init_h0_c0(num_objects: int, hidden_size: int, some_existing_tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    return h0 and c0, use some_existing_tensor.new_zeros() to gen them\n",
    "    h0 shape: num_objects x hidden_size\n",
    "    c0 shape: num_objects x hidden_size\n",
    "    \"\"\"\n",
    "    h0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    c0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    \n",
    "    return h0, c0"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:50.117770600Z",
     "start_time": "2024-04-15T18:57:50.088758800Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def gen_dropout_mask(input_size, hidden_size, is_training, p, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    is_training: if True, gen masks from Bernoulli\n",
    "                 if False, gen masks consisting of (1-p)\n",
    "    \n",
    "    return dropout masks of size input_size, hidden_size if p is not None\n",
    "    return one masks if p is None\n",
    "    \"\"\"\n",
    "    if p is None:\n",
    "        return some_existing_tensor.new_ones((input_size, hidden_size))\n",
    "    \n",
    "    if is_training:\n",
    "        return some_existing_tensor.new_empty((input_size, hidden_size)).bernoulli_(1 - p)\n",
    "    else:\n",
    "        return some_existing_tensor.new_full((input_size, hidden_size), fill_value=1 - p)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:50.313704600Z",
     "start_time": "2024-04-15T18:57:50.245185800Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите класс-обёртку над `LSTMCell` для реализации Variational Dropout. **Используйте только цикл по времени**"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:09:12.282613Z",
     "start_time": "2021-04-01T21:09:12.256019Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout: Optional[float] = None, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device if device is not None else torch.device('cpu')\n",
    "        \n",
    "        self.rnn_cell = torch.nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize h_0, c_0\n",
    "        h, c = init_h0_c0(\n",
    "            num_objects=x.shape[1],\n",
    "            hidden_size=self.hidden_size,\n",
    "            some_existing_tensor=x\n",
    "        )\n",
    "        \n",
    "        h = h.to(self.device)\n",
    "        c = c.to(self.device)\n",
    "        \n",
    "        # Gen masks for input and hidden state\n",
    "        p = self.dropout\n",
    "        \n",
    "        input_mask = gen_dropout_mask(\n",
    "            input_size=x.shape[1],\n",
    "            hidden_size=self.input_size,\n",
    "            is_training=self.training,\n",
    "            p=p,\n",
    "            some_existing_tensor=x,\n",
    "        ).to(device)\n",
    "        \n",
    "        hidden_st_mask = gen_dropout_mask(\n",
    "            input_size=x.shape[1],\n",
    "            hidden_size=self.hidden_size,\n",
    "            is_training=self.training,\n",
    "            p=p,\n",
    "            some_existing_tensor=x,\n",
    "        ).to(device)\n",
    "                \n",
    "        # Implement recurrent logic and return what nn.LSTM returns\n",
    "        # Do not forget to apply generated dropout masks!\n",
    "        \n",
    "        rs = x.new_empty((x.shape[0], x.shape[1], self.hidden_size)).to(self.device)\n",
    "        \n",
    "        for idx in range(x.shape[0]):\n",
    "            # print(f\"DEB: {x.shape=}\\t{h.shape=}\\t{hidden_st_mask.shape=}\\t{input_mask.shape=}\")\n",
    "#             inp = x[idx] * input_mask.broadcast_to((x.shape[1], x.shape[2]))\n",
    "#             h = h * hidden_st_mask.broadcast_to((x.shape[1], self.hidden_size))\n",
    "            \n",
    "            inp = x[idx] * input_mask\n",
    "            h = h * hidden_st_mask\n",
    "    \n",
    "            h, c = self.rnn_cell(inp, (h, c))\n",
    "            rs[idx] = h\n",
    "        \n",
    "        return rs, (h, c)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:52.439981300Z",
     "start_time": "2024-04-15T18:57:52.370986900Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте реализованную модель с выключенным дропаутом (слой `RNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Сильно ли оно увеличилось по сравнению с `torch.nn.LSTM` (LSTM \"из коробки\")?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:39:51.746874900Z",
     "start_time": "2024-04-12T11:39:51.731876100Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=RNNLayer, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:39:52.200061400Z",
     "start_time": "2024-04-12T11:39:52.183332Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"DEB RNN ratings DO=0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:40:05.893103700Z",
     "start_time": "2024-04-12T11:39:52.826026400Z"
    }
   },
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_143952-nq67sl7m</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace' target=\"_blank\">DEB RNN ratings DO=0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0bc9e0d10594b4cbae49d71b92c94a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bcb8b67433b42838dd4aa12922f0f3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте полученную модель с `dropout=0.25`, вновь замерив время обучения. Получилось ли побороть переобучение? Сильно ли дольше обучается данная модель по сравнению с предыдущей? (доп. время тратится на генерацию масок дропаута)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=RNNLayer, dropout=0.25,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T15:30:11.356544800Z",
     "start_time": "2024-04-06T15:30:11.293802500Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"RNN ratings DO=0.25\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T16:01:38.201037Z",
     "start_time": "2024-04-06T15:30:11.341544200Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Гала и Гарамани. Дубль 2 (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<начало взлома pytorch>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "При разворачивании цикла по времени средствами python обучение рекуррентной нейросети сильно замедляется. Однако для реализации дропаута Гала и Гарамани необязательно явно задавать в коде умножение нейронов на маски. Можно схитрить и обойтись использованием слоя `torch.nn.LSTM`: перед вызовом `forward` слоя `torch.nn.LSTM` подменять его веса на веса, домноженные по строкам на маски. А обучаемые веса хранить отдельно. Именно так этот дропаут реализован в библиотеке `fastai`, код из которой использован в ячейке ниже.\n",
    "\n",
    "Такой слой реализуется в виде обертки над `torch.nn.LSTM`. Допишите класс:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:56.031158700Z",
     "start_time": "2024-04-15T18:57:55.976162500Z"
    }
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FastRNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, layers_dropout=0.0, num_layers=1, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.layers_dropout = layers_dropout\n",
    "        self.module = torch.nn.LSTM(input_size, hidden_size, dropout=layers_dropout, num_layers=num_layers, device=device)\n",
    "\n",
    "        self.layer_names = []\n",
    "        self.layer_name_tuples = []\n",
    "        for layer_n in range(self.num_layers):\n",
    "            self.layer_names += [f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}']\n",
    "            self.layer_name_tuples.append((f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}'))\n",
    "        \n",
    "        for layer in self.layer_names:\n",
    "            # Get torch.nn.Parameter with weights from torch.nn.LSTM instance\n",
    "            w = getattr(self.module, layer)\n",
    "\n",
    "            # Remove it from model\n",
    "            delattr(self.module, layer)\n",
    "\n",
    "            # And create new torch.nn.Parameter with the same data but different name\n",
    "            self.register_parameter(f'{layer}_raw', torch.nn.Parameter(w.data))\n",
    "\n",
    "            # Note. In torch.nn.LSTM.forward parameter with name `layer` will be used\n",
    "            #     so we must initialize it using `layer_raw` before forward pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _setweights(self, x):\n",
    "        \"\"\"\n",
    "            Apply dropout to the raw weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        for layer in self.layer_names:\n",
    "            # Generate mask\n",
    "            \n",
    "            # Get torch.nn.Parameter with weights\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            \n",
    "            # Apply dropout mask\n",
    "            if 'ih' in layer:\n",
    "                input_mask = gen_dropout_mask(\n",
    "                    input_size=raw_w.shape[0],\n",
    "                    hidden_size=raw_w.shape[1],\n",
    "                    is_training=self.training,\n",
    "                    p=self.layers_dropout,\n",
    "                    some_existing_tensor=x,\n",
    "                )\n",
    "                \n",
    "                # print(f\"DEB: {raw_w.shape=}\\t{input_mask.shape=}\")\n",
    "                \n",
    "                masked_raw_w = raw_w * input_mask\n",
    "            elif 'hh' in layer:\n",
    "                hidden_st_mask = gen_dropout_mask(\n",
    "                    input_size=raw_w.shape[0],\n",
    "                    hidden_size=raw_w.shape[1],\n",
    "                    is_training=self.training,\n",
    "                    p=self.dropout,\n",
    "                    some_existing_tensor=x,\n",
    "                )\n",
    "                \n",
    "                # print(f\"DEB: {raw_w.shape=}\\t{hidden_st_mask.shape=}\")\n",
    "                \n",
    "                masked_raw_w = raw_w * hidden_st_mask\n",
    "            else:\n",
    "                assert False, \"WHF???\"\n",
    "            \n",
    "            # Set modified weights in its place\n",
    "            setattr(self.module, layer, masked_raw_w)\n",
    "\n",
    "    def forward(self, x, h_c: Optional[Tuple[torch.Tensor, torch.Tensor]]=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor containing the features of the input sequence.\n",
    "        :param Optional[Tuple[torch.Tensor, torch.Tensor]] h_c: initial hidden state and initial cell state\n",
    "        \"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            # To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # Set new weights of self.module and call its forward\n",
    "            # Pass h_c with x if it is not None. Otherwise pass only x\n",
    "           \n",
    "            self._setweights(x)  # set weights from layer_raw to layer\n",
    "            \n",
    "            if h_c is not None:\n",
    "                return self.module.forward(x, h_c)\n",
    "            else:\n",
    "                return self.module.forward(x)\n",
    "            \n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'):\n",
    "            self.module.reset()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:56.241869100Z",
     "start_time": "2024-04-15T18:57:56.187910300Z"
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте реализованную модель с выключенным дропаутом (слой `FastRNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Убедитесь, что модель выдаёт такое же качество, как и оригинальная реализация LSTM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=FastRNNLayer, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T14:52:56.064754200Z",
     "start_time": "2024-04-14T14:52:55.968492300Z"
    }
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"FastRNN ratings DO=0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T14:54:32.045015900Z",
     "start_time": "2024-04-14T14:52:56.212804900Z"
    }
   },
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240414_175256-461lw0d4</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace' target=\"_blank\">FastRNN ratings DO=0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "075d3034362241e4a95fbd488520e7c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87c2402ff0be47d6b42df3a96948ce61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.031/0.032. Accuracy (Train/Test): 0.234/0.232\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36b54db43ef14766af1749a842b8caa4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.031/0.031. Accuracy (Train/Test): 0.256/0.247\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33e119f0c2c142a4927b5b06061bd9e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.281/0.267\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d675ff54b5c04430b10c4c0f3abdac47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.029/0.030. Accuracy (Train/Test): 0.312/0.297\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6970e7262ea418a940f9d605ebb3284"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.026/0.027. Accuracy (Train/Test): 0.368/0.339\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d521172bd87a476286e4dacc8b256532"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.390/0.353\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a49b44a05814492951ea407d9dc8cb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.024/0.027. Accuracy (Train/Test): 0.395/0.349\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d1578433d9f4f5e91fb8dae8c7f747a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.024/0.026. Accuracy (Train/Test): 0.414/0.357\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76795eaa891b4e04b5bf441235042510"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.022/0.026. Accuracy (Train/Test): 0.436/0.371\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c91ecaa4dc9435eae32bcae098898b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.022/0.025. Accuracy (Train/Test): 0.452/0.373\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06fa2ae316d84c7f93d1d01a83534f1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.021/0.025. Accuracy (Train/Test): 0.462/0.365\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bd179bc335246599f5de399ea6d8c5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.462/0.372\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "506e5b5e3915471fa693298cff0d7149"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.487/0.379\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "820d9b418a77435aa00a29cbeee512f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.493/0.382\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c1fc00a00034315aca154d7242cd2e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.499/0.380\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6a3b01864c2476fa57f411b5cd25256"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▃▄▆▇▆▇▇█▇████</td></tr><tr><td>eval/loss</td><td>██▇▆▃▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▅▅▅▆▆▇▇▇███</td></tr><tr><td>train/loss</td><td>██▇▆▅▄▄▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.3804</td></tr><tr><td>eval/loss</td><td>0.02591</td></tr><tr><td>train/accuracy</td><td>0.49924</td></tr><tr><td>train/loss</td><td>0.01992</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">FastRNN ratings DO=0</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240414_175256-461lw0d4\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте полученный слой (вновь подставив его в `RNNClassifier` в качестве `rec_layer`) с `dropout=0.25`. Сравните время обучения с предыдущими моделями. Проследите, чтобы качество получилось такое же, как при первой реализации этого дропаута."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=FastRNNLayer, dropout=0.25,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T14:54:32.075999100Z",
     "start_time": "2024-04-14T14:54:32.039486900Z"
    }
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"FastRNN ratings DO=0.25\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T14:56:07.753270400Z",
     "start_time": "2024-04-14T14:54:32.046997700Z"
    }
   },
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01127777777777131, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc3a95e9bcb7479e9b1d6eb6db9d9b5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240414_175432-ffqdhdo1</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace' target=\"_blank\">FastRNN ratings DO=0.25</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e61ed27a858a4c51a2fc6c7d77cb6c37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1e2656634044f01a9704ae6e8f2a402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.031/0.032. Accuracy (Train/Test): 0.233/0.229\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75b91705b6b940849ce60fb569266541"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.031/0.031. Accuracy (Train/Test): 0.253/0.246\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0d112e9a8c340f78a6b0afac535a88a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.277/0.263\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ab636090fb641ef9beef2544f6a2407"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.028/0.030. Accuracy (Train/Test): 0.313/0.296\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b638ff464e64328bc91d6a101162e06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.026/0.028. Accuracy (Train/Test): 0.360/0.336\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b21b4a8edf94d5db1c0b845a94c3ee9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.380/0.349\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9678f556f1db4568a43666f1fdd85835"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.024/0.027. Accuracy (Train/Test): 0.391/0.348\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f90638c355d4f4491678d9e5201d30f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.024/0.026. Accuracy (Train/Test): 0.409/0.358\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d35355cdf4e44e08cbe36300cf46c22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.023/0.026. Accuracy (Train/Test): 0.416/0.356\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d9867dbac5e46479b511c12cfdea795"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.023/0.026. Accuracy (Train/Test): 0.431/0.360\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb8fb1ce285f48869b95b9ae41c6eaa8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.022/0.026. Accuracy (Train/Test): 0.441/0.364\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51d10331f42241c9b195ae59227d291a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.023/0.027. Accuracy (Train/Test): 0.425/0.343\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "505753adba0c481c98b89d5211761813"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.461/0.366\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24604d7a070348d7b114d4b6554470d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.473/0.373\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4061ab14fff48eaa9f82f4534ddd1c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.484/0.370\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d41fdbdd91848aaacab558c52fd4b8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▃▄▆▇▇▇▇▇█▇███</td></tr><tr><td>eval/loss</td><td>██▇▆▃▂▂▁▁▁▁▃▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▅▅▅▆▆▇▇▆▇██</td></tr><tr><td>train/loss</td><td>██▇▆▅▄▄▃▃▂▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.36968</td></tr><tr><td>eval/loss</td><td>0.02558</td></tr><tr><td>train/accuracy</td><td>0.48428</td></tr><tr><td>train/loss</td><td>0.02048</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">FastRNN ratings DO=0.25</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240414_175432-ffqdhdo1\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "</конец взлома pytorch>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Семениуты и др. (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Перейдем к реализации дропаута для LSTM по статье [Semeniuta et al](http://www.aclweb.org/anthology/C16-1165). \n",
    "\n",
    "Этот метод применения дропаута не менее популярен, чем предыдущий. Его особенность состоит в том, что он придуман специально для гейтовых архитектур. В контексте LSTM этот дропаут накладывается только на информационный поток ($m_h$ — маска дропаута):\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot g \\odot {\\bf m_h} \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$\n",
    "На входы $x_t$ маска накладывается как в предыдущем дропауте. Впрочем, на входы маску можно наложить вообще до вызова рекуррентного слоя.\n",
    "\n",
    "Согласно статье, маска дропаута может быть как одинаковая, так и разная для всех моментов времени. Мы сделаем одинаковую для всех моментов времени.\n",
    "\n",
    "Для реализации этого дропаута можно: \n",
    "1. самостоятельно реализовать LSTM (интерфейса LSTMCell не хватит) \n",
    "2. снова воспользоваться трюком с установкой весов (но тут мы опираемся на свойство $tanh(0)=0$, к тому же, трюк в данном случае выглядит менее тривиально, чем с дропаутом Гала). \n",
    "\n",
    "Предлагается реализовать дропаут по сценарию 1. Допишите класс:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class HandmadeLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device('cpu') if device is None else device\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_weights = torch.nn.Linear(input_size, 4 * hidden_size).to(self.device)\n",
    "        self.hidden_weights = torch.nn.Linear(hidden_size, 4 * hidden_size).to(self.device)\n",
    "        self.activations = nn.ModuleList([\n",
    "            nn.ReLU(), # i\n",
    "            nn.ReLU(), # o\n",
    "            nn.ReLU(), # f\n",
    "            nn.Tanh(), # g\n",
    "            nn.Tanh(), # h\n",
    "        ]).to(self.device)\n",
    "        \n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        \"\"\"\n",
    "        Initialization as in Pytorch. \n",
    "        Do not forget to call this method!\n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert not torch.any(torch.isnan(x)), f\"x has nan: {x=}\"\n",
    "        # Use functions init_h0_c0 and gen_dropout_masks defined above\n",
    "        seq_len = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        input_dim = x.shape[2]\n",
    "        \n",
    "        h, c = init_h0_c0(\n",
    "            num_objects=batch_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            some_existing_tensor=x,\n",
    "        )\n",
    "        \n",
    "        # input_mask, hidden_st_mask = gen_dropout_mask(\n",
    "        #     input_size=input_dim,\n",
    "        #     hidden_size=hidden_dim,\n",
    "        #     is_training=self.training,\n",
    "        #     p=self.dropout,\n",
    "        #     some_existing_tensor=x,\n",
    "        # )\n",
    "        \n",
    "        # print(f\"DEB: {torch.all(input_mask == 1.0)}\\t{torch.all(hidden_st_mask == 1.0)}\")\n",
    "        \n",
    "        # Implement recurrent logic to mimic torch.nn.LSTM\n",
    "        # Do not forget to apply dropout mask\n",
    "        rs = x.new_empty((seq_len, batch_size, self.hidden_size))\n",
    "        for idx in range(seq_len):\n",
    "            # x ~ [L, B, F], h ~ [H]\n",
    "            # print(f\"DEB: {x.shape=}\\t{h.shape=}\\t{c.shape=}\")\n",
    "            \n",
    "            # DEB: disable masks\n",
    "            # inp = x[idx, :, :] * input_mask # x --> [B, F]\n",
    "            inp = x[idx, :, :]\n",
    "            assert not torch.any(torch.isnan(inp)), f\"inp has nan: idx={idx}\\n{inp=}\"\n",
    "            \n",
    "            inp4 = self.input_weights(inp)  # x4 ~ [B, 4H]\n",
    "            h4 = self.hidden_weights(h)  # h4 ~ [4H]\n",
    "            assert not torch.any(torch.isnan(inp4)), f\"inp4 has nan: idx={idx}\\n{inp4=}\"\n",
    "            assert not torch.any(torch.isnan(h4)), f\"h4 has nan: idx={idx}\\n{h4=}\"\n",
    "            \n",
    "            y = inp4 + h4.broadcast_to((batch_size, 4 * self.hidden_size))\n",
    "            assert not torch.any(torch.isnan(y)), f\"y has nan: idx={idx}\\n{y=}\"\n",
    "            \n",
    "            i = self.activations[0](y[:, 0:self.hidden_size])\n",
    "            o = self.activations[1](y[:, 1*self.hidden_size:2*self.hidden_size])\n",
    "            f = self.activations[2](y[:, 2*self.hidden_size:3*self.hidden_size])\n",
    "            g = self.activations[3](y[:, 3*self.hidden_size:4*self.hidden_size])\n",
    "            \n",
    "            assert not torch.any(torch.isnan(i)), f\"i has nan: idx={idx}\\n{i=}\"\n",
    "            assert not torch.any(torch.isnan(o)), f\"o has nan: idx={idx}\\n{o=}\"\n",
    "            assert not torch.any(torch.isnan(f)), f\"f has nan: idx={idx}\\n{f=}\"\n",
    "            assert not torch.any(torch.isnan(g)), f\"g has nan: idx={idx}\\n{g=}\"\n",
    "            \n",
    "            # DEB: disable masks\n",
    "            # c = f * c + i * g * hidden_st_mask\n",
    "            c = f * c + i * g\n",
    "            h = o * self.activations[4](c)\n",
    "            assert not torch.any(torch.isnan(c)), f\"c has nan: idx={idx}\\n{c=}\"\n",
    "            assert not torch.any(torch.isnan(h)), f\"h has nan: idx={idx}\\n{h=}\"\n",
    "            \n",
    "            rs[idx, :, :] = h\n",
    "        \n",
    "        assert not torch.any(torch.isnan(rs)), f\"rs has nan: {rs=}\"\n",
    "        return rs, (h, c)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:47.175776300Z",
     "start_time": "2024-04-12T11:56:47.121999800Z"
    }
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте вашу реализацию без дропаута (проконтролируйте качество и сравните время обучения с временем обучения `torch.nn.LSTM` и `RNNLayer`), а также с `dropout=0.25`. Сравните качество модели с таким дропаутом с качеством модели с дропаутом Гала и Гарамани."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:48.427670500Z",
     "start_time": "2024-04-12T11:56:48.365671500Z"
    }
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=HandmadeLSTM, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:48.935714200Z",
     "start_time": "2024-04-12T11:56:48.890122200Z"
    }
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"HandmadeLSTM ratings DO=0.0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:09:13.369299700Z",
     "start_time": "2024-04-12T11:56:49.600384700Z"
    }
   },
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01145555555555499, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6a25206998d45aba4d7eec256bf2312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_145649-57fgbyzk</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace' target=\"_blank\">HandmadeLSTM ratings DO=0.0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7127aa3a73084c9d85403fb4f76f17cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bebf6b51d52945bd93d60276aebb160e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.030/0.032. Accuracy (Train/Test): 0.264/0.233\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a955bc0791124d4884a1592a89fd63d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.029/0.031. Accuracy (Train/Test): 0.293/0.242\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac264afed37a46399e18cb26076b9785"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Embeddings has NaNs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHandmadeLSTM ratings DO=0.0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[36], line 66\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs, name)\u001B[0m\n\u001B[0;32m     63\u001B[0m rng \u001B[38;5;241m=\u001B[39m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs))\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m rng:\n\u001B[1;32m---> 66\u001B[0m     \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m evaluate(train_loader, model, loss_fn, device)\n\u001B[0;32m     69\u001B[0m     train_accuracies\u001B[38;5;241m.\u001B[39mappend(train_acc)\n",
      "Cell \u001B[1;32mIn[36], line 17\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(dataloader, model, loss_fn, optimizer, device)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(torch\u001B[38;5;241m.\u001B[39misnan(tokens)), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEval epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 17\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens_lens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits, ratings)\n\u001B[0;32m     19\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[33], line 71\u001B[0m, in \u001B[0;36mRNNClassifier.forward\u001B[1;34m(self, tokens, tokens_lens)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# DEBUG: store last input in globals\u001B[39;00m\n\u001B[0;32m     69\u001B[0m emb_dump \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(torch\u001B[38;5;241m.\u001B[39misnan(x)), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbeddings has NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# Make forward pass through recurrent network\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# YOUR CODE HERE\u001B[39;00m\n\u001B[0;32m     75\u001B[0m x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(x)\n",
      "\u001B[1;31mAssertionError\u001B[0m: Embeddings has NaNs"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([   0,    1,    4,  ..., 5001, 5004, 5016])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dump = inp_dump.cpu()\n",
    "\n",
    "torch.unique(inp_dump)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:12:56.040351100Z",
     "start_time": "2024-04-12T12:12:55.983535100Z"
    }
   },
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:15:24.836069800Z",
     "start_time": "2024-04-12T12:15:24.780492400Z"
    }
   },
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Token: 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(vocab)):\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(\n\u001B[0;32m      3\u001B[0m         torch\u001B[38;5;241m.\u001B[39misnan(\n\u001B[0;32m      4\u001B[0m             model\u001B[38;5;241m.\u001B[39mword_embeddings(\n\u001B[0;32m      5\u001B[0m                 torch\u001B[38;5;241m.\u001B[39mLongTensor([i])\n\u001B[0;32m      6\u001B[0m             )       \n\u001B[0;32m      7\u001B[0m         )\n\u001B[0;32m      8\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToken: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Token: 1"
     ]
    }
   ],
   "source": [
    "for i in range(len(vocab)):\n",
    "    assert not torch.any(\n",
    "        torch.isnan(\n",
    "            model.word_embeddings(\n",
    "                torch.LongTensor([i])\n",
    "            )       \n",
    "        )\n",
    "    ), f\"Token: {i}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:24.317561300Z",
     "start_time": "2024-04-12T12:16:24.250852100Z"
    }
   },
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        [ 1.1044, -1.1028,  0.5543,  ..., -0.6463,  0.0749,  0.1939],\n        ...,\n        [ 1.0837, -0.6411,  1.0597,  ...,  1.3895, -0.3897,  0.6304],\n        [ 0.4510, -0.0567, -1.1788,  ..., -0.0786,  2.0267,  0.6129],\n        [-0.6576, -0.7521,  0.2827,  ..., -0.0446,  1.5368, -0.1816]],\n       requires_grad=True)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embeddings.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:45.419413500Z",
     "start_time": "2024-04-12T12:16:45.392862300Z"
    }
   },
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.387698Z",
     "start_time": "2024-03-30T22:22:19.019606Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Сравнение всех предложенных моделей (1 балл)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T23:33:28.831346Z",
     "start_time": "2021-04-01T23:33:28.810453Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Используя замеры времени заполните табличку с временем работы четырёх реализованных моделей в следующей ячейке:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T23:48:05.361634Z",
     "start_time": "2021-04-01T23:48:05.333901Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| torch.nn.LSTM | RNNLayer | FastRNNLayer | HandmadeLSTM |\n",
    "|---------------|----------|--------------|--------------|\n",
    "| 2m 35s        | 14m 16s  | 2m 41s       | 31m 44s      |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.566217Z",
     "start_time": "2024-03-30T22:43:31.389958Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Крайне желательно рисовать графики в векторном формате. \n",
    "\n",
    "Если по каким-то причинам, отрисовка не будет работать, закомментируйте следующую ячейку."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib_inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.573342Z",
     "start_time": "2024-03-30T22:43:31.567690Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нарисуйте два графика — функция потерь и качество на обучающей и тестовой выборке для всех 7 моделей обученных выше."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "...\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_title('CrossEntropy Loss')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_title('Accuracy')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:32.290598Z",
     "start_time": "2024-03-30T22:43:31.575031Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сделайте итоговые выводы о качестве работы моделей с разными реализациями DropOut:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Бонус. Zoneout (0.5 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Это еще одна модификация идеи дропаута применительно к рекуррентным нейросетям. В Zoneout на каждом временном шаге с вероятностью $p$ компонента скрытого состояния обновляется, а с вероятностью $1-p$ берется с предыдущего шага. \n",
    "В Виде формул ($m^t_h$ - бинарная маска):\n",
    " \n",
    "(сначала обычный рекуррентный переход, например LSTM)\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$\n",
    "Затем Zoneout:\n",
    "$$\n",
    "h_t = h_t * m_h^t + h_{t-1}*(1-m_h^t)\n",
    "$$\n",
    "В этом методе маска уже должна быть разная во все моменты времени (иначе метод упрощается до дропаута Гала и Гарамани). На входы $x_t$ вновь можно накладывать маску до начала работы рекуррентного слоя.  \n",
    "\n",
    "Если у вас осталось время, вы можете реализовать этот метод. Выберите основу из трех рассмотренных случаев самостоятельно.\n",
    "\n",
    "**Полный балл ставится только при наличии качественного и количественного сравнения с предыдущими моделями.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 2. Language Modeling с помощью LSTM (5 баллов)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:05:00.702763Z",
     "start_time": "2021-03-31T16:05:00.674835Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Во второй части мы попробуем обучить модель для генерации отзывов по их началу."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Концептуально модель будет выглядеть следующим образом:\n",
    "    \n",
    "![image info](https://blog.feedly.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-12.08.35-PM.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В процессе обучения будем тренировать сеть предсказывать вероятность следующего символа при условии всех предыдущих. Эту вероятность можно моделировать с помощью скрытого состояния $h^{(t)}$ пропуская его через линейный слой с выходной размерностью равной размерности словаря:\n",
    "$$\n",
    "p(x^{t}|x^{t-1}, ..., x^{1}) = SoftMax(Linear(h^{(t)}))\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обратите внимание, что для вычисления $p(x^{t}|x^{t-1}, ..., x^{1})$ для всех моментов времени достаточно сделать один проход по RNN, а затем применить линейное преобразование ко всем скрытым состояниям."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве функции потерь необходимо использовать `CrossEntropy`."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T00:37:56.100520Z",
     "start_time": "2021-04-02T00:37:56.072747Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим другой важный момент. Для того, чтобы решить данную задачу, модель должна уметь определять момент начала генерации предложения и оповещать о завершении генерации — конце предложения. Для этого добавим в словарь вспомогательные токены `<sos>`, `<eos>`. Добавив `<sos>` в начало каждого предложения и `<eos>` в конец.\n",
    "\n",
    "Модель сможет начинать генерацию как только ей будет передан токен `<sos>` и заканчивать генерацию, как только на очередном месте самым вероятным токеном оказывается `<eos>`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для решения этой задачи мы воспользуемся уже реализованной LSTM с дропаутом `FastRNNLayer` и классом `RNNClassifier`, то есть архитектура сети принципиально не поменяется. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация модели и цикла обучения (2 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Не используйте циклы в `RNNLM`, `LMCrossEntropyLoss`, `LMAccuracy`**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNLM(RNNClassifier):\n",
    "    def __init__(\n",
    "        self, embedding_dim, hidden_dim, vocab, dropout=0.5, layers_dropout=0.5, num_layers=1\n",
    "    ):\n",
    "        super().__init__(\n",
    "            embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(vocab), vocab=vocab,\n",
    "            rec_layer=FastRNNLayer, dropout=dropout, layers_dropout=layers_dropout, num_layers=num_layers\n",
    "        )\n",
    "    \n",
    "        # super().__init__(\n",
    "        #     embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(vocab), vocab=vocab,\n",
    "        #     rec_layer=nn.LSTM, dropout=None, num_layers=num_layers\n",
    "        # )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, len(vocab))\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor(dtype=torch.long) tokens: \n",
    "            Batch of texts represented with tokens. Shape: [T, B]\n",
    "        :param torch.Tensor(dtype=torch.long) tokens_lens: \n",
    "            Number of non-padding tokens for each object in batch. Shape: [B]\n",
    "        :return torch.Tensor: \n",
    "            Distribution of next token for each time step. Shape: [T, B, V], V — size of vocabulary\n",
    "        \"\"\"\n",
    "        # Make embeddings for all tokens\n",
    "        x = self.word_embeddings(tokens)\n",
    "        \n",
    "        # Forward pass embeddings through network\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Take all hidden states from the last layer of LSTM for each step and perform linear transformation\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:44.623407700Z",
     "start_time": "2024-04-15T19:59:44.549402400Z"
    }
   },
   "execution_count": 182,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуем функцию потерь для данной задачи. \n",
    "\n",
    "Моменты на которые нужно обратить внимание:\n",
    "1. Распределение вероятности следующего токена для последнего токена в последовательности не участвует в подсчёте функции потерь.\n",
    "2. Необходимо учитывать, что в одном батче могут быть тексты разной длины."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для решения второй проблемы можно воспользоваться функцией `torch.nn.utils.rnn.pack_padded_sequence`. \n",
    "\n",
    "Принимая на вход батч тензоров и длину каждого тензора без учёта паддинга эта функция позволяет получить все элементы в тензорах, которые не относятся к паддингу в виде плоского массива:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "padded_tensors = torch.tensor([\n",
    "    [[1, 11, 111], [2, 22, 222], [3, 33, 333]],\n",
    "    [[4, 44, 444], [5, 55, 555], [6, 66, 666]],\n",
    "    [[7, 77, 777], [0, 0, 0], [8, 88, 888]],\n",
    "    [[9, 99, 999], [0, 0, 0], [0, 0, 0]]\n",
    "])\n",
    "tensors_lens = torch.tensor([4, 2, 3])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:45.486132300Z",
     "start_time": "2024-04-15T19:59:45.432619700Z"
    }
   },
   "execution_count": 183,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обратите внимание, что `torch.nn.utils.rnn.pack_padded_sequence` автоматически переупорядочивает тензоры в батче по убыванию их длины."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.nn.utils.rnn.pack_padded_sequence(padded_tensors, tensors_lens, batch_first=False, enforce_sorted=False)[0].shape"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:47.014888900Z",
     "start_time": "2024-04-15T19:59:46.945319Z"
    }
   },
   "execution_count": 184,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([9, 3])"
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class LMCrossEntropyLoss(torch.nn.CrossEntropyLoss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, outputs, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n",
    "        :param torch.Tensor tokens: Batch of tokens. Shape: [T, B]\n",
    "        :param torch.Tensor tokens_lens: Length of each sequence in batch\n",
    "        :return torch.Tensor: CrossEntropyLoss between corresponding logits and tokens\n",
    "        \"\"\"\n",
    "        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n",
    "        # Do not forget specify enforce_sorted=False and correct value of batch_first \n",
    "        \n",
    "        # print(f\"DEB: {outputs.shape=}\\t{tokens.shape=}\")\n",
    "        \n",
    "        # packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs[:-1], tokens_lens.cpu()-1, batch_first=False, enforce_sorted=False)[0]\n",
    "        # packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens[1:], tokens_lens.cpu()-1, batch_first=False, enforce_sorted=False)[0]\n",
    "        \n",
    "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "        \n",
    "        # print(f\"DEB: {packed_outputs.shape=}\\t{packed_tokens.shape=}\")\n",
    "        \n",
    "        # Use super().forward(..., ...) to compute CrossEntropyLoss\n",
    "        return super().forward(\n",
    "            input=packed_outputs,\n",
    "            target=packed_tokens\n",
    "        )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:47.271639900Z",
     "start_time": "2024-04-15T19:59:47.217107400Z"
    }
   },
   "execution_count": 185,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для оценки качества нам также необходимо вычислять долю правильно предсказанных токенов. Реализуйте класс для вычисления точности."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class LMAccuracy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, outputs, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n",
    "        :param torch.Tensor tokens: Batch of tokens. Shape: [T, B]\n",
    "        :param torch.Tensor tokens_lens: Length of each sequence in batch\n",
    "        :return torch.Tensor: Accuracy for given logits and tokens\n",
    "        \"\"\"\n",
    "        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n",
    "        # Do not forget specify enforce_sorted=False and correct value of batch_first \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "        # packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "    \n",
    "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "    \n",
    "        preds = torch.argmax(packed_outputs, dim=1)\n",
    "    \n",
    "        return torch.sum(\n",
    "            preds == packed_tokens\n",
    "        )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:47.877403200Z",
     "start_time": "2024-04-15T19:59:47.823396100Z"
    }
   },
   "execution_count": 186,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Модифицируйте функции `train_epoch`, `evaluate`, `train` для обучения LM.\n",
    "\n",
    "**При вычислении точности, обратите внимание на то, что мы не предсказываем первый токен в каждой последовательности и токены, относящиеся к паддингу.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch_lm(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Make optimizer step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tokens = data['tokens'][:-1, :].to(device)\n",
    "        targets = data['tokens'][1:, :].to(device)\n",
    "        tokens_lens = data['tokens_lens'].cpu()\n",
    "        \n",
    "        logits = model(tokens, tokens_lens - 1)\n",
    "        loss = loss_fn(logits, targets, tokens_lens - 1)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_lm(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    accuracy_fn = LMAccuracy()  # DEB: return 0.0\n",
    "    \n",
    "    for idx, data in enumerate(dataloader):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Evaluate accuracy\n",
    "        \n",
    "        tokens = data['tokens'][:-1, :].to(device)\n",
    "        targets = data['tokens'][1:, :].to(device)\n",
    "        tokens_lens = data['tokens_lens'].cpu()\n",
    "        \n",
    "        logits = model(tokens, tokens_lens - 1)\n",
    "        loss = loss_fn(logits, targets, tokens_lens - 1).item()\n",
    "        acc = accuracy_fn(logits, targets, tokens_lens - 1).item()\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += torch.sum(tokens_lens)\n",
    "        total_accuracy += acc\n",
    "        \n",
    "            \n",
    "    return total_loss / total_tokens, total_accuracy / total_tokens\n",
    "\n",
    "def train_lm(\n",
    "    train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs\n",
    "):\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_epoch_lm(train_loader, model, loss_fn, optimizer, device)\n",
    "        \n",
    "        train_loss, train_acc = evaluate_lm(train_loader, model, loss_fn, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss, test_acc = evaluate_lm(test_loader, model, loss_fn, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(\n",
    "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. Accuracy (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
    "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:49.878122900Z",
     "start_time": "2024-04-15T19:59:49.858609200Z"
    }
   },
   "execution_count": 187,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь у нас всё готово для обучения модели."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим словарь с `<sos>`, `<eos>` токенами.\n",
    "\n",
    "Обратите внимание, что в отличие от классификации текстов нам необходимо значительно увеличить размер словаря, чтобы доля `<unk>` токенов была не велика.\n",
    "\n",
    "Так же, так как задача генерации значительно сложнее задачи классификации текстов будем обучать модель только на префиксах рецензий длины $20$. Это позволяет значительно ускорить обучение."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T01:06:12.736180Z",
     "start_time": "2021-04-02T01:06:12.708814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for special in specials:\n",
    "    counter[special] = 0\n",
    "# min_freq=8 is approximately equivalent to max_size=30000. \n",
    "#   You can lower min_freq in order to make model vocabulary more diverse \n",
    "lm_vocab = torchtext.vocab.vocab(counter, specials=specials, special_first=True, min_freq=8)\n",
    "lm_vocab.set_default_index(vocab['<unk>'])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:11:47.157093800Z",
     "start_time": "2024-04-15T19:11:47.103579200Z"
    }
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lm_datasets_dump_path = pathlib.Path(\"./lm_datasets_dump.bin\")\n",
    "\n",
    "if not lm_datasets_dump_path.exists():\n",
    "    lm_test_dataset = LargeMovieReviewDataset(test_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)\n",
    "    lm_train_dataset = LargeMovieReviewDataset(train_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)\n",
    "    \n",
    "    with open(lm_datasets_dump_path, \"wb\") as f:\n",
    "        torch.save(\n",
    "            obj=(lm_test_dataset, lm_train_dataset),\n",
    "            f=f\n",
    "        )\n",
    "else:\n",
    "    with open(lm_datasets_dump_path, \"rb\") as f:\n",
    "        lm_test_dataset, lm_train_dataset = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:11:49.553279900Z",
     "start_time": "2024-04-15T19:11:47.734173800Z"
    }
   },
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",\n 'label': tensor(0),\n 'rating': tensor(1),\n 'tokens': tensor([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n         21, 22, 23,  3]),\n 'tokens_len': 22}"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_test_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:11:50.062055900Z",
     "start_time": "2024-04-15T19:11:49.973366400Z"
    }
   },
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим даталоадеры для тестовой и обучающей выборок:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "lm_test_dataloader = DataLoader(\n",
    "    lm_test_dataset, batch_size=196, shuffle=False, num_workers=0, \n",
    "    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n",
    ")\n",
    "lm_train_dataloader = DataLoader(\n",
    "    lm_train_dataset, batch_size=196, shuffle=True, num_workers=0, \n",
    "    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:02.835636800Z",
     "start_time": "2024-04-15T19:59:02.785122900Z"
    }
   },
   "execution_count": 174,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Убедитесь, что все предложения имеют в начале `<sos>` токен, а в конце — `<eos>` токен."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch = next(iter(lm_train_dataloader))\n",
    "batch['tokens'], batch['tokens_lens']"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:03.185392100Z",
     "start_time": "2024-04-15T19:59:03.117857700Z"
    }
   },
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[    2,     2,     2,  ...,     2,     2,     2],\n         [ 3236,  1768,     7,  ...,    50, 10243,   431],\n         [ 1385,   369,  2059,  ...,    78,    35,  1096],\n         ...,\n         [ 7065,     1,  3278,  ...,  3694,   325,  1886],\n         [ 4045,  3826,    94,  ...,   495,    24,  1223],\n         [    3,     3,     3,  ...,     3,     3,     3]]),\n tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]))"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим модель, функцию потерь и оптимизатор: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:04.222309800Z",
     "start_time": "2024-04-15T19:59:04.195303500Z"
    }
   },
   "execution_count": 176
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2486"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:04.740501300Z",
     "start_time": "2024-04-15T19:59:04.549302200Z"
    }
   },
   "execution_count": 177
  },
  {
   "cell_type": "code",
   "source": [
    "lm_model = RNNLM(\n",
    "    embedding_dim=512, hidden_dim=512, vocab=lm_vocab, dropout=0.6, layers_dropout=0.6, num_layers=2\n",
    ").to(device=device)\n",
    "\n",
    "# lm_model = RNNLM(\n",
    "#     embedding_dim=512, hidden_dim=512, vocab=lm_vocab, num_layers=2\n",
    "# ).to(device=device)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:10.423207600Z",
     "start_time": "2024-04-15T19:59:10.233707Z"
    }
   },
   "execution_count": 178,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_loss_fn = LMCrossEntropyLoss(reduction='sum')\n",
    "lm_optimizer = torch.optim.Adam(lm_model.parameters(), lr=0.005, weight_decay=1.2e-6)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:10.813146400Z",
     "start_time": "2024-04-15T19:59:10.761411900Z"
    }
   },
   "execution_count": 179,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучим модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# lm_model = torch.compile(lm_model)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:11.530186Z",
     "start_time": "2024-04-15T19:59:11.483673100Z"
    }
   },
   "execution_count": 180,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_train_losses, lm_train_accuracies, lm_test_losses, lm_test_accuracies = train_lm(\n",
    "    lm_train_dataloader, lm_test_dataloader, lm_model, lm_loss_fn, lm_optimizer, device, 10\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:18.932039600Z",
     "start_time": "2024-04-15T19:59:11.782008900Z"
    }
   },
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0021407f39846c582098eb94a772ad9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "670bf8803cb943b28750848905f94cc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[181], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m lm_train_losses, lm_train_accuracies, lm_test_losses, lm_test_accuracies \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_lm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlm_train_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_test_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_loss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\n\u001B[0;32m      3\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[173], line 60\u001B[0m, in \u001B[0;36mtrain_lm\u001B[1;34m(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs)\u001B[0m\n\u001B[0;32m     58\u001B[0m train_accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs)):\n\u001B[1;32m---> 60\u001B[0m     \u001B[43mtrain_epoch_lm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m evaluate_lm(train_loader, model, loss_fn, device)\n\u001B[0;32m     63\u001B[0m     train_accuracies\u001B[38;5;241m.\u001B[39mappend(train_acc)\n",
      "Cell \u001B[1;32mIn[173], line 19\u001B[0m, in \u001B[0;36mtrain_epoch_lm\u001B[1;34m(dataloader, model, loss_fn, optimizer, device)\u001B[0m\n\u001B[0;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits, targets, tokens_lens \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     17\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 19\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:356\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprofile_hook_step\u001B[39m(func: Callable[_P, R]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Callable[_P, R]:\n\u001B[1;32m--> 356\u001B[0m     \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    357\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m R:\n\u001B[0;32m    358\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m_ \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    359\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m cast(Optimizer, \u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lm_model_dump_path = pathlib.Path(\"./lm_model_dump.bin\")\n",
    "\n",
    "with open(lm_model_dump_path, \"wb\") as f:\n",
    "    torch.save(\n",
    "        obj=lm_model,\n",
    "        f=f\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:34:50.487930Z",
     "start_time": "2024-04-15T19:34:50.217132500Z"
    }
   },
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(lm_model_dump_path, \"rb\") as f:\n",
    "    lm_model = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:01:58.778993300Z",
     "start_time": "2024-04-15T20:01:58.612215500Z"
    }
   },
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pred_0 = lm_model(batch['tokens'].to(device), batch['tokens_lens'].to(device))[:, 1, :].cpu()\n",
    "pred_0 = torch.argmax(pred_0, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:03:01.496366100Z",
     "start_time": "2024-04-15T20:03:01.206170400Z"
    }
   },
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([  7,   7,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 169,   1,\n           1,   1,   3,   3,   3,   3,   3,   3]),\n tensor([    2,  1768,   369,  1205,  7630,   502,  1363, 12547,   524,  2417,\n          5326,    76,  1109,  9018,     1,  1168,   657,   657,     1,     1,\n          3826,     3]))"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_0, batch['tokens'][:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:03:01.500366300Z",
     "start_time": "2024-04-15T20:03:01.486032500Z"
    }
   },
   "execution_count": 193
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация декодера (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь, реализуем последнюю деталь — декодирование с использованием обученной модели.\n",
    "Есть несколько вариантов. Рассмотрим два самых простых:\n",
    "1. **Жадное декодирование.** На каждом шаге мы выбираем токен с максимальной вероятностью и используем его для обновления скрытого состояния RNN.\n",
    "2. **Top-k sampling.** На очередном шаге рассматриваются $k$ токенов с самыми большими вероятностями. Остальные токены игнорируются. Из выбранных токенов семплируется следующий токен пропорционально их вероятностям.\n",
    "\n",
    "Прочитать подробнее про разные варианты декодирования можно по ссылкам:\n",
    "1. [От huggingface](https://huggingface.co/blog/how-to-generate)\n",
    "2. [На towardsdatascience](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Существенным в процессе декодирования является критерий останова. Как только очередной самый вероятный символ оказался `<eos>`, то данная последовательность считается сгенерированной. Однако, может так оказаться, что `<eos>` никогда не будет выбран, тогда необходимо прекратить генерацию, как только длина последовательности перейдёт порог `max_generated_len`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def decode(model, start_tokens, start_tokens_lens, max_generated_len=20, top_k=None):\n",
    "    \"\"\"\n",
    "    :param RNNLM model: Model\n",
    "    :param torch.Tensor start_tokens: Batch of seed tokens. Shape: [T, B]\n",
    "    :param torch.Tensor start_tokens_lens: Length of each sequence in batch. Shape: [B]\n",
    "    :param int max_generated_len: Maximum lenght of generated samples\n",
    "    :param Optional[int] top_k: Number of tokens with the largest probability to sample from\n",
    "    :return Tuple[torch.Tensor, torch.Tensor]. \n",
    "        Newly predicted tokens and length of generated part. Shape [T*, B], [B]\n",
    "    \"\"\"\n",
    "    # Get embedding for start_tokens\n",
    "    embedding = model.word_embeddings(start_tokens)\n",
    "    \n",
    "    # Pass embedding through rnn and collect hidden states and cell states for each time moment\n",
    "    all_h, all_c = [], []\n",
    "    h = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    c = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    for time_step in range(start_tokens.shape[0]):\n",
    "        _, (h, c) = model.rnn(embedding[time_step][None, :, :], (h, c))\n",
    "        all_h.append(h)\n",
    "        all_c.append(c)\n",
    "    \n",
    "    all_h = torch.stack(all_h, dim=1)\n",
    "    all_c = torch.stack(all_c, dim=1)\n",
    "    \n",
    "    # Take final hidden state and cell state for each start sequence in batch\n",
    "    # We will use them as h_0, c_0 for generation new tokens\n",
    "    h = all_h[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n",
    "    c = all_c[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n",
    "    \n",
    "    # List of predicted tokens for each time step\n",
    "    predicted_tokens = []\n",
    "    # Length of generated part for each object in the batch\n",
    "    decoded_lens = torch.zeros_like(start_tokens_lens, dtype=torch.long)\n",
    "    # Boolean mask where we store if the sequence has already generated\n",
    "    # i.e. `<eos>` was selected on any step\n",
    "    is_finished_decoding = torch.zeros_like(start_tokens_lens, dtype=torch.bool)\n",
    "    \n",
    "    # Stop when all sequences in the batch are finished\n",
    "    while not torch.all(is_finished_decoding) and torch.max(decoded_lens) < max_generated_len:\n",
    "        # Evaluate next token distribution using hidden state h.\n",
    "        # Note. Over first dimension h has hidden states for each layer of LSTM.\n",
    "        #     We must use hidden state from the last layer\n",
    "        # logits, (h, c) = model.rnn(h, (h, c))\n",
    "        # logits = model.linear(logits)\n",
    "        \n",
    "        logits = model.linear(h[-1, :, :])\n",
    "        \n",
    "        if top_k is not None:\n",
    "            # Top-k sampling. Use only top-k most probable logits to sample next token\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            # Mask non top-k logits\n",
    "            logits[indices_to_remove] = -1e10\n",
    "            # Sample next_token. \n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(\n",
    "                input=logits,\n",
    "                num_samples=1,\n",
    "            )[:, 0]\n",
    "        else:\n",
    "            # Select most probable token\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        predicted_tokens.append(next_token)\n",
    "        \n",
    "        decoded_lens += (~is_finished_decoding)\n",
    "        is_finished_decoding |= (next_token == torch.tensor(model.vocab.lookup_indices(['<eos>'])[0]))\n",
    "\n",
    "        # Compute embedding for next token\n",
    "        embedding = model.word_embeddings(next_token)\n",
    "        \n",
    "        # Update hidden and cell states\n",
    "        _, (h, c) = model.rnn(embedding[None, :, :], (h, c))\n",
    "    \n",
    "    return torch.stack(predicted_tokens), decoded_lens"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:35:40.990672Z",
     "start_time": "2024-04-15T19:35:40.927872Z"
    }
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем сгенерировать продолжения для нескольких префиксов:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T01:38:06.232189Z",
     "start_time": "2021-04-02T01:38:06.205413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "start_tokens = torch.tensor([\n",
    "    lm_model.vocab.lookup_indices(['<sos>', '<pad>', '<pad>', '<pad>']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'my', 'favorite', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'best', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'worst', 'movie']),\n",
    "]).T\n",
    "\n",
    "start_tokens_lens = torch.tensor([1, 4, 4, 4])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:35:41.829200600Z",
     "start_time": "2024-04-15T19:35:41.769171100Z"
    }
   },
   "execution_count": 103,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_model = lm_model.cpu()\n",
    "lm_model.eval()\n",
    "# decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=20)\n",
    "# decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=5)\n",
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=None)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:36:17.928816300Z",
     "start_time": "2024-04-15T19:36:17.781783Z"
    }
   },
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 4]), torch.Size([4]))"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens.shape, decoded_lens.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:36:18.144888100Z",
     "start_time": "2024-04-15T19:36:18.016830900Z"
    }
   },
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "source": [
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:36:18.263901400Z",
     "start_time": "2024-04-15T19:36:18.217888500Z"
    }
   },
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1406, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> favorite movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 60, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> best movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1203, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> worst movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуйте выполнить семплирование для разных $k$. Сравните результаты top-k семплирования с жадным декодированием. Опишите ваши наблюдения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=15)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:37:43.382025500Z",
     "start_time": "2024-04-15T19:37:43.166386100Z"
    }
   },
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 7, 525, 1891, 131, 7, 136, 169, 93, 241, 28]\n",
      "<sos> movie ever heard seen movie made like one movies time\n",
      "tokens=[2, 1, 1406, 7, 525, 131, 7, 169, 169, 371, 93, 60, 3017, 93]\n",
      "<sos> <unk> favorite movie ever seen movie like like love one best flicks one\n",
      "tokens=[2, 1, 60, 7, 1, 136, 1, 115, 131, 159, 159, 174, 28, 28]\n",
      "<sos> <unk> best movie <unk> made <unk> film seen first first minutes time time\n",
      "tokens=[2, 1, 1203, 7, 525, 130, 131, 131, 88, 7, 1, 7, 689, 689]\n",
      "<sos> <unk> worst movie ever ive seen seen times movie <unk> movie bad bad\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 93, 241, 525, 169, 7, 131, 88, 704, 115, 131]\n",
      "<sos> one movies ever like movie seen times say film seen\n",
      "tokens=[2, 1, 1406, 7, 1023, 435, 72, 1023, 159, 7, 65, 211, 1, 438]\n",
      "<sos> <unk> favorite movie great director films great first movie could get <unk> make\n",
      "tokens=[2, 1, 60, 7, 159, 28, 99, 1, 1, 7, 136, 3]\n",
      "<sos> <unk> best movie first time watch <unk> <unk> movie made <eos>\n",
      "tokens=[2, 1, 1203, 7, 131, 1, 1, 463, 1, 93, 72, 60, 525, 525]\n",
      "<sos> <unk> worst movie seen <unk> <unk> horror <unk> one films best ever ever\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=25)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:37:50.078633700Z",
     "start_time": "2024-04-15T19:37:49.884945400Z"
    }
   },
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 115, 239, 159, 261, 7, 3340, 1, 261, 7, 119]\n",
      "<sos> film thought first saw movie theater <unk> saw movie still\n",
      "tokens=[2, 1, 1406, 7, 241, 882, 169, 7, 230, 895, 709, 169, 241, 704]\n",
      "<sos> <unk> favorite movie movies terrible like movie even worse think like movies say\n",
      "tokens=[2, 1, 60, 7, 525, 525, 131, 129, 65, 689, 159, 91, 99, 7]\n",
      "<sos> <unk> best movie ever ever seen every could bad first dont watch movie\n",
      "tokens=[2, 1, 1203, 7, 131, 882, 1, 7, 169, 7, 689, 204, 689, 147]\n",
      "<sos> <unk> worst movie seen terrible <unk> movie like movie bad pretty bad script\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=50)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:37:56.926254100Z",
     "start_time": "2024-04-15T19:37:56.722079600Z"
    }
   },
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1406, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> favorite movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 60, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> best movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1203, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> worst movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=None)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:38:28.258702700Z",
     "start_time": "2024-04-15T19:38:28.096160Z"
    }
   },
   "execution_count": 116
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Beam Search (2 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим более продвинутый алгоритм для декодирования. Реализуйте алгоритм Beam Search."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Несколько замечаний по имплементации:\n",
    "\n",
    "1. При больших размерах `beam_size` число гипотез ($B \\times \\text{beam\\_size}$) на очередном шаге может быть слишком большим. Поэтому может потребоваться разбить все гипотезы на отдельные батчи и делать forward-pass в несколько итераций. Используйте [`torch.split`](https://pytorch.org/docs/stable/generated/torch.split.html)\n",
    "2. Для выбора лучших гипотез используйте [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html). Обратите внимание на индексы, которые возвращает эта функция (может пригодиться метод [`torch.remainder`](https://pytorch.org/docs/stable/generated/torch.remainder.html))\n",
    "3. Можно отслеживать, какие элементы в батче (или какие гипотезы) закончили генерацию. Делая forward-pass только для незавершённых гипотез, можно ускорить декодинг, однако, это усложнит реализацию"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_tensor(t):\n",
    "    print(f\"{t.shape=}\")\n",
    "    print(\"tensor=(\")\n",
    "    for i in range(t.shape[0]):\n",
    "        print(f\"\\t{t[i]}\")\n",
    "    print(f\")\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:32:57.913438700Z",
     "start_time": "2024-04-15T20:32:57.867925Z"
    }
   },
   "execution_count": 213
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def decode_beam_search(model, start_tokens, start_tokens_lens, max_generated_len=20, beam_size=5):\n",
    "    \"\"\"\n",
    "    :param RNNLM model: Model\n",
    "    :param torch.Tensor start_tokens: Batch of seed tokens. Shape: [T, B]\n",
    "    :param torch.Tensor start_tokens_lens: Length of each sequence in batch. Shape: [B]\n",
    "    :param int max_generated_len: Maximum length of generated samples\n",
    "    :param int beam_size: Size of beam\n",
    "    :return Tuple[torch.Tensor, torch.Tensor, torch.Tensor]. \n",
    "        Newly predicted tokens, lengths of generated parts and log probabilities for each hypotheses \n",
    "        Shape [T*, B, beam_size], [T*, beam_size], [T*, beam_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    # L — number of RNN layers in the model, H — hidden size, BS — beam size\n",
    "    #\n",
    "    # 1. Make forward pass of start_tokens through the model. \n",
    "    #      Obtain the last cell and hidden state for each element in the batch \n",
    "    #          (i.e. tensors of shape [L, B, H])\n",
    "    #      Use those states as the initialization for each hypotheses in the beam \n",
    "    #          (i.e. tensors of shape [L, B * BS, H])\n",
    "    #      Initialize probabilities for each hypotheses in the beam with 1.0\n",
    "    #          (i.e. tensor of shape [B * BS])\n",
    "    #      Initialize vector that show whether hypothesis is finished\n",
    "    #          (i.e. tensor of shape [B * BS])\n",
    "    # 2. While all sequences do not end with <eos> and their length less than max_generated_len\n",
    "    #      1. Get probabilities for the next token for each hypothesis \n",
    "    #          (i.e. tensor of shape [B * BS, V])\n",
    "    #      2. Use those probabilities to compute probability for each extension of each hypothesis\n",
    "    #          (i.e. tensor of shape [B * BS, V])\n",
    "    #      3. For each element in the batch select new BS best hypotheses\n",
    "    #          Note, that some of the hypotheses on the previous step have been finished\n",
    "    #            so their probability should not change. So you have to select BS best hypotheses\n",
    "    #            among all extension of unfinished hypotheses and finished hypotheses\n",
    "    #          As a result you will have a new token for best extensions of unfinished hypotheses\n",
    "    #          For simplisity you can use <EOS> token if you select finished hypothesis in the beam\n",
    "    #            i.e. tensor of shape [B * BS] of indices for selected hypotheses and\n",
    "    #                 tensor of shape [B * BS] of extension tokens for each hypothesis\n",
    "    #      4. Update probabilities for each hypotheses and is_finished state for each hypothesis\n",
    "    #          Concat new tokens to the existing prefixes\n",
    "    #      5. Update hidden and cell state to correspond to the selected hypothesis\n",
    "\n",
    "    batch_size = start_tokens.shape[1]\n",
    "\n",
    "    eos_idx = model.vocab.get_stoi()['<eos>']\n",
    "    \n",
    "    # Get embeddings for the start tokens\n",
    "    embedding = model.word_embeddings(start_tokens)\n",
    "    \n",
    "    # Make forward pass through the RNN and \n",
    "    #   obtain the last cell and hidden state for each element in the batch\n",
    "    all_h, all_c = [], []\n",
    "    h = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    c = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    for time_step in range(start_tokens.shape[0]):\n",
    "        _, (h, c) = model.rnn(embedding[time_step][None, :, :], (h, c))\n",
    "        all_h.append(h)\n",
    "        all_c.append(c)\n",
    "    \n",
    "    all_h = torch.stack(all_h, dim=1)\n",
    "    all_c = torch.stack(all_c, dim=1)\n",
    "\n",
    "    start_h = all_h[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])] # [L, B, H]\n",
    "    start_c = all_c[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])] # [L, B, H]\n",
    "\n",
    "    # Use those states as the initialization for each hypotheses in the beam\n",
    "    h = torch.cat([start_h] * beam_size, dim=1) # [L, B * BS, H]\n",
    "    c = torch.cat([start_c] * beam_size, dim=1) # [L, B * BS, H]\n",
    "    \n",
    "    # Select initial tokens for each hypotheses in the beam\n",
    "    #   Compute log probabilities and select top-beam_size tokens for each element\n",
    "    #   Use them to initialize beam search state\n",
    "    \n",
    "    \n",
    "    start_log_probas = F.log_softmax(model.linear(start_h[-1, :, :]), dim=-1) # [B, V]\n",
    "    log_probas, new_tokens = torch.topk(\n",
    "        input=start_log_probas,\n",
    "        k=beam_size,\n",
    "    ) # ([B, BS], [B, BS])\n",
    "    log_probas = log_probas.view((-1, )) # [B * BS]\n",
    "    new_tokens = new_tokens.view((-1, )) # [B * BS]\n",
    "    print(f\"DEB: {log_probas.shape=}\\t{new_tokens.shape=}\")\n",
    "    # hypotesis = new_tokens.reshape((1, -1)) # [1, B * BS]\n",
    "    hypotesis = torch.clone(new_tokens.reshape((1, -1))) # [1, B * BS]\n",
    "    print(f\"DEB: \", end=\"\")\n",
    "    print_tensor(hypotesis)\n",
    "    \n",
    "    is_finished = h.new_zeros((batch_size * beam_size, ), dtype=torch.bool) # [B * BS]\n",
    "    decoded_lens = h.new_zeros((batch_size * beam_size, ), dtype=torch.int64) # [B * BS]\n",
    "\n",
    "    while not torch.all(is_finished) and hypotesis.shape[0] < max_generated_len:\n",
    "        embedding = model.word_embeddings(new_tokens)\n",
    "        # print(f\"DEB: {embedding.shape=}\")\n",
    "        _, (new_h, new_c) = model.rnn(embedding[None, :, :], (h, c))\n",
    "        # print(f\"DEB: {h.shape=}\\t{new_h.shape=}\")\n",
    "        \n",
    "        # Get probabilities for the next token for each hypothesis\n",
    "        next_tokens_logits = model.linear(h[-1, :, :])\n",
    "        next_tokens_logits[is_finished] = -1e-10\n",
    "        next_tokens_logits[is_finished, eos_idx] = 0\n",
    "        \n",
    "        next_token_log_probas = F.log_softmax(next_tokens_logits, dim=-1)  # [B * BS, V]\n",
    "        \n",
    "        # Use those probabilities to compute probability for each extension of each hypothesis\n",
    "        print(f\"DEB: {next_token_log_probas.shape=}\\t{log_probas.shape=}\")\n",
    "        # extension_log_probas = log_probas[:, None] + next_token_log_probas * (~is_finished)[:, None]  # [B * BS, V]\n",
    "        extension_log_probas = log_probas[:, None] + next_token_log_probas  # [B * BS, V]\n",
    "\n",
    "        # For each element in the batch select new BS best hypotheses\n",
    "        #   You can use loop over different beams\n",
    "        for batch_idx in range(batch_size):\n",
    "            indices_to_process = torch.topk(\n",
    "                input=extension_log_probas[batch_idx * beam_size:(batch_idx + 1) * beam_size].view((-1, )), \n",
    "                k=beam_size\n",
    "            ).indices\n",
    "            beam_idx = indices_to_process // extension_log_probas.shape[1]\n",
    "            token_idx = indices_to_process % extension_log_probas.shape[1]\n",
    "            \n",
    "            print(f\"Beam idx: {beam_idx}\\tToken idx: {token_idx}\\tRaw indices: {indices_to_process=}\")\n",
    "            \n",
    "            new_tokens[batch_idx * beam_size:(batch_idx + 1) * beam_size] = token_idx\n",
    "            \n",
    "            idx = batch_idx * beam_size + beam_idx\n",
    "            # hypotesis = hypotesis.swapaxes(0, 1)\n",
    "            # hypotesis[batch_idx * beam_size:(batch_idx + 1) * beam_size] = hypotesis[batch_idx * beam_size:(batch_idx + 1) * beam_size][beam_idx]\n",
    "            # hypotesis = hypotesis.swapaxes(0, 1)\n",
    "            hypotesis[:, batch_idx * beam_size:(batch_idx + 1) * beam_size] = hypotesis[:, batch_idx * beam_size:(batch_idx + 1) * beam_size][:, beam_idx]\n",
    "            \n",
    "            # Update probabilities for each hypotheses and is_finished state and decoded_lens for each hypothesis\n",
    "            log_probas[idx] = extension_log_probas[idx, token_idx]\n",
    "            is_finished[idx] = (token_idx == eos_idx)\n",
    "            \n",
    "            # Update hidden and cell state to correspond to the selected hypothesis\n",
    "            h[:, idx, ] = new_h[:, idx, :]\n",
    "            c[:, idx, ] = new_c[:, idx, :]\n",
    "            \n",
    "        decoded_lens += (~is_finished)\n",
    "\n",
    "        # Concat new tokens to the existing prefixes\n",
    "        hypotesis = torch.cat([hypotesis, new_tokens.reshape((1, -1))], dim=0)\n",
    "        # hypotesis = torch.cat([torch.clone(hypotesis), torch.clone(new_tokens.reshape((1, -1)))], dim=0)\n",
    "        print(f\"DEB: \", end=\"\")\n",
    "        print_tensor(hypotesis)     \n",
    "        \n",
    "    return (\n",
    "        hypotesis.view(-1, start_tokens.shape[1], beam_size), \n",
    "        decoded_lens.view(start_tokens.shape[1], beam_size),\n",
    "        log_probas.view(start_tokens.shape[1], beam_size)\n",
    "    )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T21:23:08.732917600Z",
     "start_time": "2024-04-15T21:23:08.641326600Z"
    }
   },
   "execution_count": 348,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start_tokens = torch.tensor([\n",
    "    lm_model.vocab.lookup_indices(['<sos>', '<pad>', '<pad>', '<pad>']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'my', 'favorite', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'best', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'worst', 'movie']),\n",
    "]).T\n",
    "\n",
    "start_tokens_lens = torch.tensor([1, 4, 4, 4])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T21:23:08.950390600Z",
     "start_time": "2024-04-15T21:23:08.885187400Z"
    }
   },
   "execution_count": 349,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# lm_model.to(device).eval()\n",
    "# start_tokens = start_tokens.to(device)\n",
    "# start_tokens_lens = start_tokens_lens.to(device)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T21:23:09.071588100Z",
     "start_time": "2024-04-15T21:23:08.997492200Z"
    }
   },
   "execution_count": 350,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lm_model.cpu().eval()\n",
    "start_tokens = start_tokens.cpu()\n",
    "start_tokens_lens = start_tokens_lens.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T21:23:09.225982300Z",
     "start_time": "2024-04-15T21:23:09.201219300Z"
    }
   },
   "execution_count": 351
  },
  {
   "cell_type": "code",
   "source": [
    "beam_size = 100\n",
    "# beam_size = 10\n",
    "decoded_tokens, decoded_lens, log_probas = decode_beam_search(\n",
    "    lm_model, start_tokens, start_tokens_lens, max_generated_len=10, beam_size=beam_size\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T21:23:11.797793600Z",
     "start_time": "2024-04-15T21:23:09.353829700Z"
    }
   },
   "execution_count": 352,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEB: log_probas.shape=torch.Size([400])\tnew_tokens.shape=torch.Size([400])\n",
      "DEB: t.shape=torch.Size([1, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([ 3,  0,  2,  7,  0,  0,  0, 11,  1,  0,  4, 15, 19,  6,  3, 23,  0, 27,\n",
      "        31,  2,  3, 35,  0, 39, 10, 43,  4,  0,  5, 47, 51,  1,  2,  0,  8, 14,\n",
      "        55,  0,  1,  1, 59, 18,  4,  4,  7, 63, 67,  3, 71,  0,  2, 75, 22, 79,\n",
      "         4,  3,  0, 83, 26,  6, 87,  0, 12,  0,  2, 91, 30, 95,  0, 99,  2,  3,\n",
      "         0,  9,  1, 16,  0,  0,  7,  0, 34,  1,  4,  0, 38,  0, 20, 11,  1,  8,\n",
      "        42,  0,  3,  6,  0,  5,  0,  1, 15,  1])\tToken idx: tensor([ 525,    7,  525,  525,  115,  159,  261,  525,    1,   93,    7,  525,\n",
      "         525,  525,  130,  525,    1,  525,  525,    1,  131,  525,   29,  525,\n",
      "         525,  525,  115,  118,    1,  525,  525,  525,  130, 2059,    7,  525,\n",
      "         525,   91,    7,   93,  525,  525,  159,  261,  130,  525,  525,  136,\n",
      "         525,   50,  136,  525,  525,  525,   93,    1,  169,  525,  525,    1,\n",
      "         525, 1165,    7,  371,   93,  525,  525,  525,  704,  525,  131,    7,\n",
      "         579,    1,  136,    7,  426, 1023,  131,  130,  525,  130,    1,   78,\n",
      "         525, 1893,    7,  130,  115,  115,  525,  872,   93,  130,  709,  525,\n",
      "        2964,  159,  130,   29])\tRaw indices: indices_to_process=tensor([ 100584,       7,   67231,  233996,     115,     159,     261,  367408,\n",
      "          33354,      93,  133419,  500820,  634232,  200643,  100189,  767644,\n",
      "              1,  901056, 1034468,   66707,  100190, 1167880,      29, 1301292,\n",
      "         334055, 1434704,  133527,     118,  166766, 1568116, 1701528,   33878,\n",
      "          66836,    2059,  266831,  467467, 1834940,      91,   33360,   33446,\n",
      "        1968352,  600879,  133571,  133673,  233601, 2101764, 2235176,  100195,\n",
      "        2368588,      50,   66842, 2502000,  734291, 2635412,  133505,  100060,\n",
      "            169, 2768824,  867703,  200119, 2902236,    1165,  400243,     371,\n",
      "          66799, 3035648, 1001115, 3169060,     704, 3302472,   66837,  100066,\n",
      "            579,  300178,   33489,  533655,     426,    1023,  233602,     130,\n",
      "        1134527,   33483,  133413,      78, 1267939,    1893,  667067,  367013,\n",
      "          33468,  266939, 1401351,     872,  100152,  200248,     709,  167290,\n",
      "           2964,   33512,  500425,   33382])\n",
      "Beam idx: tensor([ 3,  0,  7, 11,  2,  0, 15,  0,  0, 19,  6, 23,  0,  3, 10,  1, 27, 31,\n",
      "         4, 35, 39,  0, 43, 47,  2,  3, 51, 14,  8, 55,  0, 59, 63, 67, 71, 75,\n",
      "         0, 18, 12, 79,  5,  7,  2, 83, 22, 11, 87,  0, 91,  0,  4, 95, 99,  1,\n",
      "         9, 26,  3,  6, 16,  2,  1, 30,  1,  0,  3, 34, 38,  7,  8,  4, 10,  0,\n",
      "        42, 11,  4, 46, 20,  2,  0,  0,  3,  2, 15,  0, 13,  0, 24, 50,  6,  4,\n",
      "         0,  0,  0, 54, 12, 58,  0,  8, 28,  8])\tToken idx: tensor([ 525,    7,  525,  525,  525,  115,  525,  159,  261,  525,  525,  525,\n",
      "          93,  130,  525,    1,  525,  525,    7,  525,  525,    1,  525,  525,\n",
      "           1,  131,  525,  525,    7,  525,   29,  525,  525,  525,  525,  525,\n",
      "         118,  525,    7,  525,    1,  130,  130,  525,  525,  130,  525, 2059,\n",
      "         525,   91,  115,  525,  525,  525,    1,  525,  136,    1,    7,  136,\n",
      "           7,  525,   93,   50,    1,  525,  525,  131,  115,  159,    1,  169,\n",
      "         525,  131,  261,  525,    7,   93, 1165,  371,    7,  131,  130,  704,\n",
      "           1,  579,    7,  525,  130,   93,  426, 1023,  130,  525,  115,  525,\n",
      "          78,  159,    7,  261])\tRaw indices: indices_to_process=tensor([ 100584,       7,  233996,  367408,   67231,     115,  500820,     159,\n",
      "            261,  634232,  200643,  767644,      93,  100189,  334055,   33354,\n",
      "         901056, 1034468,  133419, 1167880, 1301292,       1, 1434704, 1568116,\n",
      "          66707,  100190, 1701528,  467467,  266831, 1834940,      29, 1968352,\n",
      "        2101764, 2235176, 2368588, 2502000,     118,  600879,  400243, 2635412,\n",
      "         166766,  233601,   66836, 2768824,  734291,  367013, 2902236,    2059,\n",
      "        3035648,      91,  133527, 3169060, 3302472,   33878,  300178,  867703,\n",
      "         100195,  200119,  533655,   66842,   33360, 1001115,   33446,      50,\n",
      "         100060, 1134527, 1267939,  233602,  266939,  133571,  333531,     169,\n",
      "        1401351,  367014,  133673, 1534763,  667067,   66799,    1165,     371,\n",
      "         100066,   66837,  500425,     704,  433590,     579,  800479, 1668175,\n",
      "         200248,  133505,     426,    1023,     130, 1801587,  400351, 1934999,\n",
      "             78,  266983,  933891,  267085])\n",
      "Beam idx: tensor([ 0,  3,  0,  0,  0,  2,  0,  7,  0, 11,  1, 15,  0,  6,  0,  4, 19,  0,\n",
      "         3,  0,  2, 23,  0, 27,  0, 10, 31,  1,  3,  0,  5,  0, 35,  2,  0,  0,\n",
      "         1,  1, 14, 39,  0,  0,  0, 43,  8,  0,  4, 47,  0, 51, 55,  0,  0, 59,\n",
      "         2, 18,  0, 63,  0,  0, 67,  0,  7,  0, 12, 71,  0,  0,  4,  0, 75,  0,\n",
      "         4,  2, 79,  3,  0,  1,  6,  2, 83,  0,  0,  1,  0,  3,  0, 22,  0,  0,\n",
      "        87,  4,  0, 16, 91,  1, 95,  0,  0,  9])\tToken idx: tensor([   7,  525,  115,  159,  261,  525,   93,  525,    1,  525,    1,  525,\n",
      "          29,  525,  118,    7,  525, 2059,  130,   91,    1,  525,   50,  525,\n",
      "         169,  525,  525,  525,  131, 1165,    1,  371,  525,  130,  704,  579,\n",
      "           7,   93,  525,  525,  426, 1023,  130,  525,    7,   78,  115,  525,\n",
      "        1893,  525,  525,  872,  709,  525,  136,  525, 2964,  525, 1123,  462,\n",
      "         525, 3694,  130,  369,    7,  525,  239,   95,  159,  510,  525,  262,\n",
      "         261,   93,  525,  136,  689,  136,    1,  131,  525,   87,  236,  130,\n",
      "         131,    1,  518,  525,  455, 1213,  525,   93, 1497,    7,  525,  115,\n",
      "         525,  874, 1203,    1])\tRaw indices: indices_to_process=tensor([      7,  100584,     115,     159,     261,   67231,      93,  233996,\n",
      "              1,  367408,   33354,  500820,      29,  200643,     118,  133419,\n",
      "         634232,    2059,  100189,      91,   66707,  767644,      50,  901056,\n",
      "            169,  334055, 1034468,   33878,  100190,    1165,  166766,     371,\n",
      "        1167880,   66836,     704,     579,   33360,   33446,  467467, 1301292,\n",
      "            426,    1023,     130, 1434704,  266831,      78,  133527, 1568116,\n",
      "           1893, 1701528, 1834940,     872,     709, 1968352,   66842,  600879,\n",
      "           2964, 2101764,    1123,     462, 2235176,    3694,  233601,     369,\n",
      "         400243, 2368588,     239,      95,  133571,     510, 2502000,     262,\n",
      "         133673,   66799, 2635412,  100195,     689,   33489,  200119,   66837,\n",
      "        2768824,      87,     236,   33483,     131,  100060,     518,  734291,\n",
      "            455,    1213, 2902236,  133505,    1497,  533655, 3035648,   33468,\n",
      "        3169060,     874,    1203,  300178])\n",
      "Beam idx: tensor([ 0,  0,  0,  0,  0,  3,  0,  2,  0,  7,  0,  0,  0,  1,  0,  0, 11,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  6,  0,  4,  2, 15,  0,  0,  0,  0,  0,  0,\n",
      "         3,  0,  0,  0,  0,  1,  0,  0, 19,  1,  2,  0,  1,  0,  0,  0,  0,  0,\n",
      "         0, 23,  0,  0,  0,  0,  0,  0, 27,  3,  0,  0,  0, 10,  0,  0,  5,  0,\n",
      "         2,  0, 31,  4,  0,  0, 35,  0,  0,  0,  0,  0,  0,  1,  0,  7,  2,  8,\n",
      "         0, 39,  0,  0,  0,  0, 14,  0,  2,  1])\tToken idx: tensor([   7,  115,  159,  261,   93,  525,    1,  525,   29,  525,  118, 2059,\n",
      "          91,    1,   50,  169,  525, 1165,  371,  704,  579,  426, 1023,  130,\n",
      "          78,  525, 1893,    7,    1,  525,  872,  709, 2964, 1123,  462, 3694,\n",
      "         130,  369,  239,   95,  510,  525,  262,  689,  525,    7,  130,   87,\n",
      "          93,  236,  131,  518,  455, 1213, 1497,  525,  874, 1203,  305,   60,\n",
      "          40,   99,  525,  131, 1385,  706,  241,  525, 1913, 1302,    1,  283,\n",
      "         136,   26,  525,  115,  190,  885,  525,  235,  877, 1804,   28,  447,\n",
      "        4740,  136,  206,  130,   93,    7,  286,  525,  230,  212,  124,  148,\n",
      "         525, 2416,  131,  130])\tRaw indices: indices_to_process=tensor([      7,     115,     159,     261,      93,  100584,       1,   67231,\n",
      "             29,  233996,     118,    2059,      91,   33354,      50,     169,\n",
      "         367408,    1165,     371,     704,     579,     426,    1023,     130,\n",
      "             78,  200643,    1893,  133419,   66707,  500820,     872,     709,\n",
      "           2964,    1123,     462,    3694,  100189,     369,     239,      95,\n",
      "            510,   33878,     262,     689,  634232,   33360,   66836,      87,\n",
      "          33446,     236,     131,     518,     455,    1213,    1497,  767644,\n",
      "            874,    1203,     305,      60,      40,      99,  901056,  100190,\n",
      "           1385,     706,     241,  334055,    1913,    1302,  166766,     283,\n",
      "          66842,      26, 1034468,  133527,     190,     885, 1167880,     235,\n",
      "            877,    1804,      28,     447,    4740,   33489,     206,  233601,\n",
      "          66799,  266831,     286, 1301292,     230,     212,     124,     148,\n",
      "         467467,    2416,   66837,   33483])\n",
      "DEB: t.shape=torch.Size([2, 400])\n",
      "tensor=(\n",
      "\ttensor([ 261,    7,  159,  118,    7,    7,    7,  169,  115,    7,   93,  579,\n",
      "          78,   29,  261, 2964,    7,  369,  262,  159,  261,  131,    7, 1497,\n",
      "          50,   60,   93,    7,    1,  706,  283,  115,  159,    7, 2059,  704,\n",
      "         235,    7,  115,  115,  447,  130,   93,   93,  118,  230, 2416,  261,\n",
      "         322,    7,  159,   75,  709, 1604,   93,  261,    7, 3414, 3694,   29,\n",
      "        1169,    7, 1165,    7,  159,    8,  510, 1963,    7,  864,  159,  261,\n",
      "           7,   91,  115,  426,    7,    7,  118,    7,  236,  115,   93,    7,\n",
      "        1213,    7, 1893,  169,  115, 2059,  305,    7,  261,   29,    7,    1,\n",
      "           7,  115,  579,  115,   93,    1,  159,  689,    7,    1, 1722,    1,\n",
      "           1,  230,  115,  261,    1,   93, 1023,  525,   91, 1203,  136,  371,\n",
      "          76,    1,   95,  528,    7,   93, 1318,  169,   29,  190,    1, 1346,\n",
      "         431,  998,  463, 1497,    1,   60,  426,  456,  130,  159,    7,  211,\n",
      "         471,  689,   39,    1,  119,    1,  136,  518,  447,  525,   28, 2059,\n",
      "          93,  115,  241,    7,  525,  510,  525,    1,   93,   99,  343,  159,\n",
      "          29,  136, 1023,    1, 1496,  689,  136,  118,  369,    7,    1,    1,\n",
      "          93,    7, 1722,    1,  131,    1,  212, 1500,  115,  136,    1,    1,\n",
      "           1,  704,  426,   72,    1,   29,  205,   29,  525,  136,  525,  525,\n",
      "         525,  130,  525,  115,  525,   29,    1,  241,  525,    7,  525,   93,\n",
      "        1203,  525,  136,  525,  130,   65,  525,  261,  525,  689,  205,    1,\n",
      "         136,  525,  131,  525,  435,  130,  525,  525,    1,    1,  426,   90,\n",
      "         525,  525,  525,   76,   28,  525,   93,   26,  525,   99, 1491,  525,\n",
      "         525,   95,  130, 1722,  525,  882,  525,  525, 1549,  525,  115,  525,\n",
      "          60,  431,  525,  525,   93,  525,  198,  525,   93,  130,  237,  136,\n",
      "         525,    1,    7,  130,  190,  525,  525,    1,  525,  136,  525,  369,\n",
      "         525,  525,   53,   93,  525,  471,  171,    1,  250,  525,  525, 1023,\n",
      "         525,  525,  525,  525,  525,  136,  525,  131,  525,  689,  525,  525,\n",
      "         525,  130,  525,  525,  115,  525,  525,  525,  525,  525,  525,  525,\n",
      "         525,   93,  525,    1,  131,  426,  525,  525,  525,  525,  525,  525,\n",
      "         136,  525,  525,  525,  525,  130,  525,  525,  230,  130,  131,  525,\n",
      "         130,  525,  525,  525,  525,  525,  525,  471,  525,  525,  525,  525,\n",
      "         525,  525,  261,  136,  525,  525,  525,   29,  525,  525,    7,  525,\n",
      "         131,  525,  882,    1,  525,  525,    8,  525,  525,  525,  525,  525,\n",
      "         525,  130,  525,  689,  131,  241,  525,   26,  525,  525,  525,  525,\n",
      "        1023,  525,  131,  130])\n",
      "\ttensor([ 525,    7,  525,  525,  115,  159,  261,  525,    1,   93,    7,  525,\n",
      "         525,  525,  130,  525,    1,  525,  525,    1,  131,  525,   29,  525,\n",
      "         525,  525,  115,  118,    1,  525,  525,  525,  130, 2059,    7,  525,\n",
      "         525,   91,    7,   93,  525,  525,  159,  261,  130,  525,  525,  136,\n",
      "         525,   50,  136,  525,  525,  525,   93,    1,  169,  525,  525,    1,\n",
      "         525, 1165,    7,  371,   93,  525,  525,  525,  704,  525,  131,    7,\n",
      "         579,    1,  136,    7,  426, 1023,  131,  130,  525,  130,    1,   78,\n",
      "         525, 1893,    7,  130,  115,  115,  525,  872,   93,  130,  709,  525,\n",
      "        2964,  159,  130,   29,  525,    7,  525,  525,  525,  115,  525,  159,\n",
      "         261,  525,  525,  525,   93,  130,  525,    1,  525,  525,    7,  525,\n",
      "         525,    1,  525,  525,    1,  131,  525,  525,    7,  525,   29,  525,\n",
      "         525,  525,  525,  525,  118,  525,    7,  525,    1,  130,  130,  525,\n",
      "         525,  130,  525, 2059,  525,   91,  115,  525,  525,  525,    1,  525,\n",
      "         136,    1,    7,  136,    7,  525,   93,   50,    1,  525,  525,  131,\n",
      "         115,  159,    1,  169,  525,  131,  261,  525,    7,   93, 1165,  371,\n",
      "           7,  131,  130,  704,    1,  579,    7,  525,  130,   93,  426, 1023,\n",
      "         130,  525,  115,  525,   78,  159,    7,  261,    7,  525,  115,  159,\n",
      "         261,  525,   93,  525,    1,  525,    1,  525,   29,  525,  118,    7,\n",
      "         525, 2059,  130,   91,    1,  525,   50,  525,  169,  525,  525,  525,\n",
      "         131, 1165,    1,  371,  525,  130,  704,  579,    7,   93,  525,  525,\n",
      "         426, 1023,  130,  525,    7,   78,  115,  525, 1893,  525,  525,  872,\n",
      "         709,  525,  136,  525, 2964,  525, 1123,  462,  525, 3694,  130,  369,\n",
      "           7,  525,  239,   95,  159,  510,  525,  262,  261,   93,  525,  136,\n",
      "         689,  136,    1,  131,  525,   87,  236,  130,  131,    1,  518,  525,\n",
      "         455, 1213,  525,   93, 1497,    7,  525,  115,  525,  874, 1203,    1,\n",
      "           7,  115,  159,  261,   93,  525,    1,  525,   29,  525,  118, 2059,\n",
      "          91,    1,   50,  169,  525, 1165,  371,  704,  579,  426, 1023,  130,\n",
      "          78,  525, 1893,    7,    1,  525,  872,  709, 2964, 1123,  462, 3694,\n",
      "         130,  369,  239,   95,  510,  525,  262,  689,  525,    7,  130,   87,\n",
      "          93,  236,  131,  518,  455, 1213, 1497,  525,  874, 1203,  305,   60,\n",
      "          40,   99,  525,  131, 1385,  706,  241,  525, 1913, 1302,    1,  283,\n",
      "         136,   26,  525,  115,  190,  885,  525,  235,  877, 1804,   28,  447,\n",
      "        4740,  136,  206,  130,   93,    7,  286,  525,  230,  212,  124,  148,\n",
      "         525, 2416,  131,  130])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([43, 18, 24, 46, 50, 28, 13, 54, 32, 58, 17, 62, 36, 66, 70, 40, 18, 74,\n",
      "        78, 21, 44, 82, 48, 25, 86, 52, 90, 94, 29, 98, 56, 60, 39, 33, 24, 64,\n",
      "        37, 68, 72, 28, 41, 79, 76, 30, 23, 32, 19, 80, 45, 13, 84, 36, 49, 88,\n",
      "        24, 53, 92, 96, 24, 40, 13, 13, 17, 28, 44, 28, 87, 57, 91, 46, 61, 32,\n",
      "        48, 17, 24, 17, 32, 50, 65, 31, 52, 21, 69, 54, 36, 28, 73, 36, 77, 56,\n",
      "        40, 27, 21, 25, 21, 32, 58, 16, 40, 60])\tToken idx: tensor([525, 131,   7, 525, 525,   7,   1, 525,   7, 525,   1, 525,   7, 525,\n",
      "        525,   7, 525, 525, 525,   1,   7, 525,   7,   1, 525,   7, 525, 525,\n",
      "          1, 525,   7,   7, 525,   1, 115,   7,   1,   7,   7, 115,   1, 525,\n",
      "          7, 131, 525, 115,   7,   7,   1, 525,   7, 115,   1,   7, 159,   1,\n",
      "          7,   7, 261, 115,   7,  93, 525, 159, 115, 261, 143,   1, 525,   1,\n",
      "          1, 159, 115,   7,  93,  93, 261,   1,   1,   7, 115, 525,   1,   1,\n",
      "        159,  93,   1, 261,   1, 115, 159, 525,   7, 525,  93,  93,   1,   7,\n",
      "        261, 115])\tRaw indices: indices_to_process=tensor([1434704,  600485,  800479, 1534763, 1668175,  933891,  433590, 1801587,\n",
      "        1067303, 1934999,  567002, 2068411, 1200715, 2201823, 2335235, 1334127,\n",
      "         600879, 2468647, 2602059,  700414, 1467539, 2735471, 1600951,  833826,\n",
      "        2868883, 1734363, 3002295, 3135707,  967238, 3269119, 1867775, 2001187,\n",
      "        1301292, 1100650,  800587, 2134599, 1234062, 2268011, 2401423,  933999,\n",
      "        1367474, 2635412, 2534835, 1000721,  767644, 1067411,  633714, 2668247,\n",
      "        1500886,  434114, 2801659, 1200823, 1634298, 2935071,  800631, 1767710,\n",
      "        3068483, 3201895,  800733, 1334235,  433596,  433682,  567526,  934043,\n",
      "        1467647,  934145, 2901854, 1901122, 3035648, 1534239, 2034534, 1067455,\n",
      "        1601059,  567008,  800565,  567094, 1067557, 1667651, 2167946, 1033950,\n",
      "        1734471,  700938, 2301358, 1801063, 1200867,  933977, 2434770, 1200969,\n",
      "        2568182, 1867883, 1334279,  901056,  700420,  834350,  700506, 1067389,\n",
      "        1934475,  533655, 1334381, 2001295])\n",
      "Beam idx: tensor([31,  5, 62, 66, 17, 32, 70, 36, 74, 40, 78, 82, 44, 86, 48, 21, 90, 94,\n",
      "        52, 98, 56, 25, 23, 60, 29, 64, 68, 33, 72, 37, 76, 80, 18, 27, 41, 45,\n",
      "        84, 88, 49, 19, 32, 30, 92, 96, 36, 53, 40,  5, 51, 57, 44, 17,  1, 48,\n",
      "        61, 75, 15, 65, 79, 69, 39, 17, 17, 31, 73, 52, 32, 51, 77, 56, 21, 32,\n",
      "        36, 81, 36, 40, 27, 60, 40,  3, 85, 62, 66, 64, 44, 43, 68, 21, 21, 89,\n",
      "        71, 23, 44, 70, 48, 32, 93, 72, 25, 74])\tToken idx: tensor([ 525,  131,  525,  525,    1,    7,  525,    7,  525,    7,  525,  525,\n",
      "           7,  525,    7,    1,  525,  525,    7,  525,    7,    1,    7,    7,\n",
      "           1,    7,    7,    1,    7,    1,    7,    7,  525,  872,    1,    1,\n",
      "           7,    7,    1,  525,  115,  131,    7,    7,  115,    1,  115,  525,\n",
      "          28,    1,  115,  525,  131,  115,    1,  525, 1732,    1,  525,    1,\n",
      "         525,    7,   93,  130,    1,  115,  159,  525,    1,  115,  525,  261,\n",
      "         159,    1,  261,  159,  525,  115,  261,  525,    1,    1,    1,  115,\n",
      "         159,    7,  115,    7,   93,    1,  241,  115,  261,    1,  159,   93,\n",
      "           1,  115,  525,    1])\tRaw indices: indices_to_process=tensor([1034468,  166896, 2068411, 2201823,  567002, 1067303, 2335235, 1200715,\n",
      "        2468647, 1334127, 2602059, 2735471, 1467539, 2868883, 1600951,  700414,\n",
      "        3002295, 3135707, 1734363, 3269119, 1867775,  833826,  767126, 2001187,\n",
      "         967238, 2134599, 2268011, 1100650, 2401423, 1234062, 2534835, 2668247,\n",
      "         600879,  901403, 1367474, 1500886, 2801659, 2935071, 1634298,  634232,\n",
      "        1067411, 1000721, 3068483, 3201895, 1200823, 1767710, 1334235,  167290,\n",
      "        1701031, 1901122, 1467647,  567526,   33484, 1601059, 2034534, 2502000,\n",
      "         502027, 2167946, 2635412, 2301358, 1301292,  567008,  567094, 1034073,\n",
      "        2434770, 1734471, 1067455, 1701528, 2568182, 1867883,  700938, 1067557,\n",
      "        1200867, 2701594, 1200969, 1334279,  901056, 2001295, 1334381,  100584,\n",
      "        2835006, 2067887, 2201299, 2134707, 1467691, 1434186, 2268119,  700420,\n",
      "         700506, 2968418, 2368304,  767234, 1467793, 2334711, 1601103, 1067389,\n",
      "        3101830, 2401531,  834350, 2468123])\n",
      "Beam idx: tensor([19, 15, 99, 26, 30, 34, 13, 38, 20, 42, 46, 50, 24, 54,  2, 58, 17, 62,\n",
      "        28, 18, 66, 32, 70, 74, 36, 78, 11, 40, 82, 21, 44, 86, 48, 90,  2, 94,\n",
      "        52, 56, 98, 15, 60, 25, 20, 64, 29, 23, 68, 13, 72, 33, 19, 76, 24, 27,\n",
      "        80,  7, 11, 37, 13, 13, 26, 41, 28, 30, 84, 20, 45, 32, 88, 20, 92, 34,\n",
      "        49, 17, 53, 36, 96, 35, 15, 38, 57, 24, 61, 40, 24, 42, 17, 17, 20, 65,\n",
      "        44, 46, 28, 69, 48, 26, 28, 50, 73, 30])\tToken idx: tensor([ 525,  525,  525,  525,  525,  525,    1,  525,    7,  525,  525,  525,\n",
      "           7,  525,  131,  525,    1,  525,    7, 1732,  525,    7,  525,  525,\n",
      "           7,  525,  525,    7,  525,    1,    7,  525,    7,  525,  525,  525,\n",
      "           7,    7,  525,  131,    7,    1,  115,    7,    1,  525,    7,  525,\n",
      "           7,    1,  130,    7,  115,    7,    7,  525,  689,    1,    7,   93,\n",
      "           1,    1,  115,    1,    7,  159,    1,  115,    7,  261,    7,    1,\n",
      "           1,  525,    1,  115,    7,  525,  130,    1,    1,  159,    1,  115,\n",
      "         261,    1,    7,   93,   93,    1,  115,    1,  159,    1,  115,  130,\n",
      "         261,    1,    1,  130])\tRaw indices: indices_to_process=tensor([ 634232,  500820, 3302472,  867703, 1001115, 1134527,  433590, 1267939,\n",
      "         667067, 1401351, 1534763, 1668175,  800479, 1801587,   66837, 1934999,\n",
      "         567002, 2068411,  933891,  602086, 2201823, 1067303, 2335235, 2468647,\n",
      "        1200715, 2602059,  367408, 1334127, 2735471,  700414, 1467539, 2868883,\n",
      "        1600951, 3002295,   67231, 3135707, 1734363, 1867775, 3269119,  500426,\n",
      "        2001187,  833826,  667175, 2134599,  967238,  767644, 2268011,  434114,\n",
      "        2401423, 1100650,  633837, 2534835,  800587,  900538, 2668247,  233996,\n",
      "         367572, 1234062,  433596,  433682,  867179, 1367474,  933999, 1000591,\n",
      "        2801659,  667219, 1500886, 1067411, 2935071,  667321, 3068483, 1134003,\n",
      "        1634298,  567526, 1767710, 1200823, 3201895, 1167880,  500425, 1267415,\n",
      "        1901122,  800631, 2034534, 1334235,  800733, 1400827,  567008,  567094,\n",
      "         667153, 2167946, 1467647, 1534239,  934043, 2301358, 1601059,  867308,\n",
      "         934145, 1667651, 2434770, 1000720])\n",
      "Beam idx: tensor([43, 18, 47, 51, 55, 12, 11, 59, 63, 22, 67,  9, 71, 75, 26, 79,  1, 16,\n",
      "        83, 30, 87, 34, 91, 95, 13, 20, 99, 38, 42, 24,  6, 12, 28, 46, 17, 32,\n",
      "        50, 54, 58, 36,  9, 21, 16, 12, 35, 62, 40, 12, 66, 70, 18,  9,  9, 74,\n",
      "        44, 12, 25, 78, 20,  6, 82, 29, 15,  1, 22, 48, 86, 16, 52, 90, 16, 13,\n",
      "        56, 33, 94,  6, 24, 27, 18, 98, 26,  0, 60, 43, 28, 11, 13, 13, 37, 64,\n",
      "        16, 12, 30, 20, 68, 41, 19, 20, 32,  9])\tToken idx: tensor([ 525,  525,  525,  525,  525,    7,  525,  525,  525,  525,  525,    1,\n",
      "         525,  525,  525,  525,  131,    7,  525,  525,  525,  525,  525,  525,\n",
      "           1,    7,  525,  525,  525,    7,  525,  115,    7,  525,    1,    7,\n",
      "         525,  525,  525,    7,  525,    1,  115,  159,  525,  525,    7,  261,\n",
      "         525,  525,    1,    7,   93,  525,    7,   93,    1,  525,  115,   60,\n",
      "         525,    1,    7,  525,    1,    7,  525,  159,    7,  525,  261,  525,\n",
      "           7,    1,  525, 1203,  115,    7,  130,  525,    1,  131,    7,  130,\n",
      "         115,  136,    7,   93,    1,    7,   93,    1,    1,  159,    7,    1,\n",
      "         525,  261,  115,  136])\tRaw indices: indices_to_process=tensor([1434704,  600879, 1568116, 1701528, 1834940,  400243,  367408, 1968352,\n",
      "        2101764,  734291, 2235176,  300178, 2368588, 2502000,  867703, 2635412,\n",
      "          33484,  533655, 2768824, 1001115, 2902236, 1134527, 3035648, 3169060,\n",
      "         433590,  667067, 3302472, 1267939, 1401351,  800479,  200643,  400351,\n",
      "         933891, 1534763,  567002, 1067303, 1668175, 1801587, 1934999, 1200715,\n",
      "         300702,  700414,  533763,  400395, 1167880, 2068411, 1334127,  400497,\n",
      "        2201823, 2335235,  600355,  300184,  300270, 2468647, 1467539,  400329,\n",
      "         833826, 2602059,  667175,  200178, 2735471,  967238,  500302,   33878,\n",
      "         733767, 1600951, 2868883,  533807, 1734363, 3002295,  533909,  434114,\n",
      "        1867775, 1100650, 3135707,  201321,  800587,  900538,  600484, 3269119,\n",
      "         867179,     131, 2001187, 1434309,  933999,  367019,  433596,  433682,\n",
      "        1234062, 2134599,  533741,  400237, 1000591,  667219, 2268011, 1367474,\n",
      "         634232,  667321, 1067411,  300313])\n",
      "DEB: t.shape=torch.Size([3, 400])\n",
      "tensor=(\n",
      "\ttensor([  93,  262,   50, 2416,  159,    1,   29,   93,  159, 3694,  369, 1165,\n",
      "         235,  510,  159,  447,  262,  115,  118,  131,  118,   93,  322,   60,\n",
      "        1893,  709,  305,    7,  706,  579,    7, 1169,  115,    7,   50,  159,\n",
      "           7,    7,    7,    1,  130,    7,    7,  283, 1497,  159,  159,  236,\n",
      "         230,   29, 1213,  235,    7,  115,   50, 1604,  261,    7,   50,  447,\n",
      "          29,   29,  369,    1,  118,    1,  169, 3414,    7, 2416,    7,  159,\n",
      "         322,  369,   50,  369,  159,  159,    8,  115,  709,  131,  864,   93,\n",
      "         235,    1,   91,  235,    7,    7,  447,    7,  131,   60,  131,  159,\n",
      "        3694,    7,  447, 1169, 1346,    1,  525,  343, 1203,  431, 1023,    1,\n",
      "         136,  130,    1, 1722,  471,  212,  119,    1,    1,  426,  447,  205,\n",
      "          93,   93,  528,  525,  190,   93,   29,  998, 1496,   60,  369,   93,\n",
      "         136,  169,  159,  689,  131,  115,    1,  371,  431,    1,    1,    1,\n",
      "           1,  525,  130,    1,  518,  115,  471, 1203,    1,  119,  510,  118,\n",
      "         525,   99,    1,  136,  456, 1203, 1203, 1346,  689,  447,  431,  518,\n",
      "           7,   93,    1,  431,    1,    7,    1,  130,  169,  525,  130,  689,\n",
      "           1,  525,  343,   93,  471,  211,   29,    1,    1,  136,    1,  528,\n",
      "         471, 1023,  119,  431,  704, 1496,   93,  136,  525,   93, 1023,  205,\n",
      "         131,  525,    7,  426,  130,  525,   93, 1491,  525,  130,  525,  525,\n",
      "         525,  115,  136,  136,  525,  435,  198,  237,    1,    7,  241,  525,\n",
      "         525,   65,   28,  525,  525,   53,  525,  171,  525,  525,  525,   93,\n",
      "        1549,  689,  130,   60,  525,  261,   93,    7,   93,  130,  525,  525,\n",
      "         525,    1,  190,  115,  241,    1,    7,    7,  205,  525,  136,  131,\n",
      "         525,  130,  525,  435,  525,  130,  525,  525,   99,  525,   95,    1,\n",
      "         250,  525,   93,  426,  882,  525,  525,  525,  525,  525,  525,  525,\n",
      "         130,  431,   28,   93,  136,  525,  525,  205,  136, 1491,  130,  131,\n",
      "         525,  525,  525,  525,  471,  525,  525,  525,  136,  525,   29,  689,\n",
      "         525,    1,  525,  525,  525,  115,  525,  525,  689,  525,   26,  525,\n",
      "         130,  525,  130,  525,  525,  525,  525,  525,  131,  131,  525,  525,\n",
      "         525,  525,  525,  136,  689,  525,  115,  525,  525,  261,  525,  525,\n",
      "         525,    7,  525,  689,  689,  882,  230,  525,   93,    8,  525,  525,\n",
      "         525,  426,  525,  525,  525,  130,  525,  115,  525,  525,  115,  130,\n",
      "         525,  525,  525,  525,  525,    1,  525,  131,  525,  525,  525,  525,\n",
      "         131,  525,  130,  130,  525,  525,  115,  525,  525,  525,  525,  130,\n",
      "         525,  525,  525,  689])\n",
      "\ttensor([ 261,  525,  525,  525,  136,    1,  525,   93,  130,  525,  525,    7,\n",
      "         525,  525,  131,  525,  525,  136,  131,  525,  130,    1,  525,  525,\n",
      "           7,  525,  525,  709,  525,  130,  169,  525,   93, 2059,  525,   93,\n",
      "          91,  704,  579,    1,  525,  130,  426,  525,  525,  130,    1,  525,\n",
      "         525,  525,  525,  525,   50,  115,  525,  525,   93, 2964,  525,  525,\n",
      "         525,  525,  525,    1,  130,    1,  130,  525,  872,  525, 1165,  130,\n",
      "         525,  525,  525,  525,  130,  136,  525,  525,  525,  525,  525,   93,\n",
      "         525,    1,    1,  525, 1023,  169,  525,  118,  525,  525,  525,  130,\n",
      "         525,    1,  525,  525,  525,  115,   93,  525,  525,  525,    1,  118,\n",
      "         261,    1, 1165,  130,  525,    7,  525,    1,  426,  115,  525,    7,\n",
      "         136,  131,  525,    7,  525,    1,  115,  525,  525,  525,    7,    7,\n",
      "           7,  525,  130,  130,    1,  130,   91,  525,  525,   29,  130,   78,\n",
      "         118,  525,    1,  115,  525,    1,  525,  525,    7,  525,  525,  525,\n",
      "           1,  525,  371,  159,  525,  525,  525,  525,  131,  525,  525,  525,\n",
      "          93,  136,    1,  525,  118,  131,  118,    1,  525,    7,    1,  525,\n",
      "         579,   93,  525,    1,  525,  525,  115,    1,    1,   93,  169,  525,\n",
      "         525,    1,  525,  525,  525,  525,  131,  261,   91,    7,    1,  525,\n",
      "           1,  704,  525,  525,    1,  130,  115,  525,  169,  136,  115, 1123,\n",
      "        2059,  130,  131,  130,  239,  525,  525,  525,    7,    1,  525,  426,\n",
      "         236,  525,    7,  518, 1893,  525,  115,  525,  709, 2964, 1203,    7,\n",
      "         525,  525,    1,    7, 1165,  525,  159,  525,  261,  130,   91,  689,\n",
      "         169,  525,  525,  525,  525,   93,  525,  525,  525, 1023,  131,    1,\n",
      "         131,    1,   78,  525,  455,    1, 1497,  704,  525, 2059,  525,    7,\n",
      "         525,  579,    7,  525,  525,  169, 3694,  426,  169,  130, 2059, 2059,\n",
      "           1,  525,    7,  115,  131,  510, 1893,  525,  131,  525,   93,    1,\n",
      "         689,  371,   87,  518,  525,   91, 2059,   60,  131, 1023,  525,  525,\n",
      "         283,  115, 1893,  235,  115,  525,  447,  872,  130,  462,  525,  148,\n",
      "           1,  579,  130,  239,  262,   78,    1,   91,    1,  130, 1165, 2964,\n",
      "         131, 1497,  305,  130,  525,  426,  525,   91, 3694,  525,  510,   91,\n",
      "         241,    1,  371,  525,  525,  525,  525,   91,  525,  525,  579,    1,\n",
      "          28,  525,  169,  115, 1023,   93,  206,  525,  455,  286,  525,    1,\n",
      "         874, 1123,  124,    1,   78,    7,  371,  131, 1893,    7,   40,  689,\n",
      "           1, 2059,    1,    1,  369, 1385,  525,   91,  872,  579, 1913,  525,\n",
      "         704,  579, 2964,  525])\n",
      "\ttensor([ 525,  131,    7,  525,  525,    7,    1,  525,    7,  525,    1,  525,\n",
      "           7,  525,  525,    7,  525,  525,  525,    1,    7,  525,    7,    1,\n",
      "         525,    7,  525,  525,    1,  525,    7,    7,  525,    1,  115,    7,\n",
      "           1,    7,    7,  115,    1,  525,    7,  131,  525,  115,    7,    7,\n",
      "           1,  525,    7,  115,    1,    7,  159,    1,    7,    7,  261,  115,\n",
      "           7,   93,  525,  159,  115,  261,  143,    1,  525,    1,    1,  159,\n",
      "         115,    7,   93,   93,  261,    1,    1,    7,  115,  525,    1,    1,\n",
      "         159,   93,    1,  261,    1,  115,  159,  525,    7,  525,   93,   93,\n",
      "           1,    7,  261,  115,  525,  131,  525,  525,    1,    7,  525,    7,\n",
      "         525,    7,  525,  525,    7,  525,    7,    1,  525,  525,    7,  525,\n",
      "           7,    1,    7,    7,    1,    7,    7,    1,    7,    1,    7,    7,\n",
      "         525,  872,    1,    1,    7,    7,    1,  525,  115,  131,    7,    7,\n",
      "         115,    1,  115,  525,   28,    1,  115,  525,  131,  115,    1,  525,\n",
      "        1732,    1,  525,    1,  525,    7,   93,  130,    1,  115,  159,  525,\n",
      "           1,  115,  525,  261,  159,    1,  261,  159,  525,  115,  261,  525,\n",
      "           1,    1,    1,  115,  159,    7,  115,    7,   93,    1,  241,  115,\n",
      "         261,    1,  159,   93,    1,  115,  525,    1,  525,  525,  525,  525,\n",
      "         525,  525,    1,  525,    7,  525,  525,  525,    7,  525,  131,  525,\n",
      "           1,  525,    7, 1732,  525,    7,  525,  525,    7,  525,  525,    7,\n",
      "         525,    1,    7,  525,    7,  525,  525,  525,    7,    7,  525,  131,\n",
      "           7,    1,  115,    7,    1,  525,    7,  525,    7,    1,  130,    7,\n",
      "         115,    7,    7,  525,  689,    1,    7,   93,    1,    1,  115,    1,\n",
      "           7,  159,    1,  115,    7,  261,    7,    1,    1,  525,    1,  115,\n",
      "           7,  525,  130,    1,    1,  159,    1,  115,  261,    1,    7,   93,\n",
      "          93,    1,  115,    1,  159,    1,  115,  130,  261,    1,    1,  130,\n",
      "         525,  525,  525,  525,  525,    7,  525,  525,  525,  525,  525,    1,\n",
      "         525,  525,  525,  525,  131,    7,  525,  525,  525,  525,  525,  525,\n",
      "           1,    7,  525,  525,  525,    7,  525,  115,    7,  525,    1,    7,\n",
      "         525,  525,  525,    7,  525,    1,  115,  159,  525,  525,    7,  261,\n",
      "         525,  525,    1,    7,   93,  525,    7,   93,    1,  525,  115,   60,\n",
      "         525,    1,    7,  525,    1,    7,  525,  159,    7,  525,  261,  525,\n",
      "           7,    1,  525, 1203,  115,    7,  130,  525,    1,  131,    7,  130,\n",
      "         115,  136,    7,   93,    1,    7,   93,    1,    1,  159,    7,    1,\n",
      "         525,  261,  115,  136])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([18, 30, 66, 98, 81, 51, 51,  8, 85, 79, 55, 89, 47, 90, 93, 97,  9, 35,\n",
      "        98, 12,  2, 23, 87, 43, 71, 47, 67, 63, 79, 44, 10, 99, 26,  3, 35, 29,\n",
      "        32, 20, 22,  8, 46, 62, 75,  4,  4, 41, 43, 47, 83, 81, 45, 85, 51, 58,\n",
      "        87,  2,  5, 89, 81, 86, 81, 10, 93, 14, 97, 71, 85, 85, 59, 53, 55, 10,\n",
      "        35, 34, 14, 14, 13, 89, 89, 76, 31, 93, 93, 97, 97,  7, 35, 57, 12, 67,\n",
      "        59, 17, 65, 37, 39, 22, 95, 69, 66, 47])\tToken idx: tensor([ 131,  131,  131,  131,    1,  525,  131,    7,    1,  131,    7,    1,\n",
      "           7,  131,    1,    1,  872,    7,  525,    7,   28,  131,  131,    7,\n",
      "         525,  525,    7,  525,  525,  131,  525,  525,  525,    7,    1,  131,\n",
      "         131,    7,  525,  115,  131,  525,  525, 1203,   60,  131,  115,  130,\n",
      "           7,  525,  131,  525,  704,  131,  525,  525,    1,  525,    7,  525,\n",
      "          93,  136,  525,    1,  525,  131,    7,   93,    7,  131,    1,    1,\n",
      "         241,  525,  525,    7,  131,    7,   93,    7,  131,    7,   93,    7,\n",
      "          93,  525,  115,  131,  115,  525,  525,  131,  131,  872,  525,    1,\n",
      "           7,  131,  136,  131])\tRaw indices: indices_to_process=tensor([ 600485, 1000721, 2201429, 3268725, 2701594, 1701528, 1701134,  266831,\n",
      "        2835006, 2635018, 1834422, 2968418, 1567598, 3001901, 3101830, 3235242,\n",
      "         301049, 1167362, 3269119,  400243,   66734,  767250, 2901842, 1434186,\n",
      "        2368588, 1568116, 2234658, 2101764, 2635412, 1467663,  334055, 3302472,\n",
      "         867703,  100066, 1167356,  967368, 1067427,  667067,  734291,  266939,\n",
      "        1534369, 2068411, 2502000,  134615,  133472, 1367604, 1434294, 1567721,\n",
      "        2768306, 2702118, 1501016, 2835530, 1701707, 1934605, 2902236,   67231,\n",
      "         166766, 2968942, 2701600, 2868883, 2701686,  333666, 3102354,  466943,\n",
      "        3235766, 2368194, 2835012, 2835098, 1967834, 1767840, 1834416,  333531,\n",
      "        1167596, 1134527,  467467,  466949,  433720, 2968424, 2968510, 2534835,\n",
      "        1034074, 3101836, 3101922, 3235248, 3235334,  233996, 1167470, 1901252,\n",
      "         400351, 2235176, 1968352,  567132, 2168076, 1234933, 1301292,  733767,\n",
      "        3168542, 2301488, 2201434, 1567722])\n",
      "Beam idx: tensor([82, 82, 41, 97, 45, 58, 88, 92, 18, 47, 35, 87, 67, 12, 47,  7, 91,  7,\n",
      "         2, 55, 59, 31, 14, 35, 29, 19, 55, 95, 22, 33, 37, 35, 75,  4, 35, 55,\n",
      "        55, 31,  6, 86, 27, 66,  3, 97, 26, 51, 47, 95,  5, 53, 98, 11, 12, 39,\n",
      "        41, 97, 97, 47, 63, 45, 87, 61, 83, 46, 65, 63, 63, 67,  5, 95, 17, 47,\n",
      "        95, 59, 34, 11, 96, 11, 94, 47, 43, 99,  8,  0, 46, 87, 49, 50, 10, 67,\n",
      "        23, 38, 14, 26, 93, 67,  4, 34, 10, 55])\tToken idx: tensor([ 131,  525,  131,    1,  131,  525,  131,  131,  525,    7,    7,  525,\n",
      "         525,    7,  525,   28,  525,  525,  525,  131,  525,  131,    1,    1,\n",
      "         131,  131,  525,  131,    1,  131,  131,  241,  131,    7,  525,  169,\n",
      "         872,  525,  525,  525,  131,  131,  131,  525,    7,  131,  689,  525,\n",
      "         131,  131,  525,  689,  115,  131,  525,    7,   93,  130,    7,  525,\n",
      "           7,  131,  525,  515,  131,  525,    1,    1,  525,  704,  131,    1,\n",
      "        1231,    1,    7,  525,    7,    7,  525,  241,  131,    7,    7,    1,\n",
      "        1383,    1,  872,  525,    7,    7,  131,    7,    7,    1,  131,  130,\n",
      "         115,    1,    1,   95])\tRaw indices: indices_to_process=tensor([2735077, 2735471, 1367604, 3235242, 1501016, 1934999, 2935195, 3068607,\n",
      "         600879, 1567598, 1167362, 2902236, 2235176,  400243, 1568116,  233499,\n",
      "        3035648,  233996,   67231, 1834546, 1968352, 1034074,  466943, 1167356,\n",
      "         967368,  633838, 1834940, 3168666,  733767, 1100780, 1234192, 1167596,\n",
      "        2501606,  133419, 1167880, 1834584, 1835287, 1034468,  200643, 2868883,\n",
      "         900662, 2201429,  100190, 3235766,  867185, 1701134, 1568280, 3169060,\n",
      "         166896, 1767840, 3269119,  367572,  400351, 1300898, 1367998, 3235248,\n",
      "        3235334, 1567721, 2101246, 1501410, 2901718, 2034664, 2768824, 1534753,\n",
      "        2168076, 2101764, 2101240, 2234652,  167290, 3169239,  567132, 1567592,\n",
      "        3169766, 1967828, 1134009,  367408, 3201895,  366890, 3135707, 1567832,\n",
      "        1434310, 3301954,  266831,       1, 1535621, 2901712, 1635169, 1668175,\n",
      "         333537, 2234658,  767250, 1267421,  466949,  867179, 3101960, 2234781,\n",
      "         133527, 1134003,  333531, 1834510])\n",
      "Beam idx: tensor([18, 62, 70, 62, 74, 98, 12, 47, 77,  6, 81, 43, 33,  0, 85, 89, 90, 93,\n",
      "        94, 23, 97, 79,  2,  7, 12, 39, 59, 51, 39,  5, 42, 10, 95, 19,  2, 21,\n",
      "         8,  5, 14, 10, 31,  4, 11,  4, 75,  1, 67, 55, 10, 25, 91, 79, 38, 63,\n",
      "        63, 14, 31, 18, 77, 51, 26, 50, 42, 81, 99, 83, 63, 72, 27, 95, 95, 13,\n",
      "        77, 77, 15, 85, 81, 81,  9, 89, 93, 10,  8, 33, 99, 60, 49, 85, 97, 85,\n",
      "        59, 83, 53, 89, 89,  4, 16, 16, 93, 93])\tToken idx: tensor([ 131,  131,  131,  525,  131,  525,    7,  525,    1,  525,    1,  525,\n",
      "         131,  131,    1,    1,  131,    1,  131,  131,    1,  525,  525,  131,\n",
      "         115,  689,    7,    7,  525,    1,  131,  689,  525,  525,  131,  131,\n",
      "           7,    7,    7,    7,  525, 1203,  131,   60,  525,    1,    1,  525,\n",
      "           1,  131,  525,   28,  131,    7,  525,    1,  689,  525,  525,  525,\n",
      "         131,  131,  525,  525,  525,  131,  689,    7,  131, 2569,  375,  131,\n",
      "           7,   93,  525,  525,    7,   93,    7,  525,  525,  525,  115,  525,\n",
      "           1,  131,  131,    7,  525,   93,    1,  525,  131,    7,   93,  241,\n",
      "           1,    7,    7,   93])\tRaw indices: indices_to_process=tensor([ 600485, 2068017, 2334841, 2068411, 2468253, 3269119,  400243, 1568116,\n",
      "        2568182,  200643, 2701594, 1434704, 1100780,     131, 2835006, 2968418,\n",
      "        3001901, 3101830, 3135313,  767250, 3235242, 2635412,   67231,  233602,\n",
      "         400351, 1301456, 1967834, 1701010, 1301292,  166766, 1400957,  334219,\n",
      "        3169060,  634232,   66837,  700544,  266831,  166772,  466949,  333537,\n",
      "        1034468,  134615,  367014,  133472, 2502000,   33354, 2234652, 1834940,\n",
      "         333531,  833956, 3035648, 2634915, 1267545, 2101246, 2101764,  466943,\n",
      "        1034632,  600879, 2568706, 1701528,  867309, 1667781, 1401351, 2702118,\n",
      "        3302472, 2768430, 2101928, 2401423,  900662, 3171104, 3168910,  433720,\n",
      "        2568188, 2568274,  500820, 2835530, 2701600, 2701686,  300184, 2968942,\n",
      "        3102354,  334055,  266939, 1101174, 3301948, 2001311, 1634428, 2835012,\n",
      "        3235766, 2835098, 1967828, 2768824, 1767840, 2968424, 2968510,  133653,\n",
      "         533649,  533655, 3101836, 3101922])\n",
      "Beam idx: tensor([55, 67, 59, 87, 91, 99, 87, 99, 46, 72, 76, 80, 45, 36, 84, 75, 88, 49,\n",
      "        92, 39, 46, 53, 96, 57, 61, 62, 65, 69, 23,  4, 55, 73, 74,  7, 77, 78,\n",
      "        81, 66, 85, 67, 89,  3, 31, 31, 93, 72,  4, 23, 47, 76, 97, 80, 31,  7,\n",
      "         7, 51, 84, 10,  3, 71, 88, 71,  2,  5, 92, 79, 45, 55, 96, 91, 14, 25,\n",
      "        72, 14,  4, 72, 45, 45,  1, 29, 49, 76, 95, 80, 53, 76, 67, 57, 80,  3,\n",
      "        10, 51, 59, 84, 49, 49, 84, 88, 53, 53])\tToken idx: tensor([131, 131, 525, 131, 131, 131, 525, 525, 131,   7,   7,   7,   1, 131,\n",
      "          7, 525,   7,   1,   7, 525, 525,   1,   7,   1,   1, 131,   1,   1,\n",
      "          1,   1, 525,   1, 131, 689,   1, 131,   1, 525,   1, 525,   1, 525,\n",
      "          7, 525,   1, 115,   7, 525, 525, 115,   1, 115, 689, 525,   7, 131,\n",
      "        115, 525,   7, 525, 115, 131,   1, 525, 115,   7, 525, 136, 115, 525,\n",
      "          7, 131, 159,   1, 115, 261,   7,  93, 131, 131, 525, 159, 525, 159,\n",
      "        525, 261, 136, 525, 261,   1, 689, 525, 130, 159,   7,  93, 261, 159,\n",
      "          7,  93])\tRaw indices: indices_to_process=tensor([1834546, 2234782, 1968352, 2901842, 3035254, 3302078, 2902236, 3302472,\n",
      "        1534369, 2401423, 2534835, 2668247, 1500886, 1200839, 2801659, 2502000,\n",
      "        2935071, 1634298, 3068483, 1301292, 1534763, 1767710, 3201895, 1901122,\n",
      "        2034534, 2068017, 2167946, 2301358,  767120,  133413, 1834940, 2434770,\n",
      "        2468253,  234160, 2568182, 2601665, 2701594, 2201823, 2835006, 2235176,\n",
      "        2968418,  100584, 1033950, 1034468, 3101830, 2401531,  133419,  767644,\n",
      "        1568116, 2534943, 3235242, 2668355, 1034632,  233996,  233478, 1701134,\n",
      "        2801767,  334055,  100066, 2368588, 2935179, 2368194,   66707,  167290,\n",
      "        3068591, 2634894, 1501410, 1834551, 3202003, 3035648,  466949,  833956,\n",
      "        2401575,  466943,  133527, 2401677, 1500892, 1500978,   33484,  967368,\n",
      "        1634822, 2534987, 3169060, 2668399, 1768234, 2535089, 2234787, 1901646,\n",
      "        2668501,  100060,  334219, 1701528, 1967957, 2801811, 1634304, 1634390,\n",
      "        2801913, 2935223, 1767716, 1767802])\n",
      "DEB: t.shape=torch.Size([4, 400])\n",
      "tensor=(\n",
      "\ttensor([ 118,    7,  169,  447,  131,  235,  235,  159,    1,  115, 1604,    7,\n",
      "         236,  447,   60,    7, 3694,  159,  447,  235,   50,   60,  235,  283,\n",
      "         159,  236, 3414,    1,  115, 1497,  369, 1169,  305, 2416,  159,  579,\n",
      "         115,  118,  322,  159,  159,  369,  369,  159,  159,    7,  283,  236,\n",
      "          93,  131,  159,    1,  235,   50,  235,   50,    1,    7,  131,   91,\n",
      "         131,  369,   60,  159,    7,  159,    1,    1,  447,  115, 1604,  369,\n",
      "         159,   50,  159,  159,  510,    7,    7,  159, 1169,   60,   60,    7,\n",
      "           7,   93,  159,    7,  235, 3414,  447,  115,    1,    7,    1,  322,\n",
      "         159, 2416,  169,  236,  343,  343,    1, 1496,  525,    1,    1,  471,\n",
      "         447,    1,  689,    1,  518,  471,    1,    1,  528,    1,  525,  118,\n",
      "         136,   93,  119,  689,   60,  205,  118,  431,  528,  169,  115,  689,\n",
      "         130, 1203,  689,  118,  118,   93, 1023,   29,  998,  431,  343, 1496,\n",
      "          29, 1203,    1,  431,  431,  119,   93, 1722,  471,  371,    1, 1496,\n",
      "        1496,    1, 1346,  525,    1, 1203,   93,  130,  447, 1346, 1346,  518,\n",
      "         431,  431,  426,    1,  431,  136,  159, 1722,  704, 1722,  119,    1,\n",
      "           1,  136,  136, 1346,  130,    1,  115,  471,    1,  518,  525,    1,\n",
      "         119,   29, 1023,  518, 1203,  159,    1,  118,  136,  136,  525,  136,\n",
      "          95,  130,  525,    7,  525,    7,  525,   60,   53,  525,  525,  431,\n",
      "          28,  525,  525,  237, 1491,  426, 1023,  426,  525,   93,    7,  525,\n",
      "          93,  525,  130,   93,  205,  136, 1023,  435,  130,  525,  525,   93,\n",
      "         525,  131, 1491,  131,    1,   93,  435,  115,   93,    7,   93,  426,\n",
      "         525,  131,  131,  525,  525,  136,  525,  525,  241,  525,  130,  525,\n",
      "         131,  525,  131,   99,  525,  205,  205,  130,  525,  525,  525,  525,\n",
      "         525,  525,  525,  431,  525,   93,  130,   53,  131,  205,  130,  525,\n",
      "        1491,  525,    7,  525,    1,  431,  431,  131,  525,  525,  525,  525,\n",
      "         525,  115,  525,  130,  525,  689,  130,  689,  525,  525,  525,  525,\n",
      "         261,  525,  131,  525,  525,    7,  525,  136,  525,  882,  525,    8,\n",
      "         426,  525,  130,  525,  525,  471,  525,  525,  525,  525,    1,  525,\n",
      "         525,  525,  525,  115,  525,  525,  525,  525,  525,  525,  471,  525,\n",
      "         525,  525,  525,  525,  525,  525,  525,  689,  131,   29,  525,  130,\n",
      "         525,  130,  525,  525,  525,  131,  261,  525,  525,  525,  525,  525,\n",
      "         525,  525,  471,  525,  261,  261,  525,  525,    7,  525,  130,  525,\n",
      "         882,  525,  115,    8,  525,  525,   29,  689,  525,  131,    7,    7,\n",
      "         131,  525,  882,  882])\n",
      "\ttensor([ 131,  169,  130,  525,  525,  525,  525,  130,    1,  525,  525,  169,\n",
      "         525,  525,  525,    1,  525,   93,  525,  525,  525,  525,  525,  525,\n",
      "         130,  525,  525,    1,  525,  525,  525,  525,  525,  525,   93,  130,\n",
      "          93,  130,  525,  130,    1,  525,  525,  136,  136,  130,  525,  525,\n",
      "          93,  525,  130,    1,  525,  525,  525,  525,    1,  169,  525,    1,\n",
      "         525,  525,  525,  131,    1,  130,    1,    1,  525,  115,  525,  525,\n",
      "          93,  525,  131,  131,  525,  169,  169,  130,  525,  525,  525,    1,\n",
      "           1,   93,   93, 2964,  525,  525,  525,  136,    1,  704,    1,  525,\n",
      "         130,  525,  130,  525,  525,  525,   29,  525,  525,  371,    1,  525,\n",
      "         525,  115,  130,    1,  525,  525,  115,  118,  525,  118,   93,  525,\n",
      "         159,    7,  525,  130,  525,    7,  525,  525,  525,  525,  130,  130,\n",
      "           1,  525,  130,  525,  525,    7,    1,  115,  525,  525,  525,  525,\n",
      "         115,  525,  115,  525,  525,  525,  131,  130,  525,  525,   29,  525,\n",
      "         525,  115,  525,  525,    1,  525,    1,    1,  525,  525,  525,  525,\n",
      "         525,  525,  115,  115,  525,  159,  130,  130,  525,  130,  525,  115,\n",
      "          78,  261,  261,  525,    1,    1,    1,  525, 1165,  525,    7,   91,\n",
      "         525,  115,    1,  525,  525,  130, 1165,  525,  131,  131, 1497,  131,\n",
      "         525,   93,  169,  525,  579,  525,  169,    7,  525,   91,  130,  525,\n",
      "           7,  510, 1893,  525,  525,  525,    1,  525,  169,    7,  525,  689,\n",
      "           7,  704,    1,  115,  525,  130,    1,  525,    1,  704,  115,  115,\n",
      "         518,    1,  525,    1,    7,    7,  525,  525,  115,    1,  115,  525,\n",
      "        1203,    1,    1,  115,  518,  131,  579,  689,  525,   91,    1,  169,\n",
      "           1,  426,    1,  525,  426,  525,  525,  136,  579,  579, 1123,  130,\n",
      "         169,  169,  130,  525,  510,  115,    1,  525,    1,  525,  130,  130,\n",
      "         525,  130,  525,  426,  525,  525,  525,    1, 2059, 2059,  510,  510,\n",
      "          91,  525,    1,    1,   91,  525,    1,  525,  510,  874,   78, 1893,\n",
      "         525,  131,    1,    1,  369,    1,  872,  130,  510,  525,  704,  525,\n",
      "         525,  169,   93,  286,  148,  525,   91, 1123,  124,   60,    7,  371,\n",
      "           7,  206, 2059,  525, 1385,  518,   91,   91,  579,  874,  525,  148,\n",
      "          91,   78,  579, 1893,   91,   60,   60,  525,    1,  525,  518,    1,\n",
      "         369,    1,   87,   91,  872,  131,  525,   91,  704,   91, 1893,  579,\n",
      "         874, 1893,  525,  874,  525,  525,  371,   78,    1,   78,  525, 1893,\n",
      "         525,   78,  525,  525, 1893,  518,  525,  525,    1,    1,    1,    1,\n",
      "           1,  369,  525,  525])\n",
      "\ttensor([ 525,    7,  143,  261,  525,  115,  115,    7,   93,    7,    1,  115,\n",
      "           7,  159,  525,    7,  525,    7,  261,    7,    7,    1,  261,  131,\n",
      "         159,    7,    1,  159,    7,  525,    1,  115,  525,  525,    7,  525,\n",
      "         525,    7,    7,    7,    7,  525,   93,  525,  525,  525,  131,    7,\n",
      "           1,  525,  115,   93,  115,  261,  261,    7,    7,  115,  525,    1,\n",
      "         525,    1,  525,  525,    7,  159,   93,   93,  115,    7,    1,    1,\n",
      "           7,  115,  525,  525,  525,  115,  115,  261,    7,  525,  525,    7,\n",
      "           7,  525,    7,    7,    7,    1,  115,  525,  261,    7,  115,    7,\n",
      "          93,    1,  143,    7,    1,    1,  131,  115,    1,  525,   93,  261,\n",
      "           7,  525,    1,    7,  525,    7,  525,    7,  115,    7,  525,  525,\n",
      "           1,    7,    7,    1,    1,  525,  525,   93,    7,  872,    7,    1,\n",
      "         159,    1,    1,  525,  525,    7,  525,  115,    1,  159,  525,  115,\n",
      "           7,  525,  525,   93,    7,  115,  525,  525,    7,  525,  131,  115,\n",
      "         115,  525,  130,    1,    7,    7,  115,  115,  115,  130,  130,  525,\n",
      "           7,   93,  525,  525,   93,    1,    1,  525,    1,  525,  159,  525,\n",
      "           7,    1,  525,  525,  115,    7,    1,  115,  525,  525,    7,    1,\n",
      "           7,    7,    1,  525,    1,    1,  525,  525,    7,  115,    7,  115,\n",
      "           1,    1,    7,  525,  525,    1,  159,    7,  525,  525,    1,    1,\n",
      "         115,    1,  115,  525,    1,    1,  525,  525,    7,  131,   93,    7,\n",
      "         131,  525,  115,  525,  130, 1732,  525,    7,    7,  525,  131,  525,\n",
      "         525,  525,  525,  525,  115,  525,  115,  525,  525,  525,    1,    1,\n",
      "         525,    1,    1,  131,  525,    7,  525,    7,  525,  130,  115,  159,\n",
      "         130,  115,    1,    1,    7,  130,  130,  525,  525,  525,  525,    1,\n",
      "         159,  159,  525,    1,    1,  525,    7,  525,  130,    1,    1,    1,\n",
      "           1,    1,   93,  115,    7,    1,    1,  525,    1,    1,    1,    1,\n",
      "          93,  159,   60,   93,    1,  136,   93,  136,    7,    7,  115,    1,\n",
      "         525,  525,  115, 1203,    1,  525,    1,    7,    7,  525,  525,  525,\n",
      "           1,    7,    7,  525,  525,  525,   93,    1,  525,  525,    7,  130,\n",
      "         131,  525,  136,  159,    7,  525,  115,  115,  159,    7,  525,  525,\n",
      "         261,  115,  261,    1,  115,  525,  525,    7,  115,  525,  525,  525,\n",
      "           1,  525,  525,    7,    1,  525,  525,   93,  525,    1,  525,    7,\n",
      "           7,  525,  525,    7,  525,  525,  525,    7,  525,  115,    1,    1,\n",
      "         525,  115,  159,  525,    1,  525,  525,    7,   60,  115,  525,  525,\n",
      "         115,    1,  525,  525])\n",
      "\ttensor([ 131,  131,  131,  131,    1,  525,  131,    7,    1,  131,    7,    1,\n",
      "           7,  131,    1,    1,  872,    7,  525,    7,   28,  131,  131,    7,\n",
      "         525,  525,    7,  525,  525,  131,  525,  525,  525,    7,    1,  131,\n",
      "         131,    7,  525,  115,  131,  525,  525, 1203,   60,  131,  115,  130,\n",
      "           7,  525,  131,  525,  704,  131,  525,  525,    1,  525,    7,  525,\n",
      "          93,  136,  525,    1,  525,  131,    7,   93,    7,  131,    1,    1,\n",
      "         241,  525,  525,    7,  131,    7,   93,    7,  131,    7,   93,    7,\n",
      "          93,  525,  115,  131,  115,  525,  525,  131,  131,  872,  525,    1,\n",
      "           7,  131,  136,  131,  131,  525,  131,    1,  131,  525,  131,  131,\n",
      "         525,    7,    7,  525,  525,    7,  525,   28,  525,  525,  525,  131,\n",
      "         525,  131,    1,    1,  131,  131,  525,  131,    1,  131,  131,  241,\n",
      "         131,    7,  525,  169,  872,  525,  525,  525,  131,  131,  131,  525,\n",
      "           7,  131,  689,  525,  131,  131,  525,  689,  115,  131,  525,    7,\n",
      "          93,  130,    7,  525,    7,  131,  525,  515,  131,  525,    1,    1,\n",
      "         525,  704,  131,    1, 1231,    1,    7,  525,    7,    7,  525,  241,\n",
      "         131,    7,    7,    1, 1383,    1,  872,  525,    7,    7,  131,    7,\n",
      "           7,    1,  131,  130,  115,    1,    1,   95,  131,  131,  131,  525,\n",
      "         131,  525,    7,  525,    1,  525,    1,  525,  131,  131,    1,    1,\n",
      "         131,    1,  131,  131,    1,  525,  525,  131,  115,  689,    7,    7,\n",
      "         525,    1,  131,  689,  525,  525,  131,  131,    7,    7,    7,    7,\n",
      "         525, 1203,  131,   60,  525,    1,    1,  525,    1,  131,  525,   28,\n",
      "         131,    7,  525,    1,  689,  525,  525,  525,  131,  131,  525,  525,\n",
      "         525,  131,  689,    7,  131, 2569,  375,  131,    7,   93,  525,  525,\n",
      "           7,   93,    7,  525,  525,  525,  115,  525,    1,  131,  131,    7,\n",
      "         525,   93,    1,  525,  131,    7,   93,  241,    1,    7,    7,   93,\n",
      "         131,  131,  525,  131,  131,  131,  525,  525,  131,    7,    7,    7,\n",
      "           1,  131,    7,  525,    7,    1,    7,  525,  525,    1,    7,    1,\n",
      "           1,  131,    1,    1,    1,    1,  525,    1,  131,  689,    1,  131,\n",
      "           1,  525,    1,  525,    1,  525,    7,  525,    1,  115,    7,  525,\n",
      "         525,  115,    1,  115,  689,  525,    7,  131,  115,  525,    7,  525,\n",
      "         115,  131,    1,  525,  115,    7,  525,  136,  115,  525,    7,  131,\n",
      "         159,    1,  115,  261,    7,   93,  131,  131,  525,  159,  525,  159,\n",
      "         525,  261,  136,  525,  261,    1,  689,  525,  130,  159,    7,   93,\n",
      "         261,  159,    7,   93])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([18, 44, 32, 29, 79, 30, 41, 80, 68, 84, 25, 21, 26, 15, 42, 11, 82, 13,\n",
      "        11, 48, 24,  6, 52, 17, 11,  9, 36, 62, 70, 40, 64, 60, 64, 33, 78, 79,\n",
      "        94, 54,  6, 38, 42, 11, 96,  3,  6,  1,  1, 68, 74,  7, 15, 38, 92, 92,\n",
      "         6, 70, 30, 19, 78,  6, 90,  4, 64, 38, 38, 56,  0, 14, 76,  1, 11, 73,\n",
      "        38, 94, 81, 54, 15,  0, 90, 19, 93, 38, 70, 82, 74, 11, 92, 25, 21, 28,\n",
      "         6, 33, 54,  6, 16, 16,  0, 78, 72,  6])\tToken idx: tensor([ 131,  131,  131,  131,  525,  131,  131,  131,    7,  131,  131,  131,\n",
      "         131,  525,  525,  525,    1,  131,    1,  131,  131,  525,  131,  131,\n",
      "           7,  131,  131,  131,    1,  131, 1203,  131,   60,    7,    1,  131,\n",
      "         525,  525,  689,    1,    1,  241,    7,  131,    1,  136,  525,  115,\n",
      "           1,  131,    1,  525, 1203,   60,  169,    7,  136,    1,    7,  426,\n",
      "         131,  131,  241,   28,    7,    7,    1,  131,    7,    1,  130,    1,\n",
      "          93,    1,  131,   60, 2034,   93,   28,    7,  131, 1722,  115,  525,\n",
      "           7,   93,  241,  136,  136,    1,  130,    1, 1203,    7,    1,    7,\n",
      "          29,  115, 2443,   93])\tRaw indices: indices_to_process=tensor([ 600485, 1467663, 1067427,  967368, 2635412, 1000721, 1367604, 2668371,\n",
      "        2268011, 2801783,  833956,  700544,  867309,  500820, 1401351,  367408,\n",
      "        2734947,  433720,  366884, 1601075,  800603,  200643, 1734487,  567132,\n",
      "         366890,  300308, 1200839, 2068017, 2334711, 1334251, 2135795, 2001311,\n",
      "        2134652, 1100656, 2601535, 2635018, 3135707, 1801587,  200807, 1267415,\n",
      "        1400827,  367124, 3201895,  100190,  200119,   33489,   33878, 2268119,\n",
      "        2468123,  233602,  500296, 1267939, 3069679, 3068536,  200287, 2334717,\n",
      "        1000726,  633708, 2601541,  200544, 3001901,  133543, 2134833, 1267442,\n",
      "        1267421, 1867775,       1,  467073, 2534835,   33354,  367013, 2434770,\n",
      "        1267507, 3135183, 2701724, 1801122,  502329,      93, 3001798,  633714,\n",
      "        3101960, 1269136, 2334825, 2735471, 2468129,  366976, 3068717,  833961,\n",
      "         700549,  933885,  200248, 1100650, 1802265,  200125,  533649,  533655,\n",
      "             29, 2601649, 2403859,  200211])\n",
      "Beam idx: tensor([58, 19,  3, 51,  2, 39, 18, 13, 42, 82, 13, 52,  9, 17, 63, 20,  6, 44,\n",
      "        16, 69, 48, 24, 24, 32, 72, 28, 71, 31, 54, 62, 16, 54, 90, 54, 98, 24,\n",
      "         1, 56, 20, 82, 16, 57, 84, 16, 36, 92, 78,  1, 90,  7,  9, 28, 30, 16,\n",
      "        13, 16, 88, 78, 54, 16,  9, 84, 58, 16, 31, 21, 16, 55, 36, 77, 70, 62,\n",
      "        20, 71, 28, 54, 77, 20, 74, 67, 40, 47, 92, 90, 89, 88, 64,  2, 54, 24,\n",
      "        69, 62, 16, 13,  9, 78, 15, 84, 58, 56])\tToken idx: tensor([ 131,  131,  131,  131,  131,  131,  525,    1,    1,  525,    7,  131,\n",
      "           1,  131,  131,    1,  131,  131,  131,   28,  131,    1,    7,  131,\n",
      "         131,    7,  241,  525,    1,  525,   93,  525,    7,    7,  131,  115,\n",
      "         131,    7,    7,  131,  169,    1,    1,    7,  515,    7,    1,  136,\n",
      "           1,  525,    7,    1,  131,    1,  115,  130,  525,    7,   93,  525,\n",
      "          93,    7,  136,  261,  131,    1,   29,  131, 1383,   60,    1,   60,\n",
      "          93,    7,  115,  130, 1203,  115,    7,  131,    1,  131,  115,  115,\n",
      "          60, 1203,    1,  136,  115,   93,  261, 1203,  159,   93,   28,  525,\n",
      "           1,  115,  525,  115])\tRaw indices: indices_to_process=tensor([1934605,  633838,  100190, 1701134,   66837, 1300898,  600879,  433590,\n",
      "        1400827, 2735471,  433596, 1734487,  300178,  567132, 2101370,  667061,\n",
      "         200249, 1467663,  533779, 2301385, 1601075,  800473,  800479, 1067427,\n",
      "        2401547,  933891, 2368304, 1034468, 1801063, 2068411,  533741, 1801587,\n",
      "        3001777, 1801069, 3268725,  800587,   33484, 1867775,  667067, 2735077,\n",
      "         533817, 1901122, 2801653,  533655, 1201223, 3068483, 2601535,   33489,\n",
      "        3001771,  233996,  300184,  933885, 1000721,  533649,  433704,  533778,\n",
      "        2935589, 2601541, 1801155,  534173,  300270, 2801659, 1934610,  533909,\n",
      "        1034074,  700414,  533677, 1834546, 1202091, 2568241, 2334711, 2067946,\n",
      "         667153, 2368070,  933999, 1801192, 2569384,  667175, 2468129, 2234782,\n",
      "        1334121, 1567722, 3068591, 3001885, 2968477, 2936267, 2134593,   66842,\n",
      "        1801177,  800565, 2301618, 2069089,  533807,  433682,  300205, 2602059,\n",
      "         500296, 2801767, 1934999, 1867883])\n",
      "Beam idx: tensor([23,  7,  0,  2, 47, 50, 62, 38, 11,  3, 57, 25, 26, 98, 87, 76, 65, 71,\n",
      "        33, 62, 80, 71, 71,  3, 13, 22, 68, 52, 32, 40, 78, 96, 84, 69, 56, 95,\n",
      "         3, 71, 58, 99, 15, 68, 82,  3, 70, 87, 28, 22, 54, 71, 42, 84, 87, 66,\n",
      "        71, 86, 95, 90, 94, 24, 46, 22, 86, 70, 40, 76, 37, 87, 55, 48, 18,  3,\n",
      "        42, 52,  3, 99, 37, 66, 41, 18, 31, 58, 17, 28, 45, 57, 56, 90, 54, 68,\n",
      "         3, 77, 43, 71, 71, 94, 22, 65, 30, 86])\tToken idx: tensor([ 131,  131,  131,  131,  131,  131,  525,  131,  131,  525,  131,  131,\n",
      "         131,  525,  525,    7,  131,    7,  131,  131,  131,  525,    1,    7,\n",
      "         131,    1,  261,    7,  131,    7,    1,  131,    7,  131,    7,  131,\n",
      "           1,  689,    1,  131,  131,    7,  525,  130,  131,    1,    7,  525,\n",
      "           1,  130,  525,  115,    7,    7,   93, 1231,  525,  131,  131,    7,\n",
      "         525,  318,  131,  136,  115,  115,   60,  130,  131,    7,  131,  241,\n",
      "         131,  115,   93,  525, 1203,    1,    7,  525,  131,    7,    7,  115,\n",
      "           7,  136,  115,  136,    7,   28,  689,  131,  525,  115,  136,  136,\n",
      "         136,  136,    1,  704])\tRaw indices: indices_to_process=tensor([ 767250,  233602,     131,   66837, 1567722, 1667781, 2068411, 1267545,\n",
      "         367014,  100584, 1901252,  833956,  867309, 3269119, 2902236, 2534835,\n",
      "        2168076, 2368070, 1100780, 2068017, 2668371, 2368588, 2368064,  100066,\n",
      "         433720,  733767, 2268265, 1734363, 1067427, 1334127, 2601535, 3202019,\n",
      "        2801659, 2301488, 1867775, 3168666,  100060, 2368752, 1934475, 3302078,\n",
      "         500426, 2268011, 2735471,  100189, 2334841, 2901712,  933891,  734291,\n",
      "        1801063, 2368193, 1401351, 2801767, 2901718, 2201305, 2368156, 2869589,\n",
      "        3169060, 3001901, 3135313,  800479, 1534763,  734084, 2868489, 2334846,\n",
      "        1334235, 2534943, 1234121, 2901841, 1834546, 1600951,  600485,  100300,\n",
      "        1400957, 1734471,  100152, 3302472, 1235264, 2201299, 1367480,  600879,\n",
      "        1034074, 1934481,  567008,  933999, 1500892, 1901257, 1867883, 3001906,\n",
      "        1801069, 2268032,  100748, 2568312, 1434704, 2368178, 2368199, 3135318,\n",
      "         733902, 2168081, 1000591, 2869062])\n",
      "Beam idx: tensor([36, 87, 78, 75, 74, 46, 63, 66, 41, 35, 44,  9, 99,  1,  8, 86, 12, 24,\n",
      "        71, 63, 46,  8, 16, 40, 56, 54,  8, 69,  8, 34, 99, 83, 42, 70, 42,  8,\n",
      "         8, 87, 83, 59,  8, 21,  8, 24, 38, 63, 50, 48,  8, 48, 56, 23, 83, 63,\n",
      "        15,  7,  0, 32, 62, 38, 13, 58,  8, 17, 79, 34, 39, 43,  3, 68, 34,  8,\n",
      "        35, 10,  8, 50, 17, 45, 63, 90, 41, 28,  6, 57, 83,  9,  8, 35, 21, 42,\n",
      "        48, 49,  6, 62, 75, 40, 53, 63, 22, 42])\tToken idx: tensor([ 131,  525,  131,  525,  131,  525,    7,  131,  131,  525,  131,  131,\n",
      "         525,  131,  131,  515,  872,    7,  131,    1,  131,   93,  131,  131,\n",
      "           7,  525,  169,  131,    7,    1,  131,    7,    7,    1,    1,    1,\n",
      "         130,  131,  525,  525,  525,    7,  261,  115,    7,  241,    1, 1203,\n",
      "          29,   60,  115,  131,    1,  115,    7,  131,  131,    7,  131,    1,\n",
      "           1,  525,  159,    7,  131,  525,  525,  689,  131,    7,    7, 2059,\n",
      "         130,  131,  136,    7,    1,  131,   93,    7,  136,    1,  525,  131,\n",
      "         130,  136,  115,  131,    1,  115,  241,  131,    1,  136,  130, 1891,\n",
      "         131,  525,    7,  525])\tRaw indices: indices_to_process=tensor([1200839, 2902236, 2601665, 2502000, 2468253, 1534763, 2101246, 2201429,\n",
      "        1367604, 1167880, 1467663,  300308, 3302472,   33484,  266955, 2868873,\n",
      "         401108,  800479, 2368194, 2101240, 1534369,  266917,  533779, 1334251,\n",
      "        1867775, 1801587,  266993, 2301488,  266831, 1134003, 3302078, 2768306,\n",
      "        1400833, 2334711, 1400827,  266825,  266954, 2901842, 2768824, 1968352,\n",
      "         267349,  700420,  267085,  800587, 1267421, 2101480, 1667651, 1602147,\n",
      "         266853, 1601004, 1867883,  767250, 2768300, 2101354,  500302,  233602,\n",
      "            131, 1067303, 2068017, 1267415,  433590, 1934999,  266983,  567008,\n",
      "        2635018, 1134527, 1301292, 1434868,  100190, 2268011, 1134009,  268883,\n",
      "        1167485,  333661,  266960, 1667657,  567002, 1501016, 2101332, 3001777,\n",
      "        1367609,  933885,  200643, 1901252, 2768429,  300313,  266939, 1167486,\n",
      "         700414, 1400941, 1601185, 1634428,  200119, 2068022, 2501605, 1336011,\n",
      "        1767840, 2101764,  733773, 1401351])\n",
      "DEB: t.shape=torch.Size([5, 400])\n",
      "tensor=(\n",
      "\ttensor([ 447,  159,  305, 1497,  159,  369,  369, 1169,  447,    7,  236,   60,\n",
      "        3414,    7,  369,    7,   60,  447,    7,   93,  159,  235,  235,  159,\n",
      "           7,  115,  115,   60, 1604,  159,    7,  131,    7, 2416,    7,  159,\n",
      "           1,  235,  235,  322,  369,    7,  159,  447,  235,    7,    7,  447,\n",
      "         159,  159,    7,  322,    1,    1,  235, 1604,  369,  235,    7,  235,\n",
      "         447,  131,    7,  322,  322,    1,  118,   60,  510,    7,    7,   50,\n",
      "         322,    1,   60,  235,    7,  118,  447,  235,    7,  322, 1604,   60,\n",
      "         159,    7,    1,  236,   60,  115,  235, 2416,  235,  235, 3694, 3694,\n",
      "         118,    7,  159,  235, 1346,  118, 1496, 1722,    1,   29,  525,  471,\n",
      "         343,  136,  471,  471,    1,    1,  130,  136,    1,   29,  528,  431,\n",
      "         431,   60,   60,  130,  431,  528,    1,  689,    1,   93,  528,    1,\n",
      "         525,    1,    1,   60,  343, 1496,  136,  136,  528,    1,  130,  528,\n",
      "         118,  119,  119,  343,  525,  471,    1,  528,  115,  528,  471,  528,\n",
      "           1,  119,    1,  528,    1,  130, 1346,  528,  689,   93,  528, 1496,\n",
      "         118, 1722,  426,   93,  136,    1,  528,    1, 1722,  136,  159,  518,\n",
      "         998,  431,  119,  525,  518,    1,  447,    1,    1,   60,  431,   93,\n",
      "         528,  471,    1,  119,    1,  130, 1346, 1496,  426,    7,  136,  525,\n",
      "         115,   93,  130,  525,   60,  136,  136,   93,    7,  525,  525,  525,\n",
      "         525,  130,  136,  130,  525,  130,  130,  136,  525, 1023,  525,  525,\n",
      "         205,  525,  525,  525,  131,  205,  525,  131,  136,  130,  525,  525,\n",
      "         431,  525,  130,  136,  205,  525,   93, 1023,  131,  130, 1491,  131,\n",
      "         525,  131,  130,  130,  131,    7,  431,  525,  435, 1023,  130,  205,\n",
      "         525,  525,  525,  525,  525,   93,  525,  136, 1491,  525,  136,  525,\n",
      "         525,  131,  131,  525,   93,  525,  525,   93,   93,  136,  525,    7,\n",
      "         131,  525,  136,  525,  131,  130,  130,  431, 1023,  525,  130,  130,\n",
      "         525,    8,  525,  525,  471,  471,  525,  261,  525,  525,  525,  525,\n",
      "         882,  115,  525,  115,  261,  426,  525,  525,  471,  525,  525,  525,\n",
      "         131,  525,  525,  525,  525,    1,  882,  525,  525,  525,  525,  525,\n",
      "         525,    8,  525,  130,  525,  882,  525,  426,  525,  525,  525,  525,\n",
      "         525,  525,  131,    8,  525,  525,  525,  689,  525,  525,  525,  525,\n",
      "         525,  525,  525,    7,  525,    1,  115,  525,  130,  525,    1,  525,\n",
      "         525,  525,  525,  525,    7,  525,  525,   29,  525,  525,  130,   29,\n",
      "         525,  525,  525,  525,  882,  525,  525,  525,  130,  525,  525,  525,\n",
      "         525,  525,  525,  525])\n",
      "\ttensor([ 525,  136,  525,  525,  130,  525,  525,  525,  525,    1,  525,  525,\n",
      "         525,    1,  525,  169,  525,  525,  169,   93,  130,  525,  525,   93,\n",
      "         169,  525,   93,  525,  525,    1,    1,  525,    1,  525,  169,  130,\n",
      "           1,  525,  525,  525,  525,  169,  130,  525,  525,  169,  169,  525,\n",
      "         131,  130,    1,  525,    1,    1,  525,  525,  525,  525,  169,  525,\n",
      "         525,  525,    1,  525,  525,    1,  131,  525,  525,  169,  169,  525,\n",
      "         525,    1,  525,  525,    1,  131,  525,  525,  704,  525,  525,  525,\n",
      "         131,  169,    1,  525,  525,  525,  525,  525,  525,  525,  525,  525,\n",
      "         131,  169,   93,  525,  525,  525,  525,  130,   29,  115,   93,  525,\n",
      "         525,  261,  525,  525,  115,  118,    1,  159,    1,  115,  525,  525,\n",
      "         525,  525,  525,    1,  525,  525,  115,  130,   29,    1,  525,   29,\n",
      "           7,   29, 1165,  525,  525,  525,  159,  261,  525,  115,    1,  525,\n",
      "         525,  525,  525,  525,    7,  525,  115,  525,  130,  525,  525,  525,\n",
      "        1165,  525,   29,  525,  115,    1,  525,  525,  130,    7,  525,  525,\n",
      "         525,  130,  115,    1,  159,  115,  525,   29,  130,  159,  130,  525,\n",
      "         525,  525,  525,    7,  525, 1165,  525,   29,   29,  525,  525,    1,\n",
      "         525,  525,  115,  525,  118,    1,  525,  525,  525,  525,  131, 1497,\n",
      "         525,  115,    1,  115,    7,  131,  131,    7,  525,  510,  130,  169,\n",
      "         426,  136,  130,    1,  510,  136,  136,  131,   91,    1,  426, 1203,\n",
      "         525,  518,  130, 2059,    1,  525,  518,    1,  131,  136,  579,  510,\n",
      "         525,  426,    1,  131,  525,  130,    7,    1,    1,  136,  525,    1,\n",
      "         130,    1,  136,  130,    1,  525,  525,  169,  525,    1,  130,  525,\n",
      "         518,  169,  704,  130,  115,  115, 1893,  131,  525, 1203,  131,  510,\n",
      "         704,    1,    1, 1893,  115,  579,  510,    7,    7,  131,  518,  525,\n",
      "           1,  426,  131,  169,    1,  136,  136,  525,    1,  426,    1,  130,\n",
      "           7,  525,  371,  874,  525,  525,   91,  525,  518,  371,  579,  874,\n",
      "         525,  525,  510,  525,  525,  525,  579,   91,  525,  510,  369, 1385,\n",
      "           1,   60,  510,   91,  510,    7,  525, 1893,   91, 1893,   91,  510,\n",
      "         510,  525, 1893,    1,  510,  525,  510,  525, 2059,   91,  579,   91,\n",
      "         510,   91,    1,  525, 1893,   91,    1,  525,   91,  124,   87, 2059,\n",
      "         131,  518,  510,    1,   78,    7,  525,   91,    1,  704,    7,  510,\n",
      "         371,   78,  510,  579,    1,  874,   91,  525,  518,  148,    1,  525,\n",
      "        1893,  874,  510,  371,  525,   91,   91,   78,    1,   87,  874, 1385,\n",
      "          60,   91,  704,   91])\n",
      "\ttensor([ 261,  525,  525,  525,  261,    1,  525,    7,  115,    7,    7,    1,\n",
      "           1,    7,   93,  115,  525,  159,  115,    1,  159,  115,  115,    7,\n",
      "         115,    7,  525,  525,    1,    7,    7,  525,    7,  525,  115,  261,\n",
      "         115,  261,  115,    7,   93,  115,   93,  261,  115,    7,    7,  115,\n",
      "         525,    7,    7,    7,  261,  261,  115,    1,    1,    7,  115,  115,\n",
      "         115,  525,    7,    7,    7,    7,  525,  525,  525,    7,  115,  115,\n",
      "           7,  115,  525,  261,    7,  525,  115,    7,    7,    7,    1,  525,\n",
      "         525,  115,  261,    7,    1,    7,  115,  525,  261,  115,  525,  525,\n",
      "         525,  115,    7,  115,  130,  525,  115,  525,  131,  115,  525,    7,\n",
      "         525,  525,    7,    7,  525,    7,  115,    1,   93,    7,  115,   93,\n",
      "           7,    1,    1,  159,   93,    7,  525,    1,  131,  115,  115,  131,\n",
      "           7,  131,  525,    1,    1,  115,    1,  525,  115,  525,  115,  115,\n",
      "         525,    7,  159,    1,    7,  261,  525,    7,    7,  115,    7,  115,\n",
      "         525,  159,  131,  115,  525,  115,  130,  115,    1,    7,  115,  115,\n",
      "         525,  525,  525,  115,    1,  525,    7,  131,  525,    1,    1,  525,\n",
      "           1,   93,    7,    7,  525,  525,  115,  131,  131,    1,   93,  115,\n",
      "         115,    7,  525,  159,    7,  115,  130,  115,  525,  525,    7,    7,\n",
      "         525,    1,  115,  131,    7,  115,    7,  131,   93,    1,    1,  159,\n",
      "         115,  525, 1732,  115,    1,  525,  525,  115,  525,  525,    7,  525,\n",
      "         130,  525,  525,    1,  130,  130,  525,  525,  115,  525,  525,    1,\n",
      "           1,    7,    7,  115,  130,    1,  131,  525,    1,  525,  525,  130,\n",
      "           1,    1,  525,    1,  525,   93,    1,    7,  115,  525,    1,  130,\n",
      "         525,  159,  525,    1,  131,  525,  115,  115,  525,  525,  115,    1,\n",
      "         525,    1,  525,  115,  525,  525,    1,  131,  525,    7,  525,   93,\n",
      "           1,    7,  115,  159,  525,  525,  525,    1,  525,  115,  115,    1,\n",
      "         131,  525,  525,    7,  525,  525,    7,  525,  525,  130,  159,    7,\n",
      "         525,  159,    7,  159,  525,    1,    7,    7,  525,    7,    1,    7,\n",
      "         115,  525,    7,    1,    7,    7,  525,    1,  115,  525,  115,    7,\n",
      "           7,  525,    1,  525,    7,  525,    7,    1,  136,    7,  261,  261,\n",
      "           7,  261,  115,  525,    1,    7, 1203,  136,   93,  525,  525,  136,\n",
      "         525,  525,    7,  525,    7,    7,  159,  115,   93,  525,    7,    7,\n",
      "         130,  115,    7,  261,  525,    7,    7,  525,  525,  525,   93,  525,\n",
      "           1,    7,    7,  130,  525,  115,  261,  115,   93,  525,    7,    7,\n",
      "         525,    7,  525,  115])\n",
      "\ttensor([ 525,   60,  525,  131,    7,  525,  525,  131,    7,   93,  525,  131,\n",
      "           7,    1,  525,    1,   93,  131,    1,    7,  525,  131,  704,    7,\n",
      "           1,  131,  131,  525,    1,  131,  525,   93,  525,    7,   93,    7,\n",
      "         525,  525,  131,  525,  525,    1,    7,  131,  131,  131,  131,    7,\n",
      "         525,    7,    1,  525,  131,  131,  131,    1,  525,    7,   93,  131,\n",
      "         525,    1,  525,  525,  525,    1,  131,    1,  131,  131,    1,  525,\n",
      "         525,  525,    7,  525,    1,  131,  525,    7,  872,  525,    1,   93,\n",
      "         525,    1,  131,  525,  131,  525,  131,    7,  525,  131,  872,  872,\n",
      "         131,   93,  241,  131,    7,  131,    1,  689,  131,  525,  525,    7,\n",
      "         131,    7,    7,  115,    7,  525,  515,  525,  131,    7,  525,  704,\n",
      "         131,  131,  131,  131, 1231,    1,    1,  241,  525,  525,  525,  525,\n",
      "         131,  525,    1,  131,  525,   93,  525,    7,  525,  130, 1383,  525,\n",
      "         872,    7,  525,  525,  131,  131,    7,    1,  131,  525,    7,  525,\n",
      "           7,  525,  525,  525,    7, 1383,    7,  525,  241,  131,  525,    7,\n",
      "         872,    7,  131,  525,  525,    1,    1,  525,    7,  525,    7,    1,\n",
      "         131,  525,    7,  131,    7,    7,  131,  131,  525,  131,  704,  525,\n",
      "         525,    7,    7,  525,   28, 1383,    7,   93,  131,  525,  131,  131,\n",
      "         525,  525,  525,    7,  525,  525,  525,  689,    7,    7,    7,    7,\n",
      "         131,  131,  525,  525,  525,  131,  131,  525,  131,  525,  131,  131,\n",
      "         525,  525,    7,    1,    1, 2569,  689,  241,  525,  131,  525,   93,\n",
      "           1,  131,  115,  525,  375,    7,  525,  525,  525,  131,  131,    1,\n",
      "           7,  689,  131,  131,  241,    1,   93,  115,    1,  525,  131,  375,\n",
      "         525,    7,    7,    7,    1,    1,  131,  525,  131,  131,  525,   93,\n",
      "           7,  689, 1203,  131,  689,  525,    1,  525,    1,  525,  689,    1,\n",
      "         525,  131,  525,   93,   60,  131,  131,   93,  525,  131,  131,  131,\n",
      "           1,  525,  131,  261,  115,    7,  525,  525,  525,  131,    1,    7,\n",
      "          93,  131,  131,  136,    1,    1,  131,  525,    7,  131,    7,    1,\n",
      "         115,    7,  131,  525,  131,    1,   93,  159,    7,    7,    7,  131,\n",
      "         131,  525,  159,  525,  131,    1,  131,    1,    1,  525,    1,  525,\n",
      "         131,  525,  115,    1,  159,  525,  525,  525,  131,  131,    1,    1,\n",
      "         131,    7,  131,    1,  131,    1,  525,  525,  131,  115,    1,  131,\n",
      "         131,    7,  131,    1,    1,  115,  525,  689,  525,    1,  525,  525,\n",
      "         159,    7,  131,  131,    1,    7,  525,  115,  525,    1,  261,    1,\n",
      "         525,  525,    7,    7])\n",
      "\ttensor([ 131,  131,  131,  131,  525,  131,  131,  131,    7,  131,  131,  131,\n",
      "         131,  525,  525,  525,    1,  131,    1,  131,  131,  525,  131,  131,\n",
      "           7,  131,  131,  131,    1,  131, 1203,  131,   60,    7,    1,  131,\n",
      "         525,  525,  689,    1,    1,  241,    7,  131,    1,  136,  525,  115,\n",
      "           1,  131,    1,  525, 1203,   60,  169,    7,  136,    1,    7,  426,\n",
      "         131,  131,  241,   28,    7,    7,    1,  131,    7,    1,  130,    1,\n",
      "          93,    1,  131,   60, 2034,   93,   28,    7,  131, 1722,  115,  525,\n",
      "           7,   93,  241,  136,  136,    1,  130,    1, 1203,    7,    1,    7,\n",
      "          29,  115, 2443,   93,  131,  131,  131,  131,  131,  131,  525,    1,\n",
      "           1,  525,    7,  131,    1,  131,  131,    1,  131,  131,  131,   28,\n",
      "         131,    1,    7,  131,  131,    7,  241,  525,    1,  525,   93,  525,\n",
      "           7,    7,  131,  115,  131,    7,    7,  131,  169,    1,    1,    7,\n",
      "         515,    7,    1,  136,    1,  525,    7,    1,  131,    1,  115,  130,\n",
      "         525,    7,   93,  525,   93,    7,  136,  261,  131,    1,   29,  131,\n",
      "        1383,   60,    1,   60,   93,    7,  115,  130, 1203,  115,    7,  131,\n",
      "           1,  131,  115,  115,   60, 1203,    1,  136,  115,   93,  261, 1203,\n",
      "         159,   93,   28,  525,    1,  115,  525,  115,  131,  131,  131,  131,\n",
      "         131,  131,  525,  131,  131,  525,  131,  131,  131,  525,  525,    7,\n",
      "         131,    7,  131,  131,  131,  525,    1,    7,  131,    1,  261,    7,\n",
      "         131,    7,    1,  131,    7,  131,    7,  131,    1,  689,    1,  131,\n",
      "         131,    7,  525,  130,  131,    1,    7,  525,    1,  130,  525,  115,\n",
      "           7,    7,   93, 1231,  525,  131,  131,    7,  525,  318,  131,  136,\n",
      "         115,  115,   60,  130,  131,    7,  131,  241,  131,  115,   93,  525,\n",
      "        1203,    1,    7,  525,  131,    7,    7,  115,    7,  136,  115,  136,\n",
      "           7,   28,  689,  131,  525,  115,  136,  136,  136,  136,    1,  704,\n",
      "         131,  525,  131,  525,  131,  525,    7,  131,  131,  525,  131,  131,\n",
      "         525,  131,  131,  515,  872,    7,  131,    1,  131,   93,  131,  131,\n",
      "           7,  525,  169,  131,    7,    1,  131,    7,    7,    1,    1,    1,\n",
      "         130,  131,  525,  525,  525,    7,  261,  115,    7,  241,    1, 1203,\n",
      "          29,   60,  115,  131,    1,  115,    7,  131,  131,    7,  131,    1,\n",
      "           1,  525,  159,    7,  131,  525,  525,  689,  131,    7,    7, 2059,\n",
      "         130,  131,  136,    7,    1,  131,   93,    7,  136,    1,  525,  131,\n",
      "         130,  136,  115,  131,    1,  115,  241,  131,    1,  136,  130, 1891,\n",
      "         131,  525,    7,  525])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([18, 32, 41, 44, 62, 24, 30, 77, 46, 91, 18, 44, 63, 39, 43, 98, 50, 79,\n",
      "        61, 77, 46, 25, 90, 61, 49, 43, 42, 98, 98, 91, 99, 17,  2, 45, 31, 71,\n",
      "        75, 91, 75, 83, 46, 88, 53, 87, 75, 45, 26, 27, 98, 77, 50, 58, 57, 23,\n",
      "        95, 91, 63, 23, 55, 53, 88, 49, 65, 27, 51, 22, 94, 98, 91, 31, 50, 80,\n",
      "        47, 99, 57, 54, 64, 85, 10, 91, 88, 85, 86, 61, 63, 75, 63, 50, 38, 95,\n",
      "        49, 26, 67, 51, 44, 23, 63, 87, 84, 34])\tToken idx: tensor([ 131,  131,  131,  131,  131,  131,  131,    7,  131,  525,  136,  525,\n",
      "          28,  525,    7,    7,    1,  525,    7,    1,  136,  131,  131,    1,\n",
      "           1,  115,  131,  525,  131,    1,  525,  131,  525,  131,  131,   28,\n",
      "        1203,    7,  525,    1,  525,    1,  131,    7,  241,  136,  131, 1383,\n",
      "         115,  115,    7,    7,  131,    1,  241,  130,  261,  131,    1,  136,\n",
      "          93,  136,    7,    1,  525,  525,  131, 1722,  241,  136,  525,    7,\n",
      "         525,  136,  136,  131,  131,   60,    1,   93,  115, 1203,    1,   93,\n",
      "           7,   60,  174,   93,  131, 1203,   93,  136,    1,  131,    7,  136,\n",
      "         525, 1722, 1203,  525])\tRaw indices: indices_to_process=tensor([ 600485, 1067427, 1367604, 1467663, 2068017,  800603, 1000721, 2568188,\n",
      "        1534369, 3035648,  600490, 1468057, 2101267, 1301292, 1434186, 3268601,\n",
      "        1667651, 2635412, 2034540, 2568182, 1534374,  833956, 3001901, 2034534,\n",
      "        1634298, 1434294, 1400957, 3269119, 3268725, 3035124, 3302472,  567132,\n",
      "          67231, 1501016, 1034074, 2368091, 2502678, 3035130, 2502000, 2768300,\n",
      "        1534763, 2935065, 1767840, 2901718, 2501716, 1501021,  867309,  901914,\n",
      "        3268709, 2568296, 1667657, 1934481, 1901252,  767120, 3168776, 3035253,\n",
      "        2101500,  767250, 1834416, 1767845, 2935157, 1634433, 2167952,  900532,\n",
      "        1701528,  734291, 3135313, 3270316, 3035364, 1034079, 1668175, 2668247,\n",
      "        1568116, 3302083, 1901257, 1801193, 2134723, 2835065,  333531, 3035216,\n",
      "        2935179, 2836208, 2868359, 2034626, 2101246, 2501535, 2101413, 1667743,\n",
      "        1267545, 3169738, 1634390,  867314, 2234652, 1701134, 1467539,  767255,\n",
      "        2101764, 2903433, 2802855, 1134527])\n",
      "Beam idx: tensor([39, 17, 18, 73, 11, 81, 57, 73, 79, 81, 91,  1, 25, 75, 47, 25, 37, 66,\n",
      "        75, 76,  5, 80, 79, 85, 37, 76, 80, 66, 87, 95, 45, 53,  8, 79, 35,  0,\n",
      "        75, 76,  3, 31, 45, 80, 61, 86, 53, 65,  5, 73, 85, 29, 76, 10, 91, 80,\n",
      "        43, 94, 81, 65, 23, 61, 18, 16, 60, 76, 80, 79, 82, 25, 86, 66, 68, 63,\n",
      "        62, 79, 88, 76, 60, 80, 54, 95, 76, 59, 96, 80, 73, 57, 22, 22,  5, 41,\n",
      "        68, 18, 34, 95, 76, 80, 81, 85, 31, 96])\tToken idx: tensor([ 131,  131,  131,    1,  131,    1,  131,    7,    1,    7,  525,  131,\n",
      "           1,   28,  131,    7,  131,  131,  131,    1,  525,    1,    7,    1,\n",
      "         136,   93,   93,   28,  525, 1203,  525,  131,  131,  241,    1,  131,\n",
      "         261,   29,  131,  525,  131,   29,  131,  525,  136,  131,  131,  115,\n",
      "        2034,    1,    7,  131,  136,    7,  131,   28,  115,  136,  131,  136,\n",
      "         136,  131,    1,  159,  159,  525,  525,  115,  136,  261,    1,  525,\n",
      "         131,   93,  525,  689,   93,  689,  131,  525,  169,    1,    7,  169,\n",
      "          93,  525,  525,    1,  136,    7,   93,  525,    1,  241,  136,  136,\n",
      "          93, 2443,  131,    1])\tRaw indices: indices_to_process=tensor([1300898,  567132,  600485, 2434770,  367014, 2701594, 1901252, 2434776,\n",
      "        2634888, 2701600, 3035648,   33484,  833826, 2501503, 1567722,  833832,\n",
      "        1234192, 2201429, 2501606, 2534829,  167290, 2668241, 2634894, 2835006,\n",
      "        1234197, 2534921, 2668333, 2201326, 2902236, 3169738, 1501410, 1767840,\n",
      "         266955, 2635128, 1167356,     131, 2501736, 2534857,  100190, 1034468,\n",
      "        1501016, 2668269, 2034664, 2868883, 1767845, 2168076,  166896, 2434884,\n",
      "        2837039,  967238, 2534835,  333661, 3035259, 2668247, 1434310, 3135210,\n",
      "        2701708, 2168081,  767250, 2034669,  600490,  533779, 2001181, 2534987,\n",
      "        2668399, 2635412, 2735471,  833940, 2868494, 2201559, 2268005, 2101764,\n",
      "        2068017, 2634980, 2935589, 2535517, 2001273, 2668929, 1801193, 3169060,\n",
      "        2534997, 1967828, 3201895, 2668409, 2434862, 1901646,  734291,  733767,\n",
      "         166901, 1367480, 2268097,  600879, 1134003, 3168776, 2534964, 2668376,\n",
      "        2701686, 2837448, 1034074, 3201889])\n",
      "Beam idx: tensor([ 7, 50, 47, 11, 33, 62, 80, 32, 98, 38,  1, 20, 26, 29, 73,  4, 12, 10,\n",
      "        73, 29, 19, 20,  6, 74, 57, 88, 35, 21, 73, 95, 34, 88, 34, 34, 92, 21,\n",
      "        73,  5, 20, 64, 81, 95, 61, 59, 27, 38, 99, 88, 74, 64, 19, 74, 26, 14,\n",
      "         0, 61, 72, 88, 34, 72, 29, 35, 64,  1, 92, 88, 53, 36, 16, 39, 23, 51,\n",
      "        61, 64, 39, 79, 92, 97, 85,  7, 34, 44,  9, 89, 35, 92, 53, 36, 61, 29,\n",
      "        93,  3, 64, 34, 63, 91, 29,  6, 29, 72])\tToken idx: tensor([ 131,  131,  131,  131,  131,  131,  131,  131,  525,  131,  131,    1,\n",
      "         131,    7,   60,  131,  525,  131, 1203,    1,  525,    7,    1,    1,\n",
      "         131,    7,    1,  131,  241,  525,    1,  704,  525,    7,    7,  136,\n",
      "         525,  131,  115,    1,   28,  131,    7,  241,  131,  136,  525,  169,\n",
      "         131,   93,  131,  136,  136,    7,    7,    1,  115,   95,   93,    7,\n",
      "          93, 2034,   29,  136,    1,  115,  131,    1,    1,    7,    7,  525,\n",
      "         525,    7,  689,    1,   93,    1,    1,  136,  130,    1,  131,    1,\n",
      "         525,  159,  136,   93,   93,   29,    1,  131,  159,  115,    1,    1,\n",
      "         115,  525,  689,    1])\tRaw indices: indices_to_process=tensor([ 233602, 1667781, 1567722,  367014, 1100780, 2068017, 2668371, 1067427,\n",
      "        3269119, 1267545,   33484,  667061,  867309,  967244, 2434829,  133543,\n",
      "         400761,  333661, 2435972,  967238,  634232,  667067,  200119, 2468123,\n",
      "        1901252, 2935071, 1167356,  700544, 2435010, 3169060, 1134003, 2935768,\n",
      "        1134527, 1134009, 3068483,  700549, 2435294,  166896,  667175, 2134593,\n",
      "        2701621, 3168666, 2034540, 1968068,  900662, 1267550, 3302472, 2935233,\n",
      "        2468253, 2134685,  633838, 2468258,  867314,  466949,       7, 2034534,\n",
      "        2401531, 2935159, 1134095, 2401423,  967330, 1169389, 2134621,   33489,\n",
      "        3068477, 2935179, 1767840, 1200709,  533649, 1300774,  767126, 1701528,\n",
      "        2035058, 2134599, 1301456, 2634888, 3068569, 3235242, 2835006,  233607,\n",
      "        1134132, 1467533,  300308, 2968418, 1167880, 3068635, 1767845, 1200801,\n",
      "        2034626,  967266, 3101830,  100190, 2134751, 1134117, 2101240, 3035124,\n",
      "         967352,  200643,  967926, 2401417])\n",
      "Beam idx: tensor([66, 74, 69, 36, 87,  7, 33, 60, 98, 18, 96, 93, 27, 61, 26, 64, 82, 59,\n",
      "        74, 55, 52, 30, 19,  2, 41, 73, 33, 57, 46, 67, 52,  4, 98, 82, 37, 53,\n",
      "        47, 26, 97, 94, 94, 14, 18, 20, 59, 64, 11, 52, 78, 25, 60, 29, 52, 20,\n",
      "        39, 55, 20, 30, 98, 81, 67, 55, 11, 25, 52, 65, 26, 31, 81, 29, 11, 60,\n",
      "        47, 92, 93, 20, 33, 97, 20, 47, 64, 77, 19, 27, 82, 37, 87, 82, 18, 26,\n",
      "        85, 51, 95, 52, 94, 27, 64, 78, 30, 80])\tToken idx: tensor([ 131,  131,  131,  131,  131,  131,    1,    7,    1,    1,  131,   28,\n",
      "         525,    1,    1,    7,  525,  525,  136,  525,    7,    1,    7,  131,\n",
      "         131,    1,    7,  131,  525,  131,  704,  131,    7,    1,    1,  131,\n",
      "           7,  525,    7,    1,  525,  131,    7, 2443,  131,  115,    7,  169,\n",
      "         131,  131,  115,  131,   95,    7,  131,  131,    1,  525,  115,    1,\n",
      "          28, 1203,    1,  136,  115,  525,    7,  525,    7,  136,  115,    1,\n",
      "         115,    1,  261, 2034,  115,  115,  115, 1722,    1,  525,    1,    1,\n",
      "         136,  318,  525,   93,  241,   93,    1,  525,    1,  872,   93,  130,\n",
      "          93,    7,    7,    1])\tRaw indices: indices_to_process=tensor([2201429, 2468253, 2301488, 1200839, 2901842,  233602, 1100650, 2001187,\n",
      "        3268595,  600355, 3202019, 3101857,  901056, 2034534,  867179, 2134599,\n",
      "        2735471, 1968352, 2468258, 1834940, 1734363, 1000591,  633714,   66837,\n",
      "        1367604, 2434770, 1100656, 1901252, 1534763, 2234782, 1735060,  133543,\n",
      "        3268601, 2734947, 1234062, 1767840, 1567598,  867703, 3235248, 3135183,\n",
      "        3135707,  467073,  600361,  669503, 1967958, 2134707,  366890, 1734525,\n",
      "        2601665,  833956, 2001295,  967368, 1734451,  667067, 1300898, 1834546,\n",
      "         667061, 1001115, 3268709, 2701594, 2234679, 1835618,  366884,  833961,\n",
      "        1734471, 2168470,  867185, 1034468, 2701600,  967373,  366998, 2001181,\n",
      "        1567706, 3068477, 3102090,  669094, 1100764, 3235356,  667175, 1569313,\n",
      "        2134593, 2568706,  633708,  900532, 2735082, 1234379, 2902236, 2735039,\n",
      "         600595,  867271, 2835006, 1701528, 3168536, 1735228, 3135275,  900661,\n",
      "        2134685, 2601541, 1000597, 2668241])\n",
      "DEB: t.shape=torch.Size([6, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,    7,    7,  235,    7,    7,    7,  118,    7, 2416,    7,  235,\n",
      "         322,  322,  447,  159,    7,  235,  131,  118,    7,  115,  235,  131,\n",
      "         159,  447,  159,  159,  159, 2416,  235,  447,  305,    7,  131,   50,\n",
      "         235, 2416,  235,   60,    7,   60,    1,  236,  235,    7,  115,   60,\n",
      "         159,  118,    7,    7,  235,  159, 3694, 2416,  322,  159, 1604,    1,\n",
      "          60,  159,    1,   60,  322,  235, 3694,  159, 2416,  131,    7,    7,\n",
      "         447,  235,  235,  235,  322,    7,  236, 2416,   60,    7,    1,  131,\n",
      "         322,  235,  322,    7,  235, 3694,  159,  115,   60,  322,  235,  159,\n",
      "         322,  236,  159,    7,  136,   29,  528,    1,  471,  431,  119,    1,\n",
      "         518,  431,   93,  118,  528,    1,  343,  528, 1496,  528,    1, 1722,\n",
      "          29,  998,  518,    1, 1496, 1722,  998,  528,    1,  119,  119,  528,\n",
      "         343,  518,   60, 1346,    1, 1722, 1722,    1,  119,  998,  130,  447,\n",
      "         528,   93,   29,    1,    1,   93, 1722,  471,   93,  998,  528,    1,\n",
      "         431,   93,  130,  130,  528,    1,    1, 1722,  998,  518,  119,  528,\n",
      "         447,  528,  118,  528, 1346,  518,    1, 1722,    1,  998,  471,  119,\n",
      "        1722,  528,    1,  998,    1,  119,   60,   60,   29,    1,  118,  528,\n",
      "           1,  119, 1722,  998,  431,    1,    1,    1,  525, 1491, 1023,   93,\n",
      "         205,  130,   93,  131,  130,  525,    7,  525,  525,  525,  525,  115,\n",
      "           7,  136,  525,  525,  130,  525,  130,  136,    7,  131,  131,  130,\n",
      "         525,  431,  525,  131,  525,  525,  131,  130,  525,   93,  525,  525,\n",
      "         525,  431, 1023,  525,  525,  525,  130,  131,  136,  525,  130,  136,\n",
      "         525,  525,  426, 1023, 1491,  131,  525, 1491,  525,  131,  525,    7,\n",
      "         131,  131,  131,  136,  525,  525,  136,  131, 1023,  525,  525,  525,\n",
      "         131,  525,  136,  525,  525,  205,  136,  525,  131,  131,  131,  136,\n",
      "        1023,  525,  130,  525,  525,  525,  205,  525,  525,  130,  525, 1491,\n",
      "         115,  525,  525,  525,  525,  261,  525,  525,  525,  525,  525,  525,\n",
      "         525,  525,  525,  525,  130,  525,  525,  689,  525,  882,  525,  525,\n",
      "         882,  525,  525,  525,  525,  525,  525,  471,  525,  130,    8,  525,\n",
      "         525,  525,  525,  525,  525,  525,  525,  471,  525,  525,  525,  525,\n",
      "         525,  525,  525,    1,  525,  471,  130,  689,  471,  882,  525,  525,\n",
      "         525,  689,  525,  525,  525,    1,  525,  525,  525,    1,  525,  525,\n",
      "         525,  130,  525,  471,  525,  525,  471,  525,  525,  525,  525,  525,\n",
      "         130,    8,  525,  130,  525,  525,  525,    8,  525,  525,  525,  525,\n",
      "         525,  525,  882,  525])\n",
      "\ttensor([ 169,    1,  169,  525,    1,  169,    1,  131,  169,  525,  169,  525,\n",
      "         525,  525,  525,   93,    1,  525,  525,  131,  169,  525,  525,  525,\n",
      "         130,  525,  130,   93,   93,  525,  525,  525,  525,  169,  525,  525,\n",
      "         525,  525,  525,  525,  169,  525,    1,  525,  525,  169,   93,  525,\n",
      "          93,  131,    1,  169,  525,   93,  525,  525,  525,   93,  525,    1,\n",
      "         525,  130,    1,  525,  525,  525,  525,   93,  525,  525,    1,  704,\n",
      "         525,  525,  525,  525,  525,  169,  525,  525,  525,  169,    1,  525,\n",
      "         525,  525,  525,    1,  525,  525,  130,   93,  525,  525,  525,   93,\n",
      "         525,  525,  131,  169,  261,  115,  525,  115,  525,  525,  525,  115,\n",
      "         525,  525,    1,  525,  525,   29,  525,  525,  525,  525,   29,  130,\n",
      "         115,  525,  525, 1165,  525,  130,  525,  525,   29,  525,  525,  525,\n",
      "         525,  525,  525,  525,   29,  130,  130,   29,  525,  525,    1,  525,\n",
      "         525,    7,  115,  115, 1165,    1,  130,  525,    1,  525,  525,  115,\n",
      "         525,    7,    1,    1,  525,    1,  115,  130,  525,  525,  525,  525,\n",
      "         525,  525,  525,  525,  525,  525,   29,  130,  115,  525,  525,  525,\n",
      "         130,  525,  118,  525,  115,  525,  525,  525,  115,  115,  525,  525,\n",
      "        1165,  525,  130,  525,  525, 1165,   29,  118,  115,  525,    1,    7,\n",
      "         525,  130,  115,    1,    1,  579,  525,  510,  426,  518, 1203,  525,\n",
      "         525,  131, 1203,  518,    1,  510,    1,  131,  525,    1,    1,  136,\n",
      "        1203,  525,  518,    1,  518,  518,    1,  136, 1203,  115,  510,  518,\n",
      "         579,  525,    1,  169, 1203,  579,  130,    1,  131,  518,    1,  131,\n",
      "         426,  130,  525,    1,  525,    1,  518,  525,  518,    1,  518,  525,\n",
      "           1,    1,    1,  131,  426,  510,  131,    1,    1,  518,  510, 1893,\n",
      "           1,  426,  131,  115,  518,  525,  131,  426,    1,    1,    1,  131,\n",
      "           1,  518,  136, 1497,  518,  518,  525,  169,  518,    1,  518,  525,\n",
      "         525,  510,  704,  510,  371,  525, 1893,  131,  704,  579,   60,   87,\n",
      "          91,  518,  510,   78,    1, 2059,  510,  525, 1893,  525,   91,  371,\n",
      "         525,   78, 1893,  124,  579,   91, 1893,  525,  704,    1,  525,   91,\n",
      "          91,  510,   91,  874,  874,  510,  579,  525, 2059,   78,  874, 1893,\n",
      "          91,   60,  131,    7, 1893,  525,    1,  525,  525,  525,  704,  148,\n",
      "          91,  525,  874,   60, 1893,    7,  510, 1893,  148,    7,  874,  131,\n",
      "          91,    1,   87,  525, 1893,   91,  525,   91,   78,  874,   91,   91,\n",
      "           1,  525,  371,    1,  579,  510,  874,  525, 1385, 1893,  874,   91,\n",
      "          78,   91,  525,  518])\n",
      "\ttensor([115,   7, 115, 115,   7, 115,   7, 525,   7, 525, 115, 115,   7,   7,\n",
      "        261,   7,   7,   7, 525, 525,   7,   7, 115, 525,   7, 261,  93,   7,\n",
      "          7, 525, 115, 159, 525,   7, 525, 115, 261, 525, 261, 525,   7,   1,\n",
      "        261,   7, 261,   7, 525, 525,   7, 525,   7, 115,   7,   7, 525, 525,\n",
      "          7,   7,   1, 261,   1,   7,   7, 525,   7, 115, 525,   7, 525, 525,\n",
      "          7,   7, 115, 115,   7, 115,   7, 115,   7, 525,   1, 115, 261, 525,\n",
      "          7, 261,   7,   7, 115, 525,   7, 525, 525,   7, 115,   7,   7,   7,\n",
      "        525, 115, 525,   7, 115, 525,   7,  93, 159, 525, 525,  93, 115, 525,\n",
      "          7, 131,   1,   7, 115, 115, 131, 525, 115,   1, 525, 525, 115, 525,\n",
      "          1, 115, 131, 159,   7, 115, 525, 525,   1, 130, 131, 525, 525, 131,\n",
      "          7,   1, 115, 115, 115,   7, 115, 525, 525, 115, 525,   7, 115,   1,\n",
      "        115, 525,  93,   7, 159, 115, 115,  93, 525, 525,   1, 525,   7,   7,\n",
      "        115, 115, 525, 115, 130, 525, 131, 525, 525,   1,   7, 159, 525, 115,\n",
      "          7,   1, 525, 159,   1,   1, 115, 525, 525, 115, 525, 159, 525,   1,\n",
      "         93, 525, 131,   7, 131, 525, 525, 131, 130,   1, 525, 130, 115, 525,\n",
      "        525,   1,   7, 525, 525, 525,  93,   7, 525, 525, 115,   1, 115, 115,\n",
      "         93,   1, 525, 525, 525,   1, 525,   1, 525, 525, 525, 525, 525,   1,\n",
      "          1, 525, 525,   1, 525,   7, 525, 525,   1,   1, 115, 525, 115, 115,\n",
      "          7,   1, 525, 525, 525,   1, 525, 525, 525, 525, 525, 525, 525,   1,\n",
      "          1, 115, 115,   1, 115, 130, 525, 525,   1, 115, 525, 115,   7, 131,\n",
      "        525, 130, 115,   7, 525, 525,   1, 115, 525, 525, 525,   7, 525, 525,\n",
      "        130, 159, 525, 115, 525, 525, 159,   7, 525,   7, 130, 525, 525, 525,\n",
      "        525,   7, 525, 525,   1, 525,   7,   7,  93, 136,   7, 136,   1, 525,\n",
      "          7, 525, 525, 115, 525, 525, 261, 115,   1, 525, 525,  93, 525,   7,\n",
      "        261,   7,   7,   7,   7,   7,   7, 525, 136,   7,   7,   1,   7, 525,\n",
      "        525,   7,   1, 525, 525, 136, 525, 525, 525, 525, 115, 136,   7, 525,\n",
      "          1,   7,   7,   1, 525,   7,   7, 525, 261,  93, 525, 525, 525,   7,\n",
      "        525, 261,   7,   7,   7,   1,  93, 525, 130,  93,   7,   7,   7, 525,\n",
      "          7,   1,   7,   1,   7,   7, 525, 525])\n",
      "\ttensor([   1,  525,    1,  131,  525,    1,  525,  131,  131,    7,    1,  131,\n",
      "         525,  525,  131,  241,    1,    7,    1,  131,  131,  131,  131,    1,\n",
      "           7,  131,    7,  241,  241,    7,  131,  131,  525,  131,   93,  525,\n",
      "         525,    7,  525,   93,  131,  131,  131,  525,  525,  131,  131,  525,\n",
      "         241,  131,    1,   93,    7,    7,  872,    7,  525,    7,    1,  131,\n",
      "         131,    7,    1,  525,  525,  704,  872,  241,    7,   93,    1,  872,\n",
      "           7,  131,    7,  131,  525,    1,  525,    7,  131,    1,  131,    1,\n",
      "         525,  525,  525,    1,  131,  872,    7,  131,    1,  525,  131,    7,\n",
      "         525,  525,  525,   93,    7,    7,  525,    1,  115,  525,  525,    1,\n",
      "           1,  525,  525,  131,    1,  525,  525,    1,   93,  525,  525,    7,\n",
      "         525,  131,    1,    7,   93,    7,  131,  525,  131,  525,    7,  525,\n",
      "         131,    1,  131,    7,  525,    7,  689,  525,    7,  131, 1383,  131,\n",
      "         525,  131,  525,    1,    7,  525,    7,    7,  525,  131,  525,    7,\n",
      "         525,  131,  131, 1383,  525,  131,    7,    7,  131,    1,    7,    1,\n",
      "         131,  525,  872,  525,    7,    1,  525,    7,    7,  131,    7,  525,\n",
      "           7,  525,   28,  131,    1,  525,  131,  131,  525,  130,  872,  525,\n",
      "           1,  525,    7,  131,  525,    7,  525,   28,    7,  131,  525,  689,\n",
      "        2569,  131,  689,    1,  131,  525,  525,  525,  131,  525,  131,  525,\n",
      "           7,  525,  131,  525,  525,  525,  525,  525,    1,  525,  241,  131,\n",
      "         131,   93,  689,  525,  689,  689,   60,  131,  131,  525,  525,  525,\n",
      "         525,   93,  525,  115,  131,  525,  131,  525,  525,  525,  525,  525,\n",
      "         131,    7,  131,  525,  131,  525,  689,  131,  525,  241,  525,  525,\n",
      "          60,  525,  689,  525,  131,   93,  525,    1,  525,  525,   93,  131,\n",
      "          60,  131,  525,    7,  689,  375,  525,  131,  241,   60,  689,  525,\n",
      "         525,  525,  131,  131,  525,  689,  375,   93,  525,  525,  525,  131,\n",
      "         525,  131,  115,  131,  131,  525,    7,  131,    7,  131,  525,    1,\n",
      "         525,    7,  131,  131,  525,    1,  131,  525,  159,   93,  525,  131,\n",
      "           1,    7,    7,  131,    1,  525,  159,  115,    7,  525,  525,  525,\n",
      "         525,  131,  525,  261,  261,  131,  131,    7,    1,  131,    7,  159,\n",
      "         525,    7,  131,    1,  159,    7,  525,  525,    7,   93,    7,    1,\n",
      "         525,  525,    7,    7,  159,    1,  131,  159,    1,    1,    7,  131,\n",
      "         525,  525,    1,    7,    7,  525,    7,  525,  131,  115,  525,  525,\n",
      "         525,  525,  131,  525,  131,  131,    7,    1,    1,  159,  261,  525,\n",
      "         131,  525,   93,  525])\n",
      "\ttensor([   1,   60,  241,    1,  241,    7, 1203,   93,  525,    1,    1,    1,\n",
      "          28,    1,  131, 2443,    1,    7,  131,   93,  525,  131,  130,  131,\n",
      "         131,  131,    7, 2443, 2443,    1,   93,  131,  131,  136,  131,    1,\n",
      "          60,    1,   60,  525,  525,  136,   60,  136,   60,  136,  131,  131,\n",
      "        2443,   93,    1,    7,    1,  131,    7,    1,   28,  131,    7,   60,\n",
      "         136,  131,    7,  131,  525,  131,    1, 2443,    1,  131,    1,  131,\n",
      "         115,   93,    1,  169,    7,   93,  131,    1,  136,   93,  241,  131,\n",
      "          28,   60,   28,    1,  689,    7,  131,  131,  131,  525,    1,  131,\n",
      "          28,  136,    7,    1,  131,  131,  131,    7,  131,  131,    7,    7,\n",
      "         131,  131, 1203,  131,    7,  130,  136,    7,    7,   29,  130, 1203,\n",
      "         131,    1,  131, 1203,    7, 1203,    1,   29,  136,  525,    7,    1,\n",
      "           1,  131,  115,  131,  130, 1203,  131,  525,    7,    1,    7,    1,\n",
      "           1,    1,  131,    7, 1203,  525, 1203,    7, 1203,    1,    7,   28,\n",
      "         131,    1,  131,    7,  131,  131,   93, 1203,    1,  131,  115,    7,\n",
      "           1,   29, 1383,  261,  136,  131,  115, 1203,   93,    1,  115,  525,\n",
      "        1203,  525,    1,    1,    7,    7,    7,    7,  131,    1, 1383,  131,\n",
      "         131,  525, 1203,    1,  131, 1203,  525,    1,  131,  525,  525,  131,\n",
      "         131,  131,  131,    7,    1,    1,  131,  131,  261,    7,  115,  131,\n",
      "         131,  131,  115,    7,  131,  131,  525,   93,  131,    7,  131,  525,\n",
      "         115,  136,    7,    7,    7,    7,  525,  525,  115,  131,  131,  115,\n",
      "           7,  136,  318,    7,    7,    1,  704,    7,   93,  115,  131,   93,\n",
      "         261,  525,  131,  318,  131,    7,    7,  131,    7,  131,  115,  131,\n",
      "         525,    7,    7,    1,  131,  131,    7,  115,  318,  115,  131,  525,\n",
      "         525,  136,  136,  131,    7,  131,  525,   28,  131,  525,    7,    1,\n",
      "         318,    7,  115,  131,  115,    7,  136,  131,    7,  525,    7,  131,\n",
      "         525,  136,    7,  130,  131,  131,    1,    1,    7,  131,  131,  136,\n",
      "         131,  525,  169,  131,  525,    1,  136,  131,    1,  131,    1,  131,\n",
      "           7,  131,    1,    7,    1,  689,    1,  131,    7,  525,  131,  115,\n",
      "        1203,  169,  525,  130,  130,  131,  131,  131,    1,  131,  131,    1,\n",
      "          93,  525,    1,    1,    1,  131,  525,  131,  131,  131,    7,    1,\n",
      "         689,  131,  131,  525,    1,  525,  169,    7,    1,    1,  131,    1,\n",
      "        1203,    1,  136,  131,    1,  525,  131, 1203,  131,  131,    1,  131,\n",
      "         525,  131,  131,  525,  131,  169,  136,  131, 1891,    1,  130,  131,\n",
      "         131,   93,  131,  136])\n",
      "\ttensor([ 131,  131,  131,  131,  131,  131,  131,    7,  131,  525,  136,  525,\n",
      "          28,  525,    7,    7,    1,  525,    7,    1,  136,  131,  131,    1,\n",
      "           1,  115,  131,  525,  131,    1,  525,  131,  525,  131,  131,   28,\n",
      "        1203,    7,  525,    1,  525,    1,  131,    7,  241,  136,  131, 1383,\n",
      "         115,  115,    7,    7,  131,    1,  241,  130,  261,  131,    1,  136,\n",
      "          93,  136,    7,    1,  525,  525,  131, 1722,  241,  136,  525,    7,\n",
      "         525,  136,  136,  131,  131,   60,    1,   93,  115, 1203,    1,   93,\n",
      "           7,   60,  174,   93,  131, 1203,   93,  136,    1,  131,    7,  136,\n",
      "         525, 1722, 1203,  525,  131,  131,  131,    1,  131,    1,  131,    7,\n",
      "           1,    7,  525,  131,    1,   28,  131,    7,  131,  131,  131,    1,\n",
      "         525,    1,    7,    1,  136,   93,   93,   28,  525, 1203,  525,  131,\n",
      "         131,  241,    1,  131,  261,   29,  131,  525,  131,   29,  131,  525,\n",
      "         136,  131,  131,  115, 2034,    1,    7,  131,  136,    7,  131,   28,\n",
      "         115,  136,  131,  136,  136,  131,    1,  159,  159,  525,  525,  115,\n",
      "         136,  261,    1,  525,  131,   93,  525,  689,   93,  689,  131,  525,\n",
      "         169,    1,    7,  169,   93,  525,  525,    1,  136,    7,   93,  525,\n",
      "           1,  241,  136,  136,   93, 2443,  131,    1,  131,  131,  131,  131,\n",
      "         131,  131,  131,  131,  525,  131,  131,    1,  131,    7,   60,  131,\n",
      "         525,  131, 1203,    1,  525,    7,    1,    1,  131,    7,    1,  131,\n",
      "         241,  525,    1,  704,  525,    7,    7,  136,  525,  131,  115,    1,\n",
      "          28,  131,    7,  241,  131,  136,  525,  169,  131,   93,  131,  136,\n",
      "         136,    7,    7,    1,  115,   95,   93,    7,   93, 2034,   29,  136,\n",
      "           1,  115,  131,    1,    1,    7,    7,  525,  525,    7,  689,    1,\n",
      "          93,    1,    1,  136,  130,    1,  131,    1,  525,  159,  136,   93,\n",
      "          93,   29,    1,  131,  159,  115,    1,    1,  115,  525,  689,    1,\n",
      "         131,  131,  131,  131,  131,  131,    1,    7,    1,    1,  131,   28,\n",
      "         525,    1,    1,    7,  525,  525,  136,  525,    7,    1,    7,  131,\n",
      "         131,    1,    7,  131,  525,  131,  704,  131,    7,    1,    1,  131,\n",
      "           7,  525,    7,    1,  525,  131,    7, 2443,  131,  115,    7,  169,\n",
      "         131,  131,  115,  131,   95,    7,  131,  131,    1,  525,  115,    1,\n",
      "          28, 1203,    1,  136,  115,  525,    7,  525,    7,  136,  115,    1,\n",
      "         115,    1,  261, 2034,  115,  115,  115, 1722,    1,  525,    1,    1,\n",
      "         136,  318,  525,   93,  241,   93,    1,  525,    1,  872,   93,  130,\n",
      "          93,    7,    7,    1])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([32, 41, 62, 90, 30, 24, 32, 76, 46,  7, 66,  5, 74, 36, 48, 41, 40, 73,\n",
      "        29, 52, 69, 76, 97, 35, 20, 60, 59,  9,  3, 32, 18,  8,  7,  8, 60, 62,\n",
      "        29, 66, 28, 89, 24, 89, 48, 51,  5, 97, 13,  5, 37, 59, 32, 79, 56, 81,\n",
      "        35, 30, 36, 78,  8, 37,  8, 20, 40, 35, 70, 97, 89, 81, 37, 12, 69, 37,\n",
      "        83, 20, 92, 11, 20, 20, 13, 69, 73, 82, 36, 52, 32, 35, 60, 40, 37, 35,\n",
      "        66, 15,  7, 52, 37, 12, 59,  4, 14,  8])\tToken idx: tensor([ 131,  131,  131,  131,  131,  131,  525,    7,  131,  131,  131,    1,\n",
      "         131,    7,  131,  136,    7,  131,    7,    7,    1,  115,  525,  525,\n",
      "           1, 1203,  525,    7,    7,    7,  131,    7,  136,    1,   60,  136,\n",
      "           1,  136,  131,  136,  136,  525,  136,  131,  525,  136,    7,  136,\n",
      "         169,  136,  136,  525,    1,  131,  689,  525,  115,  241,   93,  872,\n",
      "         159,   93,  115,    1,    1,    1,    1,  136,  136,    1,  136,  525,\n",
      "         131,    7,  241,    1,  136,   29,    1,  131,  136,   60,    1,  115,\n",
      "         130,  136,  131,    1,  131,  130,    1,    1,  525,  131,  689,   29,\n",
      "           1,    1,    1, 1722])\tRaw indices: indices_to_process=tensor([1067427, 1367604, 2068017, 3001901, 1000721,  800603, 1067821, 2534835,\n",
      "        1534369,  233602, 2201429,  166766, 2468253, 1200715, 1601075, 1367609,\n",
      "        1334127, 2434900,  967244, 1734363, 2301358, 2534943, 3235766, 1167880,\n",
      "         667061, 2002383, 1968352,  300184,  100066, 1067303,  600485,  266831,\n",
      "         233607,  266825, 2001240, 2068022,  967238, 2201434,  934015, 2968553,\n",
      "         800608, 2968942, 1601080, 1701134,  167290, 3235377,  433596,  166901,\n",
      "        1234230, 1967963, 1067432, 2635412, 1867769, 2701724, 1168044, 1001115,\n",
      "        1200823, 2601775,  266917, 1234933,  266983,  667153, 1334235, 1167356,\n",
      "        2334711, 3235242, 2968418, 2701729, 1234197,  400237, 2301493, 1234586,\n",
      "        2768430,  667067, 3068717,  366884,  667196,  667089,  433590, 2301488,\n",
      "        2434905, 2735006, 1200709, 1734471, 1067426, 1167491, 2001311, 1334121,\n",
      "        1234192, 1167485, 2201299,  500296,  233996, 1734487, 1234750,  400265,\n",
      "        1967828,  133413,  466943,  268546])\n",
      "Beam idx: tensor([75, 99,  4, 78, 38, 19, 51, 48, 52, 27, 32, 42, 44, 33, 50, 83, 93, 27,\n",
      "        51, 77,  4, 31, 91, 89, 58, 14, 55, 33, 49, 49, 97, 97, 29, 27, 33, 50,\n",
      "        52, 12, 51, 46, 39,  4, 70, 21, 26, 19, 44, 58, 38, 20, 97, 33, 51, 27,\n",
      "        33, 99, 48, 12, 27, 14, 32, 30, 83, 46, 95, 72, 12, 40, 51, 55, 64, 46,\n",
      "        71, 17, 33,  6, 40, 98, 90, 50, 56, 64, 12, 93, 27, 83, 49, 12, 46, 48,\n",
      "        46, 69, 32, 97, 33, 58, 26, 33, 93, 27])\tToken idx: tensor([  131,     1,     1,   131,     1,     7,   131,     7,   131,   131,\n",
      "            7,     1,   131,     1,   525,   525,     1,     1,   689,   525,\n",
      "            7,   131,   525,   525,   131,   525,   131,     7,     1,   872,\n",
      "          136,   525,   131,   136,   131,   136,   136,     1,     7,   136,\n",
      "            7,   115,     1,     1,   525,     1,   136,   136,     7,   131,\n",
      "            1,   136,   136,     7,   169,     7,   115,     7,   872,     1,\n",
      "          115,     7,   136,   525,   131,     7,   689,     7,    90,   136,\n",
      "            7, 12072,     1,     7,    93,     1,     1,     1,     7,   689,\n",
      "          241,     1,    93,   136,   169,     1,     7,   426,   131,     1,\n",
      "         1383,     7,     1,   471,    29,   525,     1,   689,   131,    95])\tRaw indices: indices_to_process=tensor([2501606, 3301948,  133413, 2601665, 1267415,  633714, 1701134, 1600951,\n",
      "        1734487,  900662, 1067303, 1400827, 1467663, 1100650, 1668175, 2768824,\n",
      "        3101830,  900532, 1701692, 2568706,  133419, 1034074, 3035648, 2968942,\n",
      "        1934605,  467467, 1834546, 1100656, 1634298, 1635169, 3235377, 3235766,\n",
      "         967368,  900667, 1100780, 1667786, 1734492,  400237, 1701010, 1534374,\n",
      "        1300774,  133527, 2334711,  700414,  867703,  633708, 1467668, 1934610,\n",
      "        1267421,  667191, 3235242, 1100785, 1701139,  900538, 1100818, 3301954,\n",
      "        1601059,  400243,  901403,  466943, 1067411, 1000597, 2768435, 1534763,\n",
      "        3168666, 2401423,  400925, 1334127, 1701093, 1834551, 2134599, 1546310,\n",
      "        2368064,  567008, 1100742,  200119, 1334121, 3268595, 3001777, 1668339,\n",
      "        1868009, 2134593,  400329, 3101965,  900700, 2768300, 1634304,  400662,\n",
      "        1534369, 1600945, 1535621, 2301364, 1067297, 3235712, 1100678, 1934999,\n",
      "         867179, 1101338, 3101960,  900626])\n",
      "Beam idx: tensor([50, 47, 32, 75, 21, 58, 75, 67, 54, 49, 60, 41, 43, 37, 22, 67, 75, 41,\n",
      "        52, 56, 25, 78, 83, 25, 11,  9, 47, 83, 68, 83, 25, 32, 96, 33, 84,  2,\n",
      "        13, 80, 78,  2, 67, 70, 60, 82,  2, 33, 40, 30, 48, 69, 31, 98, 22, 66,\n",
      "        52, 49, 15, 59, 68, 83, 25, 84, 71, 75,  2, 75, 49, 78, 46,  8, 45, 98,\n",
      "        79, 13, 50, 66, 17, 15,  2, 87, 94, 58, 99, 25, 96, 84, 67, 76, 67, 82,\n",
      "         2,  6, 75, 55, 11, 75, 77, 41, 54, 11])\tToken idx: tensor([  131,   131,   131,   525,   131,   131,   136,   525,   131,     1,\n",
      "            1,     7,   525,   525,   131,     1,   131,   241,     7,     7,\n",
      "          131,   525,   136,   689,     7,   131,   136,   525,     7,   131,\n",
      "            7,   136,     1,     7,     1,     1,     7,     7,     1,     7,\n",
      "          136,   131,     7,   525,   131,   131,   131,     1,     1,   131,\n",
      "          689,     1,   136,     7,   115,   136,   131,   525,   115,   169,\n",
      "          136,     7,     7,     1,   115,   169,   131,   136,     1,     1,\n",
      "            1,   525,   131,     1,   136,   689,     1,   136,   136,   525,\n",
      "          525,   136,   525,    90,     7,   115,    93,   689, 12072,   136,\n",
      "           93,   131,   130,     1,   131,   689,  1203,   525,   136,     1])\tRaw indices: indices_to_process=tensor([1667781, 1567722, 1067427, 2502000,  700544, 1934605, 2501611, 2235176,\n",
      "        1801193, 1634298, 2001181, 1367480, 1434704, 1234586,  733897, 2234652,\n",
      "        2501606, 1367714, 1734363, 1867775,  833956, 2602059, 2768435,  834514,\n",
      "         366890,  300308, 1567727, 2768824, 2268011, 2768430,  833832, 1067432,\n",
      "        3201889, 1100656, 2801653,   66707,  433596, 2668247, 2601535,   66713,\n",
      "        2234787, 2334841, 2001187, 2735471,   66837, 1100780, 1334251, 1000591,\n",
      "        1600945, 2301488, 1034632, 3268595,  733902, 2201305, 1734471, 1634433,\n",
      "         500426, 1968352, 2268119, 2768468,  833961, 2801659, 2368070, 2501476,\n",
      "          66821, 2501644, 1634428, 2601670, 1534239,  266825, 1500886, 3269119,\n",
      "        2635018,  433590, 1667786, 2201987,  567002,  500431,   66842, 2902236,\n",
      "        3135707, 1934610, 3302472,  833915, 3201895, 2801767, 2234744, 2535517,\n",
      "        2246723, 2735082,   66799,  200249, 2501605, 1834416,  367014, 2502164,\n",
      "        2569384, 1367998, 1801198,  366884])\n",
      "Beam idx: tensor([66, 36, 39, 99, 69, 43, 89, 88, 91,  6, 99,  5, 48, 91,  5, 88, 16, 89,\n",
      "        91, 25, 32, 44, 54, 83, 94, 91, 71, 15, 89, 63, 88, 12, 75, 69, 41, 57,\n",
      "        66, 86, 99, 12, 61, 28, 53, 13, 70, 16, 24,  5, 12, 10, 90, 79, 65, 50,\n",
      "        24, 72, 45, 89, 23, 44, 32, 38, 76,  9, 34, 35, 45, 49, 24, 86, 72, 84,\n",
      "        70,  6, 82, 75, 10, 58, 59, 17,  5, 89,  9,  1, 76, 49,  5, 54, 89,  3,\n",
      "        96,  6, 21, 89, 99, 72, 89,  1, 84, 88])\tToken idx: tensor([ 131,  131,  131,  525,  131,  131,  525,    1,  131,  131,  241,  525,\n",
      "         131,    1,  136,    7,  131,  136,  136,  131,    7,    1,  525,   28,\n",
      "         131,  525,    7,  131,    1,  131,  115,  872,    7,  136,  131,  131,\n",
      "         136,    1, 1203,    7,  131,    1,  131,    1,  525,  136,    7,  689,\n",
      "           1,  131,    7,    7,  131,    1,    1,    1,  131,  689,    1,    7,\n",
      "         115,    1,    1,  131,    1,    7,  136,  131,  115,  515,   93,    1,\n",
      "           1,  525,  131,  115,  136,  525,  525,    1,  131,  131,  136,    7,\n",
      "          93,  136,  471,  136,   93,    7,    7,  136,    1,   29,   60,   29,\n",
      "         130,    1,   93,   93])\tRaw indices: indices_to_process=tensor([2201429, 1200839, 1300898, 3302472, 2301488, 1434310, 2968942, 2935065,\n",
      "        3035254,  200249, 3302188,  167290, 1601075, 3035124,  166901, 2935071,\n",
      "         533779, 2968553, 3035259,  833956, 1067303, 1467533, 1801587, 2768327,\n",
      "        3135313, 3035648, 2368070,  500426, 2968418, 2101370, 2935179,  401108,\n",
      "        2501482, 2301493, 1367604, 1901252, 2201434, 2868359, 3303150,  400243,\n",
      "        2034664,  933885, 1767840,  433590, 2335235,  533784,  800479,  167454,\n",
      "         400237,  333661, 3001777, 2634894, 2168076, 1667651,  800473, 2401417,\n",
      "        1501016, 2969106,  767120, 1467539, 1067411, 1267415, 2534829,  300308,\n",
      "        1134003, 1167362, 1501021, 1634428,  800587, 2868873, 2401509, 2801653,\n",
      "        2334711,  200643, 2735077, 2501590,  333666, 1934999, 1968352,  567002,\n",
      "         166896, 2968548,  300313,   33360, 2534921, 1634433,  167236, 1801198,\n",
      "        2968510,  100066, 3201895,  200254,  700414, 2968446, 3302007, 2401445,\n",
      "        2968547,   33354, 2801745, 2935157])\n",
      "DEB: t.shape=torch.Size([7, 400])\n",
      "tensor=(\n",
      "\ttensor([ 305,   60,    1,  159,  235,  159,  305,  322,  115,  118, 3694,    7,\n",
      "         235,  235,  159,   60,    7,  235, 2416,  235,  131,  322,  236,   50,\n",
      "           7,   60,    1, 2416,  235,  305,  131,    7,  118,    7,   60,    1,\n",
      "        2416, 3694,  159, 3694,  159, 3694,  159,    7,    7,  236,  322,    7,\n",
      "        2416,    1,  305, 2416,  322,    7,   50,  235,  235,  236,    7, 2416,\n",
      "           7,    7,    7,   50,    7,  236, 3694,    7, 2416,  322,  131, 2416,\n",
      "         131,    7,   60,  235,    7,    7,  322,  131,  235,    1,  235,  235,\n",
      "         305,   50,   60,    7, 2416,   50, 3694,  159,  118,  235, 2416,  322,\n",
      "           1,    7,  447,    7, 1722,    1,  471,  471, 1722, 1722,  471,    1,\n",
      "          93,  528,  343,  130,  528,  518, 1722,  998,  119,  528,  471,  998,\n",
      "         471,  528,  528,    1,  130,  343,    1,  518,   93,   93,    1,    1,\n",
      "         119,  528,  518, 1722,   93,  528,  471,   29,    1,  471,  118,  998,\n",
      "         998, 1722,  528,  130, 1722,   29,    1,  518,  471,  528,  518,    1,\n",
      "           1,  528,  528,  343,  343,  119,  998,   29,  998, 1346,  528,  119,\n",
      "         471,    1,  998,   29,  528,  528,  518,  119,  119,    1,  118, 1722,\n",
      "         431,  998,  528,  119,  528,  998,   93,  528,   29,    1,   29,  528,\n",
      "         343,    1,  518,  130,  998,  518,  119,  528,  130,  131,  525,  525,\n",
      "         525,  525,  525,  136,  426,  525,  525,  431,  525,   93,  130,  136,\n",
      "         525,  431,  525, 1491,  131,  136,  525,  131,  525,  525,  131,  525,\n",
      "         525,  525,  131,  525,  525,  525,  131, 1023,  525,  525,  136, 1023,\n",
      "         136,  136,  525,  136, 1023,  525,  525,  525,  136,  525,  131,  525,\n",
      "         130,  131,  525,  525,  115, 1491,  525,  525,  131,  131,  131,  525,\n",
      "        1023,  525,  525,  136,  130,  130,  525,  525,  525,  525,  130,  131,\n",
      "         136,  115, 1023,  136,  205,  525, 1491,  131,  525,  131,  136,  131,\n",
      "         136,  136, 1023,   93,  525, 1023,  525,  525,  525,  431,  426,  525,\n",
      "         525,  525,  525,  525,    1,  471,  525,  525,    8,  525,  525,  261,\n",
      "         525,    8,  261,  525,  130,  525,    8,  525,  525,  525,  130,  525,\n",
      "         525,    8,  525,  525,  525,  525,  525,  525,  471,    1,  525,  882,\n",
      "         525,  525,  525,  525,  689,  525,  471,  525,  525,  130,  882,  261,\n",
      "         525,  525,  525,  525,    1,  525,  882,  525,  525,  525,  525,  525,\n",
      "         525,  525,  525,  525,    8,  525,  525,  525,  882,  525,  525,  130,\n",
      "         525,  525,  525,  471,  525,  525,  525,  525,  261,  525,  525,  525,\n",
      "         525,  525,  261,  130,  525,  525,  525,  525,  882,  525,  525,  525,\n",
      "         525,  525,  130,  525])\n",
      "\ttensor([ 525,  525,    1,  130,  525,  130,  525,  525,   93,  131,  525,  169,\n",
      "         525,  525,   93,  525,  169,  525,  525,  525,  525,  525,  525,  525,\n",
      "         169,  525,    1,  525,  525,  525,  525,  169,  131,  169,  525,    1,\n",
      "         525,  525,   93,  525,  130,  525,   93,  169,  169,  525,  525,  169,\n",
      "         525,    1,  525,  525,  525,  169,  525,  525,  525,  525,  169,  525,\n",
      "         169,  169,  169,  525,    1,  525,  525,  169,  525,  525,  525,  525,\n",
      "         525,  169,  525,  525,  169,  169,  525,  525,  525,    1,  525,  525,\n",
      "         525,  525,  525,  169,  525,  525,  525,   93,  131,  525,  525,  525,\n",
      "           1,    1,  525,  169,  130,  118,  525,  525,  130,  130,  525, 1165,\n",
      "           1,  525,  525,    1,  525,  525,  130,  525,  525,  525,  525,  525,\n",
      "         525,  525,  525,  115,    1,  525,  115,  525,    1,    1, 1165, 1165,\n",
      "         525,  525,  525,  130,    1,  525,  525,  115,   29,  525,  525,  525,\n",
      "         525,  130,  525,    1,  130,  115, 1165,  525,  525,  525,  525,  118,\n",
      "        1165,  525,  525,  525,  525,  525,  525,  115,  525,  525,  525,  525,\n",
      "         525,  115,  525,  115,  525,  525,  525,  525,  525,   29,  525,  130,\n",
      "         525,  525,  525,  525,  525,  525,    1,  525,  115, 1165,  115,  525,\n",
      "         525, 1165,  525,    1,  525,  525,  525,  525,    1,    1,  518, 1893,\n",
      "         510,  518, 1893,  131,  525,  518,  518,  525,  169,  115,    1,  131,\n",
      "        1893,  525,  426,  525,    1,  131,  426,    1,  510,  579,    1,  426,\n",
      "         426,  426,    1,  518,  518,  518,    1,    1,  518,  518,  131,    1,\n",
      "         131,  131,  518,  131,    1,  518,  579,  518,  131,  510,    1,  518,\n",
      "           1,    1,  426,  518,  525,  525,  426,  426,    1,    1,    1, 1893,\n",
      "           1, 1893,  518,  131,  130,    1,  579,  518,  115,  518,    1,    1,\n",
      "         131,  525,    1,  131,  525,  518,  525,    1,  518,    1,  131,    1,\n",
      "         131,  131,    1,  115, 1893,    1,  510, 1893,  426,  525,  525,  510,\n",
      "         510,   91,  874,  518,    7,  525,  510,  579,  525, 1893,  518,  525,\n",
      "          91,  525,  525,  579,    1,  510,  525,   78,  704, 2059,    1,   91,\n",
      "         874,  525,  131,   78,  510,   60,  579,   91,  525,    7,  510,  525,\n",
      "         510,  371,  518,   91,  525,  579,  525,  518,  874,    1,  525,  525,\n",
      "          91,   60,  874,   91,    7,  131,  525,   91,   78,  510,  371, 2059,\n",
      "         704,   91, 1893,  579,  525,   91,   78,   60,  525,  371,   91,    1,\n",
      "         874, 1893,   91,  525,   60,  704,  148, 2059,  525,  510,  579,  510,\n",
      "        1893,   60,  525,    1,  510,  510,   78, 1893,  525,  510,  518,   91,\n",
      "         510,  510,    1,  579])\n",
      "\ttensor([525,   1,   7,   7, 115,   7, 525,   7, 525, 525, 525, 115,   7, 261,\n",
      "          7,   1,   7, 115, 525,   7, 525,   7,   7, 115,   7,   1, 261, 525,\n",
      "        115, 525, 525,   7, 525,   7,   1,   7, 525, 525,   7, 525,   7, 525,\n",
      "          7, 115, 115,   7,   7, 115, 525, 261, 525, 525,   7, 115, 115, 115,\n",
      "        261,   7,   7, 525,   7,   7,   7, 115,   7,   7, 525, 115, 525,   7,\n",
      "        525, 525, 525,   7, 525, 115,   7,   7,   7, 525, 115, 261, 261,   7,\n",
      "        525, 115,   1,   7, 525, 115, 525,   7, 525,   7, 525,   7, 261,   7,\n",
      "        261,   7, 525,   7,   7,   7, 525, 525,   7, 525, 115, 115, 525, 115,\n",
      "        115, 525, 525,   1, 159, 115,   7,   1,   7, 115, 115, 525, 159,   1,\n",
      "        525, 525, 115, 115, 525, 525, 159, 115, 525, 525, 115,   7,   7, 115,\n",
      "        131,   7, 525,   1,   1, 525, 115, 159, 525, 115, 525, 525,   7, 115,\n",
      "        525,   7, 525,   7, 115,   1, 525,   7,   1, 115,   1, 130,   7,   7,\n",
      "          7, 525,   1, 115, 115, 115, 525, 159,   7, 131, 525, 525,  93,   1,\n",
      "          7, 159, 115,   1, 115,   7, 115, 525, 115, 115, 525, 525, 525, 159,\n",
      "          1, 525, 159, 115, 115,   1, 525, 115,   1, 525, 115, 115, 525, 525,\n",
      "        525,   1,   7,   1, 115, 115, 115,   1,   7, 525,   1,   7,   7,   1,\n",
      "          1, 525,   1,   7, 115,   7,   1, 525, 525, 525, 525, 525, 525, 525,\n",
      "          7, 525, 115, 115, 525, 115, 525, 525, 525, 525, 115,   1,   1, 525,\n",
      "        115,   1,   7, 525, 525, 525, 115,   7,   1, 525, 130, 115, 525, 115,\n",
      "        525,   7,   1, 115, 525, 525, 131, 525, 115,   1,   7, 525, 525, 115,\n",
      "        130, 525, 525,   1, 525, 525, 115, 525, 115, 115, 525, 525, 115, 525,\n",
      "          1, 115, 115,   1, 525,   1,   7, 261,   7, 525,   7, 525,   7,   7,\n",
      "        525, 525, 525, 525,   7, 525, 525,   7,  93,   7, 525, 115, 525, 136,\n",
      "        525,   1,   7, 525, 525,   7,   7, 525,   7,   1, 525,   7,   7, 525,\n",
      "          7, 130, 525,   1, 136, 261, 525, 525,   7,  93, 525, 525,   1, 525,\n",
      "          7, 261,   7, 525, 525, 261,   7,   7, 525, 136, 525,   7, 525,   7,\n",
      "        525,   7,   7, 525, 525, 130, 261,  93,   7, 525,   7, 525, 525, 525,\n",
      "        525, 136, 525,   7,   7,   7, 525, 525, 525, 525,   7,   7,   7, 525,\n",
      "        525,   7, 525, 261,   7,   7,  93,   7])\n",
      "\ttensor([ 525,  131,    1,    7,  131,    7,  525,  525,  131,  131,  872,    1,\n",
      "           7,  525,  241,  131,  131,  131,    7,    7,   93,  525,  525,  525,\n",
      "         131,  131,  131,    7,  131,  525,    1,  131,  131,  131,  131,    1,\n",
      "           7,  872,  241,  872,    7,  872,  241,   93,    1,  525,  525,    1,\n",
      "           7,  131,  525,    7,  525,    1,  525,  131,  525,  525,  131,    7,\n",
      "         131,  131,  131,  525,    1,  525,  872,    1,    7,  525,   93,    7,\n",
      "           1,  131,    1,  131,  131,  131,  525,   93,  131,  131,  525,    7,\n",
      "         525,  525,  131,  131,    7,  525,  872,  241,  131,    7,    7,  525,\n",
      "         131,  525,  131,  131,    7,   28,  115,    7,  689,    7,    7,    7,\n",
      "         525,  525,  131, 1383,  525,    1,    7,  131,  525,  525,    7,  131,\n",
      "         115,  525,  525,  130,  131,  525,    7,    1,  525,  525,    7,    7,\n",
      "         525,  525,    1,    7,  525,    1,    7,  525,  525,  115,  872,  131,\n",
      "         131,    7,  525,  131,  689,  525,    7,    1,    7,  525,    1,   28,\n",
      "           7,    1,  525,  525,  131,    7,  131,  525,  131,    7,    1,    7,\n",
      "           7,    7,  131,  525,  525,  525,    1,  525,    7,  525,  872,    7,\n",
      "         525,  131,    1,  525,  525,  131,  525,    1,  525,    7,  525,  525,\n",
      "         131,    7,    1,  131,  131,    1,  525,  525,  525,  525,  689,  131,\n",
      "         525,  689,  131,  525,  131,  525,  525,   93,  115,  525,  525,  525,\n",
      "         131,   93,  131,  131,  525,  525,  131,  525,  525,  525,  525,  131,\n",
      "         131,  131,  525,  689,  525,  689,  241,  525,  525,  689,  525,  525,\n",
      "         525,  525,  525,  525,  525,  689,  525,  689,  525,   93,  525,  525,\n",
      "         525,  689,  131,  525,  525,  131,  131,  131,  525,  241,    1,  131,\n",
      "         525,  131,  525,  525,  131,  131,  525,  525,    7,  525,  525,  689,\n",
      "         525,  525,  525,  525,  375,  689,  131,  525,  525,  241,  525,   60,\n",
      "         525,  525,  525,  689,  131,  525,  525,  131,  131,   93,  131,  525,\n",
      "         131,  525,  261,  525,    1,    7,  131,  131,    1,    7,  525,  525,\n",
      "         525,    1,  525,  131,  525,  131,    1,    7,    7,    1,  525,  525,\n",
      "         261,    1,  131,  131,  131,    7,  131,  525,    7,    1,  131,   93,\n",
      "         131,  131,  525,  525,  525,    1,    7,    7,    7,  525,    1,  525,\n",
      "         525,  525,    7,  525,    1,  131,    1,  525,  131,  131,  131,    1,\n",
      "           7,  525,    7,  131,  525,  525,  131,    7,    1,  131,  525,  525,\n",
      "           7,    7,  525,    7,  525,    7,    1,    1,  525,  131,  131,  131,\n",
      "           7,    7,  525,  525,  131,  131,  131,    7,   93,  131,  525,  525,\n",
      "         131,  131,  525,  131])\n",
      "\ttensor([ 131,  136,    7,  131,   93,  131,  131,    7,  131,   93,    1,    7,\n",
      "           1,   60, 2443,  136,  525,   93,    1,    1,  131,    7,  136,    1,\n",
      "         525,  136,   60,    1,    1,  131,  131,  525,   93,  525,  136,    7,\n",
      "           1,    1, 2443,    7,  131,    7, 2443,    7,    7,  136,    1,    7,\n",
      "           1,   60,  131,    1,   28,   93,    1,   93,   60,  131,  525,    1,\n",
      "         525,  525,  525,    1,    1,  136,    7,   93,    1,   28,  131,    1,\n",
      "         131,  525,  131,    1,  525,  525,    1,  131,   93,  241,   60,    1,\n",
      "         131,    1,  136,  525,    1,    1,    1, 2443,   93,    1,    1,   28,\n",
      "          60,  241,  131,  525, 1203,    1,  131,  115,  131, 1203,    7, 1203,\n",
      "        1203,   29,    1,    7,    1,  131, 1203,    1,  525,   29,    7,    1,\n",
      "         131,    1,  131,    1,  131,  136,   28,  131,  525,  525, 1203, 1203,\n",
      "         525,   29,  131, 1203, 1203,    7,    7,  131,  525,  131, 1383,    1,\n",
      "           1, 1203,    1,  131,  131,  131, 1203,  131,    7,   29,  131,    1,\n",
      "        1203,    7,   29,  136,    1,    7,    1,  131,    1,  136,    7,    7,\n",
      "           7,   28,    1,  131,  261,   29,  131,    7,    7,  525, 1383, 1203,\n",
      "         131,    1,    7,  525,   29,    1,  525,    7,  131, 1203,  131,   29,\n",
      "           1, 1203,  131,  131,    1,  131,  525,   29,  131,    7,    7,  525,\n",
      "         131,    7,  525,    1,  131,  115,    7,  136,    7,  131,  525,    1,\n",
      "         525,  136,  261,  131,    7,  136,   28,    7,  131,    1,    7,   28,\n",
      "         131,   28,    7,    7,    7,    7,  131,  525,    7,    7,  136,  525,\n",
      "           1,    7,    7,  525,  525,    7,    7,    7,   93,  131,    7,    7,\n",
      "         525,    7,  261,  115,  131,  131,  131,   28,    7,  131,  115,  525,\n",
      "         525,  525,  115,  136,  704,    1,    1,    7,  131,    7,  131,    7,\n",
      "         131,  131,  525,    1,  136,    7,  131,    7,    7,  131,    1,  525,\n",
      "           1,  525,  525,  131,  525,  318,  131,  525,  136,  136,  131,  131,\n",
      "         169, 1203,  130,  136,    1,  131,  169,  131,  131,    1,  136,  131,\n",
      "          93,  131,  131,  131,  525,  169,  131,  131,    7,    1,  525,  131,\n",
      "         130,  131,    1,  131,  169,  525,  131,  131,  131,    1,  131,  131,\n",
      "         169,  131,  136,  131,  131,    1,  131,  525,  131,  525,    7,  131,\n",
      "         131,  131,  136, 1203,  525,    1,    7, 1203,  131,  169,  131,    1,\n",
      "           7,  525,    1,  131,  131,  115,  131,  525,    7,  131, 1203,  525,\n",
      "         131,    1,    1,  131,  131,    7,    1,    1,  131,  169,  131,  136,\n",
      "           1,  525,  131,  525,  169,  130,  131,    1,  131,  169,  136, 1203,\n",
      "         169,  136,  525,  131])\n",
      "\ttensor([ 525,    1,    7,   93,  525,    1,  525,  131,  131,    7,  131,  131,\n",
      "         136, 1203,  115,    1,  525,  136,    1,  131,  136,  131, 1722,   28,\n",
      "         136,   93,  136,  525,  131,  525,    7,  131,    7,  131,   93,    7,\n",
      "           1,  131,  131, 1203,    1, 1203,  115,    7,  131, 1722,  525,  131,\n",
      "           7,  136,  525,   93,  261, 1203,   28,  525, 1203,    1,  131,    7,\n",
      "         131,  136,  525,   28,  525, 1722, 1203, 1203,    7,   28,  136,    7,\n",
      "          93,  136,    1,  525,  136,  136,  525,  136,  136,    1, 1203,  131,\n",
      "         525,   28,   93,  525,    7,   28,  131,    7,    7,  131,    7,   28,\n",
      "         136,  131,    7,  131,  689,    1,  131,  131,  131,    1,  131, 2034,\n",
      "         136,   28,  131,  131,  136,  241,    7,  169,  241,   28,  131,  689,\n",
      "         131,  131,  525,    7,  131,  131,   28,  241,    1,    1, 2443, 2443,\n",
      "        1203,   28,  241,    7,  136,    1,  131,  131,  525,  131,    1,    1,\n",
      "          93,    1,  136,  131,  131,  525, 2443,  241,  131,   28,  241,    1,\n",
      "        2034,    1,   28,  131,  131,  525,  169,  131,  136,  131,    1,  131,\n",
      "         131,   28,  159,  131,  525,  131,  241,  131,  131,  131,   93,    7,\n",
      "         115,  159,    1,  241,   28,  169,    1,    1,  131, 2034,  131,  261,\n",
      "         131, 2443,  241,  131,   93,  241,  241,   28,  131,  169,  525,    1,\n",
      "           7,   93,    1,    1,    7,   93,   93,  131,  241,  131,    1,    1,\n",
      "           1,  131,  136,  115,    7,    1,    1,    7,    1,  131,  169,    1,\n",
      "           1,    1,    7,  525,  115,    7,  525,  131,    7,  130,    1,  131,\n",
      "           1,    7,   93,  131,  131,    7,   28,    1,  131,    7,  704,  689,\n",
      "           1,  131,  136,   93,  131,    7,    1,    1,    7,  525,  525,    1,\n",
      "         131,    1,   93,    1,  525,  525,  136,  689,  136,    7,  131,  131,\n",
      "         131,  131,  131,   93,    1,   93,    1,    7,  115,  525,    1,   93,\n",
      "           1,  131,  131,  131,    1,    1,    1,    1,    1,  131,    7,    1,\n",
      "           7,    7,    1,    1,  136, 2443,   93,  241,  525,    1,    1,  131,\n",
      "         131,  525,  131,  241,  525,   93,  525,    1,    7,  131,  131,    1,\n",
      "          93,  525,    1,    7,   93,  136,  241,  525, 2034,  136,  131,  525,\n",
      "           7,  525,    1,  525, 1203,  525,    7,    1,  115,  525,  131,  131,\n",
      "         525,  131,    1, 1722,  525,  115,  131,  115,  115,   93,  131,  131,\n",
      "           7,    7,  115,    1,    1,  131,  115,  131,  131,  525,  115,  136,\n",
      "         115,    1,    1, 2034,  131,  115,    1,  525,  131,   93,    1,  131,\n",
      "         115,  131,  131,  131,   93,  131,   93,    1,    1,   93,    1,  115,\n",
      "          93,  131,  136,  241])\n",
      "\ttensor([  131,   131,   131,   131,   131,   131,   525,     7,   131,   131,\n",
      "          131,     1,   131,     7,   131,   136,     7,   131,     7,     7,\n",
      "            1,   115,   525,   525,     1,  1203,   525,     7,     7,     7,\n",
      "          131,     7,   136,     1,    60,   136,     1,   136,   131,   136,\n",
      "          136,   525,   136,   131,   525,   136,     7,   136,   169,   136,\n",
      "          136,   525,     1,   131,   689,   525,   115,   241,    93,   872,\n",
      "          159,    93,   115,     1,     1,     1,     1,   136,   136,     1,\n",
      "          136,   525,   131,     7,   241,     1,   136,    29,     1,   131,\n",
      "          136,    60,     1,   115,   130,   136,   131,     1,   131,   130,\n",
      "            1,     1,   525,   131,   689,    29,     1,     1,     1,  1722,\n",
      "          131,     1,     1,   131,     1,     7,   131,     7,   131,   131,\n",
      "            7,     1,   131,     1,   525,   525,     1,     1,   689,   525,\n",
      "            7,   131,   525,   525,   131,   525,   131,     7,     1,   872,\n",
      "          136,   525,   131,   136,   131,   136,   136,     1,     7,   136,\n",
      "            7,   115,     1,     1,   525,     1,   136,   136,     7,   131,\n",
      "            1,   136,   136,     7,   169,     7,   115,     7,   872,     1,\n",
      "          115,     7,   136,   525,   131,     7,   689,     7,    90,   136,\n",
      "            7, 12072,     1,     7,    93,     1,     1,     1,     7,   689,\n",
      "          241,     1,    93,   136,   169,     1,     7,   426,   131,     1,\n",
      "         1383,     7,     1,   471,    29,   525,     1,   689,   131,    95,\n",
      "          131,   131,   131,   525,   131,   131,   136,   525,   131,     1,\n",
      "            1,     7,   525,   525,   131,     1,   131,   241,     7,     7,\n",
      "          131,   525,   136,   689,     7,   131,   136,   525,     7,   131,\n",
      "            7,   136,     1,     7,     1,     1,     7,     7,     1,     7,\n",
      "          136,   131,     7,   525,   131,   131,   131,     1,     1,   131,\n",
      "          689,     1,   136,     7,   115,   136,   131,   525,   115,   169,\n",
      "          136,     7,     7,     1,   115,   169,   131,   136,     1,     1,\n",
      "            1,   525,   131,     1,   136,   689,     1,   136,   136,   525,\n",
      "          525,   136,   525,    90,     7,   115,    93,   689, 12072,   136,\n",
      "           93,   131,   130,     1,   131,   689,  1203,   525,   136,     1,\n",
      "          131,   131,   131,   525,   131,   131,   525,     1,   131,   131,\n",
      "          241,   525,   131,     1,   136,     7,   131,   136,   136,   131,\n",
      "            7,     1,   525,    28,   131,   525,     7,   131,     1,   131,\n",
      "          115,   872,     7,   136,   131,   131,   136,     1,  1203,     7,\n",
      "          131,     1,   131,     1,   525,   136,     7,   689,     1,   131,\n",
      "            7,     7,   131,     1,     1,     1,   131,   689,     1,     7,\n",
      "          115,     1,     1,   131,     1,     7,   136,   131,   115,   515,\n",
      "           93,     1,     1,   525,   131,   115,   136,   525,   525,     1,\n",
      "          131,   131,   136,     7,    93,   136,   471,   136,    93,     7,\n",
      "            7,   136,     1,    29,    60,    29,   130,     1,    93,    93])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([90, 90, 30, 16, 75, 42, 95, 32, 43,  6, 16,  1, 77, 75,  0, 97, 21, 42,\n",
      "        19, 39, 43, 19,  6, 80, 64, 68, 85, 96, 21, 85,  0, 85, 39, 53, 86, 68,\n",
      "        16, 96, 98, 99, 85, 96, 43, 76, 89, 10,  6, 93, 68, 25, 87, 68, 61, 96,\n",
      "        96, 96, 64,  9, 50, 43,  1, 96, 67, 19, 68, 76, 42, 71, 77, 72, 93, 99,\n",
      "        80, 91, 81, 13, 64, 67, 33,  0, 84, 68,  1, 47, 51, 77, 93,  1, 43, 94,\n",
      "        62,  6, 96, 23, 71, 23, 77, 98,  1, 68])\tToken idx: tensor([ 131,  525,  131,    1,  241,  131,  525,  131,    7,    7,    7,    1,\n",
      "         241,  525,    7, 1732,    7,  136,  525,  525,  115,    1,    1,    7,\n",
      "         131,    1, 1203,    1,    1,   60,    1, 3460,    1,  525,  525,   93,\n",
      "         115,   29,    7,  525,  241,   93,   88,    7,  525,    1,  115,    1,\n",
      "           7,    7,    7,  689,    7,  689,  136,  169,  136,  131,    1, 1722,\n",
      "         115,    7,    7,  136,   29,  115,  525,    7,   60,  131,    7, 1203,\n",
      "         115,    1,  525,  131,  525,    1,    1,  115,  525,  426,    7,  525,\n",
      "         131,   72,  131, 1722,  241,    1,  131,  689,  131,    1,    1,    7,\n",
      "        1203,  115,   93,  136])\tRaw indices: indices_to_process=tensor([3001901, 3002295, 1000721,  533649, 2501716, 1400957, 3169060, 1067427,\n",
      "        1434186,  200125,  533655,   33354, 2568422, 2502000,       7, 3236973,\n",
      "         700420, 1400962,  634232, 1301292, 1434294,  633708,  200119, 2668247,\n",
      "        2134723, 2268005, 2836208, 3201889,  700414, 2835065,       1, 2838465,\n",
      "        1300768, 1768234, 2868883, 2268097,  533763, 3201917, 3268601, 3302472,\n",
      "        2835246, 3201981, 1434267, 2534835, 2968942,  333531,  200233, 3101830,\n",
      "        2268011,  833832, 2901718, 2268693, 2034540, 3202577, 3202024, 3202057,\n",
      "        2134728,  300308, 1667651, 1435901,   33468, 3201895, 2234658,  633843,\n",
      "        2268033, 2534943, 1401351, 2368070, 2568241, 2401547, 3101836, 3303150,\n",
      "        2668355, 3035124, 2702118,  433720, 2135117, 2234652, 1100650,     115,\n",
      "        2802177, 2268430,   33360, 1568116, 1701134, 2568253, 3101960,   35075,\n",
      "        1434420, 3135183, 2068017,  200807, 3202019,  767120, 2368064,  767126,\n",
      "        2569384, 3268709,   33446, 2268140])\n",
      "Beam idx: tensor([91, 28, 57, 11, 57, 24, 88, 20,  7, 24, 84, 59, 67, 45, 63, 36, 29,  7,\n",
      "        24, 84, 75,  7, 47, 39, 76, 15, 36, 36, 37,  7, 54, 67,  0, 36, 82,  3,\n",
      "        91, 85,  2, 96, 10,  2, 74, 67, 13, 84, 92, 11, 92,  0, 28, 10, 60, 54,\n",
      "        45, 74, 75, 11, 79, 92, 24, 76,  1, 61, 43, 89, 34, 37, 36,  9, 60, 47,\n",
      "         7, 74, 53,  3, 85,  8, 16, 94, 35, 82,  9, 67, 75, 36,  1, 76, 24, 81,\n",
      "        92, 65,  0, 11, 59,  7, 45,  9, 63, 43])\tToken idx: tensor([ 131,    1,  525,    7,  131,    7,  525,  131,    7,  115,    1,  131,\n",
      "           1,  525,    7,    7,  131,   28,    1,    7,  131,    1,  131,  131,\n",
      "         525,    1,  515,    1,  136, 1722,  131,  131,    7,  115,  525,    1,\n",
      "         525,  525,    1,    1,  136,    7,  525,  136,    1,  115,  131,  689,\n",
      "         525,    1,    7,  131, 1203,  136,  131,    1,  525,    1,    7, 1722,\n",
      "        1722,    7,    7,  136,  136,  525,    1,  131, 1383,  136,   60,    7,\n",
      "          93, 1722,    1,    7,    7,    1,    7,   28,    1,  136,  525,  525,\n",
      "           7,   93,    1,  130,  241,    7,    7,    1,  115,   90,  136,  689,\n",
      "         136,    1,  115,  131])\tRaw indices: indices_to_process=tensor([3035254,  933885, 1901646,  366890, 1901252,  800479, 2935589,  667191,\n",
      "         233478,  800587, 2801653, 1967958, 2234652, 1501410, 2101246, 1200715,\n",
      "         967368,  233499,  800473, 2801659, 2501606,  233472, 1567722, 1300898,\n",
      "        2535353,  500296, 1201223, 1200709, 1234197,  235193, 1801193, 2234782,\n",
      "              7, 1200823, 2735471,  100060, 3035648, 2835530,   66707, 3201889,\n",
      "         333666,   66713, 2468647, 2234787,  433590, 2801767, 3068607,  367572,\n",
      "        3069001,       1,  933891,  333661, 2002383, 1801198, 1501016, 2468123,\n",
      "        2502000,  366884, 2634894, 3070198,  802194, 2534835,   33360, 2034669,\n",
      "        1434315, 2968942, 1134003, 1234192, 1202091,  300313, 2001240, 1567598,\n",
      "         233564, 2469844, 1767710,  100066, 2835012,  266825,  533655, 3135210,\n",
      "        1167356, 2735082,  300702, 2235176, 2501482, 1200801,   33354, 2534958,\n",
      "         800713, 2701600, 3068483, 2167946,     115,  366973, 1967963,  234160,\n",
      "        1501021,  300178, 2101354, 1434310])\n",
      "Beam idx: tensor([43, 32, 80, 73, 28, 62, 62, 90, 95, 19, 86, 62,  4, 92, 65, 10, 20,  0,\n",
      "        20, 28, 24, 62, 12, 18, 21, 16, 65, 20, 62, 86, 42, 42,  5, 62, 12, 62,\n",
      "        57,  4, 14, 90,  4, 72, 27, 72, 18, 95, 10, 74, 62, 18, 21, 74, 86, 42,\n",
      "        27, 18, 97, 85, 39, 62, 12, 62, 42, 91, 62,  1, 81, 27, 62, 38, 26, 16,\n",
      "        46, 81,  0, 42, 24, 28, 34, 24, 28, 62, 57,  5, 53, 42, 42, 28,  3,  4,\n",
      "        10, 16, 28,  8, 18, 36, 72, 89, 63, 65])\tToken idx: tensor([ 525,  131,  131,  525,  131,    1,    7,    1,  525,    7,    7,  131,\n",
      "           7,  131,    7,    7,    7,    7,    1,    7,    1,  115,    7, 1722,\n",
      "         131,    1,    1,  115,  136,    1,    1, 1722,    7,   93,    1, 1722,\n",
      "           7,  115,  131,  136,    1,  115,  136,    7,   88,  131,    1,   60,\n",
      "          88,    1,  136, 1203,  115,   88,  131,    7,    1,    1,    7, 1469,\n",
      "         115,   28,  115,    1,  159,    1,  525,  525,  689,    1,    7,  115,\n",
      "         131,   28,  115,    7,   93,  115,  525,  115,    1, 1318,    1,    1,\n",
      "         136,   93,  525,  136,    7,  241,  689,    7,   93,  131,   28,    1,\n",
      "           1,    1,    1,  115])\tRaw indices: indices_to_process=tensor([1434704, 1067427, 2668371, 2435294,  934015, 2067887, 2067893, 3001771,\n",
      "        3169060,  633714, 2868365, 2068017,  133419, 3068607, 2167952,  333537,\n",
      "         667067,       7,  667061,  933891,  800473, 2068001,  400243,  602076,\n",
      "         700544,  533649, 2167946,  667175, 2068022, 2868359, 1400827, 1402548,\n",
      "         166772, 2067979,  400237, 2069608, 1901128,  133527,  467073, 3001906,\n",
      "         133413, 2401531,  900667, 2401423,  600442, 3168666,  333531, 2468182,\n",
      "        2067974,  600355,  700549, 2469325, 2868473, 1400914,  900662,  600361,\n",
      "        3235242, 2835006, 1300774, 2069355,  400351, 2067914, 1400941, 3035124,\n",
      "        2068045,   33354, 2702118,  901056, 2068575, 1267415,  867185,  533763,\n",
      "        1534369, 2701621,     115, 1400833,  800565,  933999, 1134527,  800587,\n",
      "         933885, 2069204, 1901122,  166766, 1767845, 1400919, 1401351,  934020,\n",
      "         100066,  133653,  334219,  533655,  933977,  266955,  600382, 1200709,\n",
      "        2401417, 2968418, 2101240, 2168060])\n",
      "Beam idx: tensor([36, 36, 57, 91,  8, 65, 73, 47, 16, 42,  0, 56, 47,  7, 62, 40, 66, 94,\n",
      "        36,  0, 79, 92,  4, 67, 97,  2, 14,  8,  4,  8, 81, 55, 40, 47, 33, 56,\n",
      "        78, 73, 61, 22, 22, 64, 43,  2, 42, 40, 20, 62, 46, 68, 31,  4, 46, 14,\n",
      "        64, 11, 45, 56, 42,  8, 61, 60, 18, 76, 95, 39,  7, 20, 80, 93,  0, 80,\n",
      "        47, 19, 74, 26, 73,  0, 78, 30,  7, 18, 29,  2, 47, 30, 31, 40, 37, 74,\n",
      "        13, 22, 74, 14, 56, 47, 11, 66, 46, 94])\tToken idx: tensor([  131,   525,   131,   131,     7,   131,     1,   241,   131,   525,\n",
      "            7,     1,     7,     7,     1,     1,   131,   525,   136,     1,\n",
      "         1732,     1,     7,     7,   131,     7,     7,     1,     1,   115,\n",
      "            1,   241,     7,   525,     1,   115,   241,     7,   131,   525,\n",
      "            1,     7,   131,     1,     1,   115,     7,   136,   525,     1,\n",
      "          525,   115,     1,     1,   115,     7, 12072,    93,   136,    93,\n",
      "          525,     1,     7, 12072,     1,   131,     1,   115,     1,     7,\n",
      "           93,     7,   115,     1,     1,     1,   115,   115,  1203,     7,\n",
      "          689,     1,     1,   115,   130,     1,   136,    93,     1,   131,\n",
      "            1,   136,   136,   115,     7,    72,   115,   136,   136,   131])\tRaw indices: indices_to_process=tensor([1200839, 1201233, 1901252, 3035254,  266831, 2168076, 2434770, 1567832,\n",
      "         533779, 1401351,       7, 1867769, 1567598,  233478, 2067887, 1334121,\n",
      "        2201429, 3135707, 1200844,       1, 2636619, 3068477,  133419, 2234658,\n",
      "        3235372,   66713,  466949,  266825,  133413,  266939, 2701594, 1834656,\n",
      "        1334127, 1568116, 1100650, 1867883, 2601775, 2434776, 2034664,  734291,\n",
      "         733767, 2134599, 1434310,   66707, 1400827, 1334235,  667067, 2068022,\n",
      "        1534763, 2268005, 1034468,  133527, 1534239,  466943, 2134707,  366890,\n",
      "        1512957, 1867861, 1400962,  266917, 2035058, 2001181,  600361, 2546900,\n",
      "        3168536, 1300898,  233472,  667175, 2668241, 3101836,      93, 2668247,\n",
      "        1567706,  633708, 2468123,  867179, 2434884,     115, 2602737, 1000597,\n",
      "         234160,  600355,  967238,   66821, 1567721, 1000591, 1034079, 1334213,\n",
      "        1234062, 2468253,  433590,  733902, 2468258,  467057, 1867775, 1567663,\n",
      "         366998, 2201434, 1534374, 3135313])\n",
      "DEB: t.shape=torch.Size([8, 400])\n",
      "tensor=(\n",
      "\ttensor([3694, 3694,  131,    7,  235,  159,  322,  118,    7,  305,    7,   60,\n",
      "           7,  235,  305,    7,  322,  159,  235, 3694,    7,  235,  305,  235,\n",
      "           7, 2416,   50,    1,  322,   50,  305,   50, 3694,    7,   60, 2416,\n",
      "           7,    1,  447,    7,   50,    1,    7,    7,   50, 3694,  305,  235,\n",
      "        2416,   60,    7, 2416,    7,    1,    1,    1,    7,  118,  305,    7,\n",
      "          60,    1,    7,  235, 2416,    7,  159, 2416,    7,  131,  235,    7,\n",
      "         235,  159,    1,  235,    7,    7,    7,  305,  305, 2416,   60,    7,\n",
      "        2416,    7,  235,   60,    7, 2416,    7,  305,    1,   50, 2416,   50,\n",
      "           7,  447,   60, 2416,  528,   93,  528,  130,  528,  130,   29,  471,\n",
      "           1,  130,  528,  343,  119, 1722,   29,   93,   93,    1,  130,  528,\n",
      "         119,    1,  130,   29,  119,  998,   93,   93,  528,    1,  518,  119,\n",
      "        1722,   93,  528,  471,  528,  998,  471,  998,  343,  471,  518,  119,\n",
      "         518,  528,  343,  130,  343, 1722,   93,  343,  343,  518, 1722,  518,\n",
      "         119,  130, 1722,  343,  130,  119,    1,  119,  998,    1,  518,  528,\n",
      "          93,  528,  343,  130,    1,  518,  528,  471,  998,   93,  119,  518,\n",
      "        1722,  528,  528,  119,  119,   93,    1,  119,  130,  998,  343, 1346,\n",
      "        1722,  130,  343,    1, 1722,  528,   29,  998,  136,  525,  205,  525,\n",
      "         525,  131,  131, 1023,  525, 1491,  136,  131,  525,  525,  525,  525,\n",
      "         131,  130,  131,  525,  525,  131,  525,  525,  136,  525,  525,  131,\n",
      "         131,  136,  525,  525,  525,  131,  525,  131, 1491,  525,  130, 1023,\n",
      "         525,  525,  525,  525,  525,  525,  525,  130,  131,  525,  136,  130,\n",
      "         136,  525,  525,  525,  431,  131, 1023,  131,  525,  131,  525,   93,\n",
      "         131,  131,  525,  525,  131,  136,  131,  525,  525,  525,  130,  525,\n",
      "         525,  525,  131,  525,  525,  131, 1491,  525,  131,  525,  525,  525,\n",
      "         525,  525,  525,  525,  525,  426,  525,  525,  525,  136,  525,  525,\n",
      "         525,  525,  525,  525,    8,  525,  525,  261,  130,  471,  525,  525,\n",
      "         261,  525,  525,  689,  525,  525,  525,  525,  525,  882,    1,  525,\n",
      "         525,  525,  261,    8,    1,    8,  525,  525,  689,  261,    1,  525,\n",
      "         525,  525,  525,  130,  130,    8,  525,  525,  471,  689,  525,  525,\n",
      "         882,  882,  525,    1,  882,  261,    8,  261,  130,  525,  471,    8,\n",
      "         525,  525,    8,  525,  525,  525,  525,  525,  261,  525,  525,  261,\n",
      "         261,  525,  525,  525,  525,  525,  525,  525,  525,    8,  525,  525,\n",
      "         261,  525,  525,  689,  525,  525,    8,  130,  525,  261,  525,  261,\n",
      "         261,  525,  882,  525])\n",
      "\ttensor([ 525,  525,  525,  169,  525,   93,  525,  131,  169,  525,  169,  525,\n",
      "         169,  525,  525,    1,  525,   93,  525,  525,  169,  525,  525,  525,\n",
      "           1,  525,  525,    1,  525,  525,  525,  525,  525,  169,  525,  525,\n",
      "         169,    1,  525,  169,  525,    1,  169,  169,  525,  525,  525,  525,\n",
      "         525,  525,  169,  525,  169,    1,    1,    1,    1,  131,  525,  169,\n",
      "         525,    1,  169,  525,  525,  169,   93,  525,  169,  525,  525,  169,\n",
      "         525,   93,    1,  525,    1,  169,  169,  525,  525,  525,  525,  169,\n",
      "         525,  169,  525,  525,  169,  525,  169,  525,    1,  525,  525,  525,\n",
      "         169,  525,  525,  525,  525,    1,  525,    1,  525,    1,  115,  525,\n",
      "        1165,    1,  525,  525,  525,  130,  115,    1,    1, 1165,    1,  525,\n",
      "         525, 1165,    1,  115,  525,  525,    1,    1,  525, 1165,  525,  525,\n",
      "         130,    1,  525,  525,  525,  525,  525,  525,  525,  525,  525,  525,\n",
      "         525,  525,  525,    1,  525,  130,    1,  525,  525,  525,  130,  525,\n",
      "         525,    1,  130,  525,    1,  525,  118,  525,  525, 1165,  525,  525,\n",
      "           1,  525,  525,    1, 1165,  525,  525,  525,  525,    1,  525,  525,\n",
      "         130,  525,  525,  525,  525,    1,  118,  525,    1,  525,  525,  525,\n",
      "         130,    1,  525, 1165,  130,  525,  115,  525,  131,  518,  525,  518,\n",
      "         426,    1,    1,    1, 1893,  525,  131,    1,  510, 1893, 1893,  518,\n",
      "           1,    1,    1,  426,  510,    1,  169,  426,  131, 1893, 1893,    1,\n",
      "           1,  131,  518,  518,  518,    1,  169,    1,  525,  510,    1,    1,\n",
      "         510,  115,  426,  115,  426, 1893,  518,    1,    1,  426,  131,    1,\n",
      "         131,  518,  426,  426,  525,    1,    1,    1,  169,    1,  518,  115,\n",
      "           1,    1,  518,  426,    1,  131,    1, 1893,  579,  518,    1,  518,\n",
      "         510,  426,    1,  510,  426,    1,  525,  518,    1,  518,  518,  426,\n",
      "        1893,  510,  518, 1893,  426,  525,  426,  518,  115,  131, 1893, 1893,\n",
      "         510,  510,  510, 1893,  525,   91, 1893,  525,    1,  525,  510,   78,\n",
      "         525,  579, 1893,  525,   78,  518,  510,  510, 2059,  525,    7,   60,\n",
      "         510,  874,  525,  525,    7,  525,  510,   91,  525,  525,    7,   78,\n",
      "         148, 1893,   91,    1,    1,  525,  518,  874,  525,  525,  704, 1893,\n",
      "         525,  525,   91,    7,  525,  525,  525,  525,    1,   78,  525,  525,\n",
      "          91,  704,  525,   60,   91,   91,  579,  704,  525,  510,  510,  525,\n",
      "         525,   78,   91,  131, 1893,  510,  148,  579,  579,  525,   60,  874,\n",
      "         525,  579,   91,  525,  371,   91,  525,    1,   91,  525,   78,  525,\n",
      "         525,   78,  525,  518])\n",
      "\ttensor([525, 525, 525,   7, 115,   7,   7, 525, 115, 525,   7,   1,   7, 115,\n",
      "        525,   7,   7,   7,   7, 525, 115,   7, 525, 115,   7, 525, 115, 261,\n",
      "          7, 115, 525, 115, 525, 115,   1, 525,   7, 261, 261,   7, 115, 261,\n",
      "        115,   7, 115, 525, 525,   7, 525,   1,   7, 525,   7, 261, 261, 261,\n",
      "          7, 525, 525, 115,   1, 261, 115,   7, 525,   7,   7, 525,   7, 525,\n",
      "          7,   7, 115,   7, 261, 261,   7, 115,   7, 525, 525, 525,   1, 115,\n",
      "        525,   7,   7,   1, 115, 525,   7, 525, 261, 115, 525, 115,   7, 261,\n",
      "          1, 525, 115, 115,   7, 115,   7, 159, 115,   7, 525, 159, 115,   1,\n",
      "          7, 525, 115, 115, 115, 525, 159, 115, 159, 525, 159, 115,   7,   1,\n",
      "        115, 115,   7, 525, 525,   7, 525, 115,   7,   7, 115,   1,   7,   1,\n",
      "        525,   7, 525,   7, 525, 115, 525, 115, 525, 525, 115, 525, 525, 525,\n",
      "        525, 525, 159, 115, 525, 525, 159,   7,   7,   7,   1, 525, 525,   7,\n",
      "        115, 115, 525, 159, 525, 525, 115,   7,   1, 115, 159, 525, 525,   7,\n",
      "        115,   7, 159, 115,   7,   7, 159,   1, 525, 130, 525, 115,   1, 525,\n",
      "        525, 115, 115,   1, 115, 525, 130, 525, 115, 130, 130, 525, 115, 525,\n",
      "        115, 130,   1, 115, 115, 525,   1, 115,   1, 115,   1, 130,   7,   7,\n",
      "          7, 115, 115,   1, 130, 115, 525, 525, 525, 130,   7, 130, 525,   1,\n",
      "        115, 525,   1, 131,   7, 131,   7, 115, 525, 115, 130,   7,   7, 115,\n",
      "        115, 525,   7,   7,   1, 525, 525, 130,   7, 130, 525, 525, 130,   1,\n",
      "        525,   7, 130,   7,   1, 115, 525, 525, 115, 525,   1, 115, 525,   1,\n",
      "        115, 130, 525, 525,   1, 525, 525, 115, 115,   1, 525, 115, 115, 525,\n",
      "          7, 525, 131, 115, 115, 115,   7,   7,   7, 525, 525,   7, 525, 525,\n",
      "         93, 525,   7,   7, 525,   7, 525, 136,   7, 525,   7,   7, 136, 525,\n",
      "          7, 525,   7,   7, 525, 525,   7, 525,   7, 261, 136, 525,   7,   7,\n",
      "        525, 525,   7, 525, 525, 525, 525,   7, 525, 136, 525, 525, 525, 525,\n",
      "          1,   7, 525, 525, 525, 525,  93,   7, 525, 525,   7, 525, 525, 525,\n",
      "        261,   1,   7, 525, 525,   7,   7, 525, 525, 115,   7, 525, 525,   7,\n",
      "        525,   7,   7, 525, 525,   7, 525,   7,   1, 136, 130,   7, 525, 525,\n",
      "          7, 525,   7, 525, 525,   7, 525, 525])\n",
      "\ttensor([ 872,  872,    1,  131,  131,  241,  525,  131,   93,  525,  131,  131,\n",
      "         131,  131,  525,  525,  525,  241,    7,  872,   93,    7,  525,  131,\n",
      "           1,    7,  525,  131,  525,  525,  525,  525,  872,    1,  131,    7,\n",
      "         131,  131,  131,  131,  525,  131,   93,  131,  525,  872,  525,    7,\n",
      "           7,  131,  131,    7,  131,  131,  131,  131,    1,  131,  525,   93,\n",
      "         131,  131,    1,    7,    7,  131,  241,    7,  131,    1,    7,  131,\n",
      "         131,  241,  131,  525,    1,    1,  131,  525,  525,    7,  131,    1,\n",
      "           7,  131,    7,  131,   93,    7,  131,  525,  131,  525,    7,  525,\n",
      "         131,  131,  131,    7,  525,  525,    1, 1383,    1,  131,  525,  115,\n",
      "           7,  131,  525,  525,    7,    7,  525,  525,  525,    7,  131,  525,\n",
      "         525,    7,  131,  525,    7,  131,  525,  525,    1,    7,    1,    7,\n",
      "           7,  525,    1,    7,  525,  131,  115,  131,  131,  115,    1,    7,\n",
      "           1,  525,  131, 1383,  131,    7,  525,  131,  131,    1,    7,    1,\n",
      "         525, 1383,    7,  131,  131,    7,   28,    7,  131,    7,    1,    1,\n",
      "         525,  525,  131,  131,    7,    1,  525,    7,  131,  525,  525,    1,\n",
      "           7,    1,  525,    7,  525,  525,   28,    7,  131,  131,  131,    7,\n",
      "           7, 1383,  525,    7,    7,  525,  525,  131,  525,  525,  375,  525,\n",
      "         131,    1,    1,  525,  131,  131,  525,    1,  525,  131,  131,  525,\n",
      "         525,  525,  525,  131,  525,    1,  115,  131,  525,  131,  131,  525,\n",
      "           1,  525,  525,  525,  689,    1,  115,    1,  131,  525,  525,  525,\n",
      "         525,    7,  131,    7,  131,  131,  525,  525,    1,  131,  525,  525,\n",
      "         525,  525,  131,  131,   93,  241,  525,    1,  115,    1,  525,  689,\n",
      "           1,  525,  689,  131,    1,  525,  525,  131,  525,  689,  525,  525,\n",
      "         525,  131,  241,  525,  131,    1,  131,  689,  689,  525,  525,  131,\n",
      "         131,  525,  525,  131,  131,  131,  131,  525,    7,  525,  131,  131,\n",
      "         131,  131,  131,    7,    1,  525,    7,  525,  525,    7,  131,  131,\n",
      "         525,  131,    7,  525,  131,  525,  131,  131,    1,   93,    1,    7,\n",
      "         131,  261,  525,    1,    1,    1,  131,  525,  525,  525,    1,  131,\n",
      "           1,    7,  525,  525,  525,  525,    7,  261,    7,  525,    7,    7,\n",
      "           1,    1,  525,    1,    1,  525,  525,  525,  525,  131,    7,    1,\n",
      "         525,    7,    1,  525,  525,  525,  131,    7,  525,  131,  131,  525,\n",
      "         525,    7,  525,  131,    7,  131,    1,  131,  131,    1,    7,  261,\n",
      "         525,  131,  525,  525,  131,  525,    1,  525,  525,  525,  131,  525,\n",
      "         525,  131,    1,  525])\n",
      "\ttensor([   1,    1,  131,  525,    1, 2443,   28,   93,    7,  131,  525,  136,\n",
      "         525,    1,  131,  241,    7, 2443,    1,    7,    7,    1,  131,   93,\n",
      "           1,    1,    1,   60,    7,    1,  131,    1,    7,   93,  136,    1,\n",
      "         525,   60,  131,  525,    1,   60,    7,  525,    1,    1,  131,    1,\n",
      "           1,  136,  525,    1,  525,   60,   60,   60,    1,   93,  131,    7,\n",
      "         136,   60,   93,    1,    1,  525, 2443,    1,  525,  131,    1,  525,\n",
      "          93, 2443,  241,   60,    1,   93,  525,  131,  131,    1,  136,    7,\n",
      "           1,  525,    1,  136,    7,    1,  525,  131,   60,    1,    1,    1,\n",
      "         525,  131,  136,    1,   29,  525,    7,    7,    7,  131,  131,  131,\n",
      "        1203,  131,   29,  136,    7, 1203,  131, 1203,  525, 1203,  131,   29,\n",
      "           7, 1203,  131,  131,    7,    1, 1203, 1203,    7, 1203,  131,    7,\n",
      "        1203, 1203,    7,  115,   29,    1,  131,    1,    1,  131,  131,    7,\n",
      "         131,   29,    1,    7,    1, 1203,  525,    1,    1,  131, 1203,  131,\n",
      "           7,    7, 1203,    1,  131,    7,    1,    7,    1, 1203,  131,    7,\n",
      "        1203,   29,    1,  131, 1203,  131,   29,  115,    1, 1203,  525,  131,\n",
      "        1203,    7,   29,    7,    7, 1203,    1,    7,  131,    1,    1,  136,\n",
      "        1203,    7,  136, 1203, 1203,   29,  131,    1,  525,    7,  136,    7,\n",
      "         131,  115,  115,  525,  525,  131,    1,  115,  131,  525,  525,    7,\n",
      "           7,  131,    7,  131,  131,  115,    7,  261,  136,  525,  525,    7,\n",
      "         115,    1,    7,    7,    7,  115,    7,  115,  131,  131,  525,  525,\n",
      "         131,  131,   28,  131,  261,  525,    7,  131,  115,  261,  136,  131,\n",
      "           1,    7,   28,  261,  136,  131,  525,  115,    7,  115,    7,  131,\n",
      "         115,    7,    7,   28,  115,  136,    7,  525,    7,    7,  131,    7,\n",
      "         131,  131,  131,  131,  131,  115,  131,    7,    7,    7,    7,  131,\n",
      "         525,  131,    7,  525,  131,  131,  261,    7,  131,  525,  525,  525,\n",
      "         169,  169,  169,    1,  131,  115,    1,  131,  525,  131,  169,  131,\n",
      "         131,  131,    1,  131,  131,  136,  169,  169,    1,  131,    1,  525,\n",
      "         136,  130,  131,  131,    1,  131,  169, 1203,  131,  131,    1,  131,\n",
      "           1,    1,  525,  525,  525,  131,  525,  130,  131,  131,    7,    1,\n",
      "           7,    7,  131,    1,    7,  131,  131,  131,  525,  131,  131,  131,\n",
      "         525,    7,  131,  131, 1203,  131,  131,    7,  131,  169,  169,  131,\n",
      "         131,  131,    1,    1,    1,  169,    1,  131,  131,  131,  525,  130,\n",
      "         131,  131,  131,  131,  131,    1,  131,  525,    1,  131,  131,  131,\n",
      "         131,  131,    7,  136])\n",
      "\ttensor([ 131,  131,    7,  525,  525,  115,   28,    7,    7,  525,  525,    1,\n",
      "         136,  525,  525,  131,  131,  115,  131, 1203,    7,  131,  525,  136,\n",
      "         525,    7,   28,  136,  131,   28,  525,   28, 1203, 1203,   93,    7,\n",
      "         525,  136,    7,  131,   28,  136,    7,  136,   28,  131,  525,  131,\n",
      "           7,   93,  525,    7,  136,  136,  136,  136,  525,    7,  525,    7,\n",
      "           1,  136, 1203,  131,    7,  136,  115,    7,  136,   93,  131,  131,\n",
      "         136,    7,    1, 1203,  525, 1203,  131,  525,  525,    7,    1,  131,\n",
      "          93,  136,  131,    1,    7,    7,  525,  525,  136,   28,    7,   28,\n",
      "         136,    7,    1,    7,  261,    1,    1,  131,    1,  131,  131,  131,\n",
      "        2034,  131,   28,  131,  131,    1,  131,  136,    1, 2034,  131,   28,\n",
      "         131, 2034,  131,  131,  131,  169,  136,  136,    1, 2034,  241,  131,\n",
      "         689,  136,    1,  131,  261,  169,  131,   93,  131,  131,  241,  131,\n",
      "         241,   28,  131,  131,  131,  689,    1,  131,  131,  241,    1,  241,\n",
      "         131,  131,    7,  131,  131,  131,    1,  525,    1, 2034,  241,    1,\n",
      "         136,   28,  131,  131, 2034,  241,   28,  131,  169,  136,  241,  241,\n",
      "           7,    1,   28,  131,  131,  136,    1,  131,  131,  159,  131,  131,\n",
      "         689,  131,  131, 2034,    1,   28,  131,    1,  131,  115,    1,    7,\n",
      "           1,  525,  525,  131,    1,  115,    1,  525,    7,    1,    1,   93,\n",
      "           7,  131,    7,    1,    1,  525,  241,  136,    1,    1,    1,    7,\n",
      "         525,    1,   93,   93,   93,  525,  241,  525,    7,    7,    1,  131,\n",
      "           7,  136,    1,  136,  136,    1,   93,  131,  525,  136,    1,  131,\n",
      "           1,   93,    1,  136,  131,  525,  131,  525,  241,  525,   93,  131,\n",
      "         525,  169,   93,    1,  525,    1,  169,    1,   28,   93,  131,   93,\n",
      "           1,    1,  525,    1,    1,  525,    7,   93,  131,   93,   93,    1,\n",
      "           1,    7,   93,    1,    1,    7,  136,    7,  136,  131,    1,    1,\n",
      "           7,    7,   93,    1,  525,  131,    1,  131,  525,    7,    7,  115,\n",
      "         131,  241,  115, 1203,  115,    1,    7,    7,  525,    1,  136,  131,\n",
      "         131,    1,  131,  525,  136,  525,   93,  115, 1203,  131,  136,  115,\n",
      "           1,    1,    7,  131,  131,    1,    1,    1,    7, 1203,    7,  115,\n",
      "         131,  131,  525,  136,  131,  131,    1,  131,  525,  115,    7,  525,\n",
      "           7,    7,  525,  131,  115,  525,  241,    7,  131,   93,    7,  131,\n",
      "         131,    1,    1,    1,    1,    7,    1,  241,  241,  525,  136,    1,\n",
      "         131,  241,  525, 1203,  525,    1,  525,  131,    1,  131,  115,  131,\n",
      "         131,  115,  131,    1])\n",
      "\ttensor([   1,    1,  131,    7,    1,  136,   29,  136,  131,  525,    7,  131,\n",
      "          29,    1,  131,    1,  115,  136,    7,  136,  131,    7,  525,  136,\n",
      "           1,  136,  136,    1,  115,  136,  131,  136,  136,  131,  131,  136,\n",
      "           7,    1,    1, 1722,  136,    1,  131,  136,  130,  131,  525,  131,\n",
      "         136, 1203,    1,  136,   93,    1,    1,    1,    1,  131,  136,  131,\n",
      "         131,    1,  136,    7,  136,  136,  136,  525,   29,  131,  131, 1722,\n",
      "         136,    1,   60,    7,    1,  136,    1,  131,  130,  136,  131,  136,\n",
      "         525,   29,  131,  131,  131,  689,  115,  525,    1,  525,  525,  525,\n",
      "          29,    1,  131,  136,    7,    1,    7,    1,    7,  131,  131,    7,\n",
      "           7,  131,  169,    1,    7,    1,  525,  136,  872,    7,  131,  169,\n",
      "           1,    7,  136,  136,    1,  525,  136,  136,    1,    7,  169,    7,\n",
      "         131,  136,   93,  131,    7,    1,    1,    1,    7,    1,   93,    7,\n",
      "           1,  169,    1,    1,    1,  131,    1,    7,  115,  169,    1,   93,\n",
      "           1,    1,  689,    1,  131,    1,    1,    7,    1,    1,  131,    1,\n",
      "         136,  131,  115,  136,    7,   93,    7,  131,    1,  131,    1,   29,\n",
      "         136,   93,  131,    7,    1,  136,    1,    1,  131,    1,    1,    7,\n",
      "         131,    1,    1,    7,    1,  131,  525,    1,  525,    1,  525,    1,\n",
      "           7,    7,    7,   93,  689,    7,   93,    7,  131,  130,  169,    1,\n",
      "         131,  131,  131,    7,    7,    7,  525,    7,  525,  131,  169,  131,\n",
      "           7,   93,    7,    7,  131,    7,  525,    7,  525,  131,  131,   93,\n",
      "         131,  131,  525,  131,    7,  689,    1,  136,    7,    7,  525,  136,\n",
      "          93,    7,  525,    7,  525,  115,    7,    7,  525,    7,    7,  131,\n",
      "           7,  131,  136,  525,    7,    1,  136,  131,  131,  136,  131,    7,\n",
      "           7,    7,    1,    7,    7,    7,  525,  131,    7,    7,    7,    7,\n",
      "         525,  131,    1,  131,    7,  131,    7,    7,  131,  136,    1,  169,\n",
      "         136,  136,  689,  136,  131,    7,  525,  689,  131,  131,  131,  131,\n",
      "         689,    1,    1,  131,  136,   60,  136,  131,    1,    1,  131,  131,\n",
      "           1,  131,  136,  131,  131,  131,  131,    1,  131,  689,  136,  131,\n",
      "         525,  525,    1,  525,  525,    1,    1,  131,  131,  131,    7,    1,\n",
      "           7,  115,  872,  131,    7,  136,    1,  525,  136,  131,  131,  131,\n",
      "           1,  115,  136,  136,   29,    7,    1,    7,  131,   29,  131,  131,\n",
      "         689,  131,  131,    7,  525,  131,  525,  115,    1,  136,  131,  131,\n",
      "         689,  115,  872,  131,    1,  131,    1,  525,  131,  136,  131,  689,\n",
      "         525,  136,    7,   60])\n",
      "\ttensor([  131,   525,   131,     1,   241,   131,   525,   131,     7,     7,\n",
      "            7,     1,   241,   525,     7,  1732,     7,   136,   525,   525,\n",
      "          115,     1,     1,     7,   131,     1,  1203,     1,     1,    60,\n",
      "            1,  3460,     1,   525,   525,    93,   115,    29,     7,   525,\n",
      "          241,    93,    88,     7,   525,     1,   115,     1,     7,     7,\n",
      "            7,   689,     7,   689,   136,   169,   136,   131,     1,  1722,\n",
      "          115,     7,     7,   136,    29,   115,   525,     7,    60,   131,\n",
      "            7,  1203,   115,     1,   525,   131,   525,     1,     1,   115,\n",
      "          525,   426,     7,   525,   131,    72,   131,  1722,   241,     1,\n",
      "          131,   689,   131,     1,     1,     7,  1203,   115,    93,   136,\n",
      "          131,     1,   525,     7,   131,     7,   525,   131,     7,   115,\n",
      "            1,   131,     1,   525,     7,     7,   131,    28,     1,     7,\n",
      "          131,     1,   131,   131,   525,     1,   515,     1,   136,  1722,\n",
      "          131,   131,     7,   115,   525,     1,   525,   525,     1,     1,\n",
      "          136,     7,   525,   136,     1,   115,   131,   689,   525,     1,\n",
      "            7,   131,  1203,   136,   131,     1,   525,     1,     7,  1722,\n",
      "         1722,     7,     7,   136,   136,   525,     1,   131,  1383,   136,\n",
      "           60,     7,    93,  1722,     1,     7,     7,     1,     7,    28,\n",
      "            1,   136,   525,   525,     7,    93,     1,   130,   241,     7,\n",
      "            7,     1,   115,    90,   136,   689,   136,     1,   115,   131,\n",
      "          525,   131,   131,   525,   131,     1,     7,     1,   525,     7,\n",
      "            7,   131,     7,   131,     7,     7,     7,     7,     1,     7,\n",
      "            1,   115,     7,  1722,   131,     1,     1,   115,   136,     1,\n",
      "            1,  1722,     7,    93,     1,  1722,     7,   115,   131,   136,\n",
      "            1,   115,   136,     7,    88,   131,     1,    60,    88,     1,\n",
      "          136,  1203,   115,    88,   131,     7,     1,     1,     7,  1469,\n",
      "          115,    28,   115,     1,   159,     1,   525,   525,   689,     1,\n",
      "            7,   115,   131,    28,   115,     7,    93,   115,   525,   115,\n",
      "            1,  1318,     1,     1,   136,    93,   525,   136,     7,   241,\n",
      "          689,     7,    93,   131,    28,     1,     1,     1,     1,   115,\n",
      "          131,   525,   131,   131,     7,   131,     1,   241,   131,   525,\n",
      "            7,     1,     7,     7,     1,     1,   131,   525,   136,     1,\n",
      "         1732,     1,     7,     7,   131,     7,     7,     1,     1,   115,\n",
      "            1,   241,     7,   525,     1,   115,   241,     7,   131,   525,\n",
      "            1,     7,   131,     1,     1,   115,     7,   136,   525,     1,\n",
      "          525,   115,     1,     1,   115,     7, 12072,    93,   136,    93,\n",
      "          525,     1,     7, 12072,     1,   131,     1,   115,     1,     7,\n",
      "           93,     7,   115,     1,     1,     1,   115,   115,  1203,     7,\n",
      "          689,     1,     1,   115,   130,     1,   136,    93,     1,   131,\n",
      "            1,   136,   136,   115,     7,    72,   115,   136,   136,   131])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([36, 55, 27,  2, 44, 56, 34,  3, 57,  2, 56, 17, 79, 22,  2, 38,  2, 63,\n",
      "        49, 84, 11, 89, 27, 60, 89, 17, 70, 49, 45, 40, 31, 18, 63, 51, 55, 44,\n",
      "        54, 44, 22, 60, 31, 38, 18, 29, 54, 88, 41, 58, 52, 79, 88, 28, 58, 81,\n",
      "        90, 26, 44,  2, 58, 63, 44, 55,  5, 26, 74, 28, 54,  3, 41, 46, 48, 82,\n",
      "        41, 36, 57, 88, 46,  3, 58, 74, 78, 17, 65, 49, 17, 30,  2, 92, 27, 56,\n",
      "        55, 75, 58, 65, 45, 27, 54, 38, 83, 22])\tToken idx: tensor([   7,    1,    1,    1,  525,    7,    1,    7,    1,   28,  115,    1,\n",
      "         525,    1, 1722,    7,    7,   28,    7,  131,  131,  131,    7, 1203,\n",
      "         525,    7,  131,    1,    1,  131,    7,  131,  261,  131,  525,    1,\n",
      "           1,    7,    7,   60,    1,  689,  136,    1,  525,    1,  131,  136,\n",
      "           7, 1203,  115,    7,  131,  525,  525,    1,  131,   93,    1,    7,\n",
      "         115,  136,    1,    7,    7,    1,  241,   88,    1,    7,  136,    1,\n",
      "         136,  241,  136,    7,    1,  115,  525,    1,    1,  689,    1,  115,\n",
      "        1346,    7,  159,    1,  115,    1,  689,  525, 1722,  136,  136,   93,\n",
      "           7,  131, 1203,   93])\tRaw indices: indices_to_process=tensor([1200715, 1834416,  900532,   66707, 1468057, 1867775, 1134003,  100066,\n",
      "        1901122,   66734, 1867883,  567002, 2635412,  733767,   68428, 1267421,\n",
      "          66713, 2101267, 1634304, 2801783,  367014, 2968548,  900538, 2002383,\n",
      "        2968942,  567008, 2334841, 1634298, 1500886, 1334251, 1033950,  600485,\n",
      "        2101500, 1701134, 1834940, 1467533, 1801063, 1467539,  733773, 2001240,\n",
      "        1033944, 1268103,  600490,  967238, 1801587, 2935065, 1367604, 1934610,\n",
      "        1734363, 2636090, 2935179,  933891, 1934605, 2702118, 3002295,  867179,\n",
      "        1467663,   66799, 1934475, 2101246, 1467647, 1834551,  166766,  867185,\n",
      "        2468129,  933885, 1801303,  100147, 1367474, 1534245, 1601080, 2734947,\n",
      "        1367609, 1200949, 1901257, 2935071, 1534239,  100174, 1934999, 2468123,\n",
      "        2601535,  567690, 2167946, 1634412,  568347, 1000597,   66865, 3068477,\n",
      "         900646, 1867769, 1835104, 2502000, 1936196, 2168081, 1501021,  900624,\n",
      "        1801069, 1267545, 2769502,  733859])\n",
      "Beam idx: tensor([86, 41, 70, 21, 80, 78, 87, 69, 87, 22, 23, 42, 62, 99, 71, 30, 90, 77,\n",
      "        22, 62, 66, 66, 41, 23, 20, 87, 25,  4,  4, 72, 87, 57, 42, 38, 32, 78,\n",
      "        62, 66, 40, 73, 49, 69, 66, 91, 62, 18,  4, 51, 19, 66, 91, 87, 38,  5,\n",
      "        17, 40, 64, 15, 18, 64, 62, 98, 23, 68, 86, 42, 48, 93, 25, 40, 22, 90,\n",
      "        41, 31, 62, 57, 87, 42, 20, 66, 87, 22, 68, 77, 66, 14, 23, 63, 73, 62,\n",
      "        32, 96,  5, 66,  5, 66, 23, 86, 62, 71])\tToken idx: tensor([   1,    1,    1,    1,    1,    7,    1,    7,    7,  525,    7,    1,\n",
      "         131,    1,  131,  131,  241,  689,  136,  525,  131,    7,    7,    1,\n",
      "         131,  689,    1,    7,    1,    7,  525,  525,    7,    1,    7,    1,\n",
      "           1,  169,    7,    1,    1,  115,   28,  525,  136,    1,  115,    7,\n",
      "           1,  261,  131,   93,    7,    1,    7,  115,   28,  131,    7,    7,\n",
      "           7,    1, 1722,    1,  136,  115,    7,  131,  689,    1,    1,   72,\n",
      "         115,    7,   93,  131,  130,   88,  136,  689,  426,  471,  115,  525,\n",
      "           1,    1,   88,  131,  689,  130,  115,    1, 1722,  136,   88,  525,\n",
      "         689,  525,  241,  136])\tRaw indices: indices_to_process=tensor([2868359, 1367474, 2334711,  700414, 2668241, 2601541, 2901712, 2301364,\n",
      "        2901718,  734291,  767126, 1400827, 2068017, 3301948, 2368194, 1000721,\n",
      "        3002011, 2568870,  733902, 2068411, 2201429, 2201305, 1367480,  767120,\n",
      "         667191, 2902400,  833826,  133419,  133413, 2401423, 2902236, 1901646,\n",
      "        1400833, 1267415, 1067303, 2601535, 2067887, 2201467, 1334127, 2434770,\n",
      "        1634298, 2301472, 2201326, 3035648, 2068022,  600355,  133527, 1701010,\n",
      "         633708, 2201559, 3035254, 2901804, 1267421,  166766,  567008, 1334235,\n",
      "        2134620,  500426,  600361, 2134599, 2067893, 3268595,  768841, 2268005,\n",
      "        2868494, 1400941, 1600951, 3101960,  834514, 1334121,  733767, 3001842,\n",
      "        1367588, 1033950, 2067979, 1901252, 2901841, 1400914,  667196, 2201987,\n",
      "        2902137,  734237, 2268119, 2568706, 2201299,  466943,  767207, 2101370,\n",
      "        2435458, 2068016, 1067411, 3201889,  168487, 2201434,  166853, 2201823,\n",
      "         767808, 2868883, 2068127, 2368199])\n",
      "Beam idx: tensor([80, 43, 92, 35, 51, 44, 59, 30, 21, 51, 93, 35, 43, 44, 52, 61, 41,  7,\n",
      "        35, 23, 41, 27,  7, 88, 66, 48,  9, 60, 48, 78, 71, 55, 60, 61,  7, 76,\n",
      "        64, 31, 51, 37, 56, 47, 93, 44, 23, 48, 51, 70, 54, 23, 31, 29, 68, 29,\n",
      "        23, 87, 43, 93, 88, 52, 17, 45, 22, 17, 87, 98, 44,  7, 88, 70, 68, 84,\n",
      "        67, 57, 88, 29,  7, 64, 54, 61, 49, 93, 49, 44, 35, 70, 51, 98, 15, 68,\n",
      "        88,  9, 45, 76, 88, 61, 29, 64, 69, 50])\tToken idx: tensor([  131,   131,   131,     1,   525,     7,   525,     1,   131,   136,\n",
      "            1,     7,   525,     1,     7,     7,     7,     7,   115,   136,\n",
      "          241,   131,   131,   169,     7,     1,     7,    60,     7,     1,\n",
      "          131,     1,  1203,     1,     1,  1203,   136,     7,   689,     1,\n",
      "          689,   131,   136,   115,   131,   115,   131,   136,   131,   689,\n",
      "          689,   689,     1,     1,     1,   525,   136, 12072,     1,   115,\n",
      "            1,     1,     1,     7,  1203,     1,    93,   136,    29,   131,\n",
      "            7,   131,     1,   131,   136,   136,  1346,   471,   136,   689,\n",
      "           60,   525,  1203,   159,    93,   525,   471,   689,     1,   115,\n",
      "           93,     1,     7,    60,   131,    93,    29,     1,   525,   131])\tRaw indices: indices_to_process=tensor([2668371, 1434310, 3068607, 1167356, 1701528, 1467539, 1968352, 1000591,\n",
      "         700544, 1701139, 3101830, 1167362, 1434704, 1467533, 1734363, 2034540,\n",
      "        1367480,  233478, 1167470,  767255, 1367714,  900662,  233602, 2935233,\n",
      "        2201305, 1600945,  300184, 2001240, 1600951, 2601535, 2368194, 1834416,\n",
      "        2002383, 2034534,  233472, 2536031, 2134728, 1033950, 1701692, 1234062,\n",
      "        1868457, 1567722, 3101965, 1467647,  767250, 1601059, 1701134, 2334846,\n",
      "        1801193,  767808, 1034632,  967926, 2268005,  967238,  767120, 2902236,\n",
      "        1434315, 3113901, 2935065, 1734471,  567002, 1500886,  733767,  567008,\n",
      "        2902914, 3268595, 1467625,  233607, 2935093, 2334841, 2268011, 2801783,\n",
      "        2234652, 1901252, 2935200,  967373,  234817, 2135063, 1801198, 2035222,\n",
      "        1634357, 3102354, 1635500, 1467691, 1167448, 2335235, 1701474, 3269283,\n",
      "         500296, 2268119, 2935157,  300178, 1500892, 2534888, 2935195, 2034626,\n",
      "         967266, 2134593, 2301882, 1667781])\n",
      "Beam idx: tensor([52, 85, 28, 12, 27, 86, 65, 48, 77, 87, 98, 34, 85, 52, 53, 71, 51, 15,\n",
      "        98, 17, 87, 35, 21, 15, 23, 53, 24, 59, 27, 27, 90, 52, 75, 83, 51, 87,\n",
      "        77, 25, 78, 44, 48,  3, 48, 87, 44, 83, 63, 24, 92, 65, 87, 25, 25, 98,\n",
      "        71, 51, 98, 24,  6, 75, 28, 85, 28, 98, 83, 96, 99, 99, 50, 67, 87, 15,\n",
      "        94, 89, 48, 68, 90, 77, 27, 87, 23, 27, 77, 89, 86, 12, 87, 27, 96, 44,\n",
      "        82, 85, 87, 98, 35, 38, 70, 48,  1, 89])\tToken idx: tensor([    1,     7,   131,   131,     7,   131,   131,     7,     1,     7,\n",
      "            1,     1,     1,     7,   136,     1,     7,   131,   525,   131,\n",
      "          241,     7,     1,   136,     1,   131,     7,     1,     1,  1722,\n",
      "            1,   115,     7,     1,     1,   131,     7,   131,   131,     1,\n",
      "          115,     7,     1,   525,     7,     7,     7,   115,     1,   136,\n",
      "            1,     1,   136,   689,     7,   689,   136,     1,     1,   115,\n",
      "            1,   115,   136,    93,    28,   241,   525,   241,     1,     7,\n",
      "          130,   525,   525,  1203,   241, 12072,     7,    93,    88,    93,\n",
      "            7,   115,   689,   525,   136,   872,    28,   159,  1203,   115,\n",
      "            1,    93,   115,   130,    88,     1,   136,    93,     7,    60])\tRaw indices: indices_to_process=tensor([1734357, 2835012,  934015,  400367,  900538, 2868489, 2168076, 1600951,\n",
      "        2568182, 2901718, 3268595, 1134003, 2835006, 1734363, 1767845, 2368064,\n",
      "        1701010,  500426, 3269119,  567132, 2901952, 1167362,  700414,  500431,\n",
      "         767120, 1767840,  800479, 1967828,  900532,  902253, 3001771, 1734471,\n",
      "        2501482, 2768300, 1701004, 2901842, 2568188,  833956, 2601665, 1467533,\n",
      "        1601059,  100066, 1600945, 2902236, 1467539, 2768306, 2101246,  800587,\n",
      "        3068477, 2168081, 2901712,  833826,  833961, 3269283, 2368070, 1701692,\n",
      "        3268730,  800473,  200119, 2501590,  933885, 2835120,  934020, 3268687,\n",
      "        2768327, 3202129, 3302472, 3302188, 1667651, 2234658, 2901841,  500820,\n",
      "        3135707, 2969620, 1601185, 2280076, 3001777, 2568274,  900619, 2901804,\n",
      "         767126,  900646, 2568870, 2968942, 2868494,  401108, 2901739,  900690,\n",
      "        3203091, 1467647, 2734947, 2835098, 2901826, 3268724, 1167443, 1267415,\n",
      "        2334846, 1601037,   33360, 2968477])\n",
      "DEB: t.shape=torch.Size([9, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,    1,    1,  131,   50,    7,   60,    7,  118,  131,    7,  159,\n",
      "         305,  305,  131,  447,  131,  235,   60, 2416,   60, 2416,    1,   60,\n",
      "        2416,  159,  235,   60, 3694,   50,   50,  235,  235, 2416,    1,   50,\n",
      "           1,   50,  305,   60,   50,  447,  235,   50,    1,    7,    1,  305,\n",
      "           7,  305,    7,  322,  305, 2416,    7,   50,   50,  131,  305,  235,\n",
      "          50,    1,  159,   50,    1,  322,    1,    7,    1,  305, 2416,   60,\n",
      "           1,    7,  118,    7,  305,    7,  305,    1,    7,  159,    7,   60,\n",
      "         159,  305,  131,    1,    1,    7,    1,  235,  305,    7, 3694,    1,\n",
      "           1,  447,    7,  305,    1,  471,  343,    1, 1722,  119,  119,  528,\n",
      "         119,  130,   29,  518,    1,  998,  130,  518,  343,   93,  130,    1,\n",
      "         518,  518,  471,   29,  119,  119,  998,  528,  528,    1,  119,  130,\n",
      "         518,  471, 1722,  119,    1,  518,  343,  518, 1722,  528,  518, 1346,\n",
      "           1,  130,  528,  343,  528,  518, 1346,  119,  471,  130,    1,  343,\n",
      "         998,   93,  130,  998,    1,   29,   29,   93,    1,  518,  343,  130,\n",
      "         998,  343,  130,  343,  471,  119,    1,  130,  119,  518,  119,  518,\n",
      "         119,  130,   93,   93,  518,   29,   29,  119,  518,    1, 1722, 1722,\n",
      "         130,  518,  130,  518,   29,    1,    1,  130,  525,  525,  525,  131,\n",
      "         130,  525,  131,  525,  131,  130,  426,  131,  525,  525,  136,  131,\n",
      "         525, 1023,  131,  525,  525,  131, 1023,  525,  525,  131, 1491,  525,\n",
      "         131,  131,  525,  525,  525,  131, 1023,  525,  131,  525,  130,  525,\n",
      "         431,  130,  426,  525,  525,  131,  130,  131,  525,  525,  525,  136,\n",
      "         131,  136,  525,  525,  525,  426,  525,  136,  130,  525,  525,  130,\n",
      "         525,  525,  525, 1023,  525,  131,  131,  131,  525,  131,  525,  136,\n",
      "        1023,  131,  525,  131,  525,  426,  525,  525,  131,  131,  130,  525,\n",
      "         525,  131,  525, 1491,  525,  525,  525,  131,  136,  131,  136,  136,\n",
      "         882,  525,    1,  261,    8,  525,  525,  882,  525,  689,  882,    1,\n",
      "         525,  882,  261,  261,    1,  689,  882,  525,  689,  525,  882,  689,\n",
      "         525,  261,  525,    8,    8,    8,    8,  882,  525,  525,    1,  689,\n",
      "         525,  525,  525,  471,  882,  525,  882,  689,  471,  525,  525,  525,\n",
      "         525,  525,  689,  525,  525,  882,  261,    1,  882,  525,  525,  525,\n",
      "           1,  525,    1,  882,  525,  261,  525,  525,  525,  525,  689,  689,\n",
      "         525,  525,  882,  261,    8,  525,    8,  689,  525,    8,  525,  525,\n",
      "         525,  261,  689,    8,  261,  471,  525,  525,  689,  882,  525,  525,\n",
      "         525,  882,  525,  525])\n",
      "\ttensor([ 169,    1,    1,  525,  525,    1,  525,  169,  131,  525,    1,   93,\n",
      "         525,  525,  525,  525,  525,  525,  525,  525,  525,  525,    1,  525,\n",
      "         525,   93,  525,  525,  525,  525,  525,  525,  525,  525,    1,  525,\n",
      "           1,  525,  525,  525,  525,  525,  525,  525,    1,  169,    1,  525,\n",
      "         169,  525,  169,  525,  525,  525,  169,  525,  525,  525,  525,  525,\n",
      "         525,    1,   93,  525,    1,  525,    1,  169,    1,  525,  525,  525,\n",
      "           1,  169,  131,  169,  525,  169,  525,    1,  169,   93,  169,  525,\n",
      "          93,  525,  525,    1,    1,    1,    1,  525,  525,  169,  525,    1,\n",
      "           1,  525,  169,  525,  118,  525,  525, 1165,  130,  525,  525,  525,\n",
      "         525,    1,  115,  525,  118,  525,    1,  525,  525,    1,    1,  118,\n",
      "         525,  525,  525,  115,  525,  525,  525,  525,  525, 1165,  525,    1,\n",
      "         525,  525,  130,  525,  118,  525,  525,  525,  130,  525,  525,  525,\n",
      "         118,    1,  525,  525,  525,  525,  525,  525,  525,    1, 1165,  525,\n",
      "         525,    1,    1,  525,  118,  115,  115,    1,  118,  525,  525,    1,\n",
      "         525,  525,    1,  525,  525,  525,  118,    1,  525,  525,  525,  525,\n",
      "         525,    1,    1,    1,  525,  115,  115,  525,  525,  118,  130,  130,\n",
      "           1,  525,    1,  525,  115,  118,  118,    1,  426,  115,  426,    1,\n",
      "           1,  426,    1,  518,    1,    1,  525,    1,  115,  426,  131,    1,\n",
      "         115,    1,    1,  426,  115,    1,    1, 1893,  518,    1,  525,  169,\n",
      "           1,    1, 1893,  426,  169,    1,    1,  510,    1,  518,    1,  510,\n",
      "         525,    1,  525,  426,  426,    1,    1,    1,  426,  426,  518,  131,\n",
      "           1,  131,  426,  426,  115,  525, 1893,  131,    1, 1893,  169,    1,\n",
      "         426, 1893,  426,    1, 1893,    1,    1,    1,  426,    1, 1893,  131,\n",
      "           1,    1,  426,    1,  426,  525,  426,  426,    1,    1,    1, 1893,\n",
      "         518,    1, 1893,  525, 1893,  510, 1893,    1,  131,    1,  131,  131,\n",
      "         525,  579,    7,  525,  525,   91,   91,  525,  510,  525,  525,    7,\n",
      "         579,  525,  525,  525,    7,  525,  525,  518,  525,   78,  525,  525,\n",
      "          60,  525,  510,  525,  525,  525,  525,  525,  131,  874,    7,  525,\n",
      "         510,  874,  148,  525,  525, 1893,  525,  525,  525,  874,   60,  510,\n",
      "          91,   91,  525,  874,  874,  525,  525,    7,  525,  510, 1893,  131,\n",
      "           7,  579,    7,  525,  874,  525,  518,  518,   91,  704,  525,  525,\n",
      "          78,   91,  525,  525,  525,  510,  525,  525,   60,  525,  510,   91,\n",
      "          91,  525,  525,  525,  525,  525,   60,  579,  525,  525,   78,   91,\n",
      "         510,  525,  510,   91])\n",
      "\ttensor([  7, 261, 261, 525, 115,   7,   1,   7, 525, 525,   7,   7, 525, 525,\n",
      "        525, 261, 525,   7,   1, 525,   1, 525, 261,   1, 525,   7,   7,   1,\n",
      "        525, 115, 115,   7,   7, 525, 261, 115, 261, 115, 525,   1, 115, 261,\n",
      "          7, 115, 261, 115, 261, 525,   7, 525, 115,   7, 525, 525,   7, 115,\n",
      "        115, 525, 525,   7, 115, 261,   7, 115, 261,   7, 261,   7, 261, 525,\n",
      "        525,   1, 261,   7, 525, 115, 525,   7, 525, 261,   7,   7,   7,   1,\n",
      "          7, 525, 525, 261, 261,   7, 261, 261, 525,   7, 525, 261, 261, 261,\n",
      "        115, 525,   7,   7, 525, 525, 525, 159,   7, 115,   7, 159, 115, 525,\n",
      "          7,   1, 159, 525, 525, 115, 159,   7, 525, 525,   7, 115, 159,   7,\n",
      "          1,   7,   7, 525,   7, 115, 525,   7, 525, 159,   7, 525, 525, 525,\n",
      "        525, 115, 525, 130,   7, 159,   7, 525, 115, 525, 130,   7,   7, 159,\n",
      "        525, 525,   1, 115, 159,   1,   7, 115, 115, 115,   7, 525, 525, 115,\n",
      "          1, 525, 159, 525,   7,   7,   7, 115,   7, 525, 159, 525,   7, 159,\n",
      "        115, 115, 525, 115, 115,   7, 525,   7, 525, 525, 159, 525, 159, 525,\n",
      "        115,   7,   7, 159, 115, 131, 115, 130, 115,   7, 130, 525, 130, 115,\n",
      "        525, 130, 131,   7, 115, 130, 131, 525, 130,   7, 131,   1, 525, 115,\n",
      "        525, 130, 525,   7, 130, 525, 115,   7,   7, 130, 525,   1, 130, 525,\n",
      "        115,   1,   1, 115, 525,   7,   7, 130, 115,   1,   7,   7, 525, 115,\n",
      "        130, 115,   7, 115, 131, 525, 115, 115, 115, 115,   7, 115, 115, 115,\n",
      "          7, 525, 115,   1, 130,   1,   7, 525, 115, 115, 525, 130,   7, 130,\n",
      "          7, 525,   7,   7, 130,   1, 115, 115, 525, 130, 115, 525, 115,   1,\n",
      "        115, 130, 115, 130,   7,   7, 525,   7,   7, 525, 525,   1,   1, 525,\n",
      "          7, 136, 525,   7,   7, 525, 525, 525,   7, 136, 525, 525, 136,   7,\n",
      "        525, 136, 525, 525,   7, 525, 525, 525, 525, 525, 525,   7,   7, 136,\n",
      "          7,   7, 525, 525, 525, 525, 525, 136, 525,   7, 525,   7,   7,   1,\n",
      "        136,   7,   7, 525, 525,   7, 525,   7, 525, 525,   7,   7,   7, 525,\n",
      "          7, 525, 525, 525,   1, 525, 136, 136,   7,   7, 525, 525, 525,   7,\n",
      "        525, 136, 525, 525,   7,   7,   1, 525, 136, 525, 525, 525, 525,   7,\n",
      "        136, 525,   7,   7,   7, 525,   7,   7])\n",
      "\ttensor([ 131,  131,  131,    1,  525,    1,  131,  131,  131,    1,    1,  241,\n",
      "         525,  525,    1,  131,    1,    7,  131,    7,  131,    7,  131,  131,\n",
      "           7,  241,    7,  131,  872,  525,  525,    7,    7,    7,  131,  525,\n",
      "         131,  525,  525,  131,  525,  131,    7,  525,  131,   93,  131,  525,\n",
      "         131,  525,   93,  525,  525,    7,  131,  525,  525,    1,  525,    7,\n",
      "         525,  131,  241,  525,  131,  525,  131,  131,  131,  525,    7,  131,\n",
      "         131,  131,  131,   93,  525,  131,  525,  131,  131,  241,  131,  131,\n",
      "         241,  525,    1,  131,  131,    1,  131,  525,  525,  131,  872,  131,\n",
      "         131,  131,    1,  525,   28,  115,  131,    7,    7,  525,    7,  525,\n",
      "           7,  131,  525,    1,   28,  131,  131,    1,  131,  525,  131,   28,\n",
      "           1,    1,  115,  525,  525,    7,  131,    1,    1,    7,    7, 1383,\n",
      "           1,  115,    7,  525,   28,    1,  131,    1,    7,  525,    1,    7,\n",
      "          28,  131,    1,  131,  525,    1,    7,    7,  115,  131,    7,  131,\n",
      "         131,  525,  131,  131,   28,  525,  525,  525,   28,    1,  131, 1383,\n",
      "         131,  131,  131,  131,  115,    7,   28, 1383,    7,    1,  525,    1,\n",
      "           7,  131,  525,  525,    1,  525,  525,    7,    1,   28,    7,    7,\n",
      "         131,    1,  131,    1,  525,   28,   28,  131,  131,    7,  131,    1,\n",
      "         525,  131,    1,  525,    1,  525,  131,    1,    7,  131,  525,    1,\n",
      "           7,  525,    1,  131,    7,  525,  525,  131,  689,    1,  131,  115,\n",
      "           1,  241,  131,  131,  115,    1,  525,  525,    1,  525,  525,  525,\n",
      "          93,  525,  131,  131,  131,    1,  525,  525,  131,  131,  525,  525,\n",
      "           1,  525,  131,  131,    7,  131,  131,  525,  525,  131,  115,  525,\n",
      "         131,  131,  131,  525,  131,  525,    1,  689,  131,  241,  131,  525,\n",
      "         525,    1,  131,    1,  131,  131,  131,  131,    1,  525,  525,  131,\n",
      "         525,    1,  131,  131,  131,  525,  131,    1,  525,    1,  525,  525,\n",
      "           1,  131,    1,  525,    1,  525,  525,    1,  131,  525,    1,    1,\n",
      "         131,    1,  525,  525,    1,  525,    1,  525,  525,  131,   93,  525,\n",
      "           7,  525,  131,    1,    1,    1,    1,    1,  131,  261,    1,  525,\n",
      "         131,  261,    1,    7,    1,    7,    1,  525,    7,  261,  525,  131,\n",
      "         525,  525,  525,  261,  261,    1,  525,    1,    1,  131,    7,  131,\n",
      "           1,  131,    1,    1,  261,  525,  525,  525,  525,    7,  525,  525,\n",
      "         131,  525,    1,  525,    1,  131,    1,  525,    7,    1,  131,  525,\n",
      "         525,  525,  525,    1,  525,    7,    7,  131,  525,    1,  131,  525,\n",
      "         131,    1,  131,  525])\n",
      "\ttensor([ 525,   60,   60,  131,    1,    1,  136,  525,   93,  131,    1, 2443,\n",
      "         131,  131,  131,  131,  131,    1,  136,    1,  136,    1,   60,  136,\n",
      "           1, 2443,    1,  136,    1,    1,    1,    1,    1,    1,   60,    1,\n",
      "          60,    1,  131,  136,    1,  131,    1,    1,   60,    7,   60,  131,\n",
      "         525,  131,    7,    7,  131,    1,  525,    1,    1,  131,  131,    1,\n",
      "           1,   60, 2443,    1,  241,    7,   60,  525,   60,  131,    1,  136,\n",
      "          60,  525,   93,    7,  131,  525,  131,  241,  525, 2443,  525,  136,\n",
      "        2443,  131,  131,   60,   60,    1,   60,   60,  131,  525,    1,   60,\n",
      "          60,  131,    7,  131,    1,  131,    1, 1203, 1203,  525,    7,   29,\n",
      "           7,  131,  131,  131,    1,    1,  131,  131,    1, 1203,  131,    1,\n",
      "         131,  131,  131,  131,    7,    7,    1,    7,    7, 1203,    7,    7,\n",
      "         131,  131, 1203,  525,    1,  131,    1,  131, 1203,   29,  131,  136,\n",
      "           1,  131,    7,    1,   29,  131,  136,    7,  131,  131, 1203,    1,\n",
      "           1, 1203,  131,    1,    1,  131,  131, 1203,    1,  131,    1,    7,\n",
      "           1,    1,  131,    1,  131,    7,    1,    7,    7,  131,    7,  131,\n",
      "           7,  131, 1203, 1203,  131,  131,  131,    7,  131,    1, 1203, 1203,\n",
      "         131,  131,  131,  131,  131,    1,    1,  131,  131,  131,  131,  115,\n",
      "         131,  261,  115,    7,  115,  131,  131,  115,  131,  261,    1,  115,\n",
      "         131,  525,  115,  261,  131,    7,  525,  525,    7,  115,  131,    7,\n",
      "         115,  131,  525,  261,    7,  115,  525,  131,  115,    7,  131,  131,\n",
      "         136,  131,  131,  261,  261,  115,  131,    7,   28,  261,    7,    1,\n",
      "         115,    1,  261,  131,  131,  131,  525,    1,  131,  525,    7,  131,\n",
      "         131,  525,  261,  525,  525,    7,  115,    7,   28,  131,  525,    1,\n",
      "         525,  115,   28,  115,  261,  131,  261,  261,  115,    7,  131,  525,\n",
      "           7,  115,  525,  131,  525,  131,  525,  115,    1,  115,  136,  136,\n",
      "           7,  131,    1,  131,  131,  131,  131,    7,  169,  131,    7,    1,\n",
      "         131,    7,  131,  131,    1,  131,    7,  136,  131,  131,  131,  131,\n",
      "         525,  131,  136,  131,  131,  131,  131,    7,    1,  130,    1,  131,\n",
      "         169,  130,    1,  131,    7,    1,    7,  131,  131,  130,  131,  136,\n",
      "           1,  131,  131,  130,  130,    7,  131,    1,    7,  136,    1,    1,\n",
      "           1,  131,    1,    7,  130,  131,  136,  136,  131,    7,  131,  131,\n",
      "         131,    1,    7,  131,  131,  169,  131,  131,  525,  131,  169,    1,\n",
      "         131,  131,  131,  131,  131,  131,  525,  131,  131,    7,  131,  525,\n",
      "         169,    7,  169,    1])\n",
      "\ttensor([ 525,  136,  136,    7,   28,  525,   93,  525,    7,    7,  525,  115,\n",
      "         525,  525,    7,    7,    7,  131,   93,   93,    1,    7,  136,    1,\n",
      "           7,  115,  131,   93,  131,   28,   28,  131,  131,    7,  136,   28,\n",
      "         136,   28,  525,    1,   28,    7,  131,   28,  136,    7,  136,  525,\n",
      "         136,  525,    7,  131,  525,    7,  525,   28,   28,    7,  525,  131,\n",
      "          28,  136,  115,   28,    1,  131,  136,  525,  136,  525,    7,    1,\n",
      "         136,  525,    7,    7,  525,  525,  525,    1,  131,  115,  136,   93,\n",
      "         115,  525,    7,  136,  136,  525,  136, 1203,  525,  136,  131,  136,\n",
      "         136,    7,  131,  525,    1,  131,  131, 2034,    7,  241,  131,   28,\n",
      "         131,  131,  131,  241,    1,    1,  131,  241,  131,  136,  131,    1,\n",
      "         241,  241,  131,  131,  131,  131,  169,    1,    1, 2034,  131,  131,\n",
      "         241,  131,  689,  241,    1,  241,  131,  241,  689,   28,  241,  131,\n",
      "           1,  131,    1,  131,   28,  241,  131,  131,  131,  131, 2034,  131,\n",
      "           1,  136,  131,    1,    1,  131,  131,  136,    1,  241,  131,  131,\n",
      "         169,  131,  131,  131,  131,  131,    1,  131,  131,  241,  131,  241,\n",
      "         131,  131,  136,  136,  241,  131,  131,  525,  241,    1,  689,    1,\n",
      "         131,  241,  131,  241,  131,    1,    1,  131,    1,  136,    1,  525,\n",
      "         131,  136,  525,   93,  525,  131,    7,  525,  136,  136,    1,  525,\n",
      "         136,  131,  525,  136,  136,    7,  131,    1,   93,  525,  115,  241,\n",
      "         525,  525,    1,  136,  241,  525,  131,    1,  525,   93,  131,    7,\n",
      "         131,  131,    7,  136,  136,  525,  131,  169,    1,  136,   93,    1,\n",
      "         525,    1,  136,    1,  136,    7,    1,    1,  131,    1,  241,  131,\n",
      "           1,    1,  136,  131,    1,  169,  525,  131,    1,  525,    1,    1,\n",
      "         131,  525,    1,  525,  136,    7,  136,  136,  525,  169,  131,    1,\n",
      "          93,  525,    1,  115,    1,    1,    1,  525,    1,  525,    1,    1,\n",
      "         131,  241,  136,  131,  525,  525,  525,  131,    7, 1203,  131,  136,\n",
      "         241,  131,  131,  131,  136, 1203,  131,    1, 1203,  115,    1, 1203,\n",
      "         131,  131,  131,  525,  525,  525,  525,  131,    1,    1,  136, 1203,\n",
      "           7,    1,    1,    7,  131,    1,  131, 1203,    7,    1,  131,  131,\n",
      "           1,  525, 1203,    1,    1,  131,  131,  136,  131,  131,    1,    1,\n",
      "         136,  241,  136,  131,    1,  131,    1,    1,  525,    7, 1203, 1203,\n",
      "         115,    1,  131,  131,  525,    7,  525, 1203,  131,  525,    7,    1,\n",
      "         525,  131, 1203,  525,  131,    7,  136,  241, 1203,  131,  115,    7,\n",
      "           7,  131,    7,    1])\n",
      "\ttensor([   7,    1,    1,  131,  130,    1,  131,    7,  131,  131,    1,  136,\n",
      "         131,  525,  131,    1,  131,    7, 1203,  525,  131,  689,    1,  131,\n",
      "         689,  136,  131, 1203,  131,  136,  136,    7,    7,  136,    1,  130,\n",
      "           1,  130,  525,  131,  136,    1,    7,  136,    1,  131,    1,  136,\n",
      "          93,  131,  131,  115,  136,  136,  115,  136,  130,  131,  136,    7,\n",
      "         130,    1,  136,  136,   60,  115,    1,    7,    1,  525,  136,  131,\n",
      "           1,    7,  131,  131,  525,    7,  136,   60,    1,  136,  136, 1203,\n",
      "         136,  131,  131,    1,    1,    1,    1,    7,  136,  136,  131,    1,\n",
      "           1,    1,  136,  525,    1,    1,  115,    7,  136,    1,    1,  131,\n",
      "           1,  136,  136,   93,    1,    1,  136,  169,    1,  131,  136,    1,\n",
      "         131,  131,    1,  136,    1,    1,  525,    7,    7,    7,    1,    1,\n",
      "          93,    1,  131,    1,    1,  131,    7,   93,  131,  131,  131,    7,\n",
      "           1,  131,    7,    7,  169,  131,    7,    1,    1,  131,    7,    7,\n",
      "           1,  136,  131,    1,    1,  525,  136,  136,    1,   93,    1,    1,\n",
      "         525,    7,  136,    1,    1,    7,    1,    1,    1,   93,    1,  131,\n",
      "           1,  136,  136,  131,  131,  525,  136,    7,   93,    1,  131,    1,\n",
      "         131,  131,  131,  131,  136,    1,    1,  136,    7,  131,    7,    7,\n",
      "         136,    7,    7,    7,    7,  136,  131,    7,  131,    7,   93,    7,\n",
      "         131,   93,    7,    7,  131,  131,   93,  525,  136,    7,    7,  525,\n",
      "           7,    1,  131,    7,  525,    7,   93,    7,    7,    7,  136,  131,\n",
      "         525,  136,  131,    7,    7,    7,  136,  136,  525,    7,    7,   93,\n",
      "           7,   93,    7,    7,  131,  131,  525,   93,  131,  689,  525,  131,\n",
      "           7,    1,    7,   93,  525,  136,    7,    7,  525,  115,  525,   93,\n",
      "          93,    7,  525,    7,    7,  131,    7,    7,    7,  136,  136,    1,\n",
      "           1,    7,  525,    7,  689,    7,  525,    7,   93,    7,    1,  525,\n",
      "           7,  115,  131,  689,  131,  872,    7,    7,  131,  131,    7,  136,\n",
      "         115,    7,  136,  131,  131,  131,    7,   60,  131,  131,    1,  131,\n",
      "         131,  136,    1,  131,  131,  131,    1,    7,    7,  131,  131,  131,\n",
      "         131,  131,  525,  131,    7,  136,    7,  131,  131,  131,  136,    1,\n",
      "         131,    7,  131,  131,  131,    7,  131,  131,    7,    1,  525,    7,\n",
      "         131,  115,  131,    7,  131,  525,   60,   60,  872,    7,  131,  131,\n",
      "         131,  131,    7,  131,    1,  131,  131,  131,  131,  131,  131,  131,\n",
      "         872,  689,  131,  131,  525,  131,  131,  115,  131,    7,  131,    1,\n",
      "         131,    7,  136,  131])\n",
      "\ttensor([  115,   169,     1,   131,   525,   136,   525,     1,   131,   131,\n",
      "          136,   136,   115,     1,   131,     7,   131,   136,     7,   131,\n",
      "            1,     1,     1,   115,     1,   136,     7,     7,     1,   241,\n",
      "         3460,   525,   136,   689,   169,   525,   136,   525,     1,   115,\n",
      "         3460,     7,   525,    60,   136,   241,    93,     1,     7,   115,\n",
      "          241,     1,     1,   426,   131,  1203,   525,   131,     1,   136,\n",
      "          525,   169,   131,  1203,   525,     1,   136,     1,    93,   115,\n",
      "            7,     7,    93,   115,   131,   241,   115,     1,     1,   525,\n",
      "            1,   136,   115,     7,   136,     1,   131,   131,     1,   136,\n",
      "          169,   131,     1,   115,     1,     1,   136,     7,   525,     1,\n",
      "            1,     7,    60,     1,     1,     7,   130,   136,   130,   131,\n",
      "          131,   525,     7,   131,     7,   131,     7,     1,   131,     7,\n",
      "            1,     1,     7,   131,   131,   130,     1,   131,   131,    93,\n",
      "          130,     1,   525,     1,     7,     7,     7,     1,   136,  1722,\n",
      "            1,   136,     1,     1,     7,     1,   131,   131,     7,     1,\n",
      "            1,   130,     1,     7,    28,   136,   136,     7,     1,   136,\n",
      "            7,   115,   131,  1383,     1,   525,   525,    90,     1,   136,\n",
      "          131,     7,     7,   131,     7,     1,   130,   525,   131,     1,\n",
      "          130,   131,  1383,     1,     1,     7,   131,   136,  1722,     7,\n",
      "            7,   136,     7,     1,     7,     1,   131,     1,     7,     7,\n",
      "            1,     7,    93,  1722,  1203,    88,  1469,     1,   115,  1203,\n",
      "          131,  1722,     7,    88,   115,    28,   115,     1,  1722,  1722,\n",
      "          115,   115,     1,     7,   525,    88,     7,   115,    88,   525,\n",
      "          115,     7,   115,    28,     1,    93,   159,  1722,  1203,   115,\n",
      "            1,    60,   131,    88,  1722,    88,  1203,     7,   131,  1722,\n",
      "         1722,     1,   689,     1,  1722,   136,     7,   131,     7,   115,\n",
      "            7,   131,     7,     7,   136,     1,    88,     1,     7,     7,\n",
      "          689,   136,   525,     1,     7,     1,     1,   159,   131,    28,\n",
      "            1,   131,     1,    88,  1722,     7,  1203,     1,     7,   689,\n",
      "            7,     7,   131,    93,     7,    28,     1,   159,     1,   136,\n",
      "            1,     1,     1,     7,     1,   136,   131,   525,   115,    93,\n",
      "          136,     1,     1,     1,     1,     7,   115,     1,   136,   525,\n",
      "           93,   115,     1,     1,     7,     1,   131,    93,     1,     1,\n",
      "            1,     1,     1,   115,   115,    93,   115,     7,  1203,     1,\n",
      "          525,   131,   525,    93,     1,   115, 12072,   131,   136,   131,\n",
      "           93,     7,     7,   136,     7,   115,   136,   131,     1,     1,\n",
      "            1,     1,     1,   136,   115,   115,   131,   131,   525,   115,\n",
      "           93,     1,     7,   131,   525,     1,     1,   115,     1,    93,\n",
      "            7,     1,   115,   131,   136,     7,    93,     1,   115,     1,\n",
      "            1,     1,    93,   136,   115,   131,    93,   525,   525,   131])\n",
      "\ttensor([    7,     1,     1,     1,   525,     7,     1,     7,     1,    28,\n",
      "          115,     1,   525,     1,  1722,     7,     7,    28,     7,   131,\n",
      "          131,   131,     7,  1203,   525,     7,   131,     1,     1,   131,\n",
      "            7,   131,   261,   131,   525,     1,     1,     7,     7,    60,\n",
      "            1,   689,   136,     1,   525,     1,   131,   136,     7,  1203,\n",
      "          115,     7,   131,   525,   525,     1,   131,    93,     1,     7,\n",
      "          115,   136,     1,     7,     7,     1,   241,    88,     1,     7,\n",
      "          136,     1,   136,   241,   136,     7,     1,   115,   525,     1,\n",
      "            1,   689,     1,   115,  1346,     7,   159,     1,   115,     1,\n",
      "          689,   525,  1722,   136,   136,    93,     7,   131,  1203,    93,\n",
      "            1,     1,     1,     1,     1,     7,     1,     7,     7,   525,\n",
      "            7,     1,   131,     1,   131,   131,   241,   689,   136,   525,\n",
      "          131,     7,     7,     1,   131,   689,     1,     7,     1,     7,\n",
      "          525,   525,     7,     1,     7,     1,     1,   169,     7,     1,\n",
      "            1,   115,    28,   525,   136,     1,   115,     7,     1,   261,\n",
      "          131,    93,     7,     1,     7,   115,    28,   131,     7,     7,\n",
      "            7,     1,  1722,     1,   136,   115,     7,   131,   689,     1,\n",
      "            1,    72,   115,     7,    93,   131,   130,    88,   136,   689,\n",
      "          426,   471,   115,   525,     1,     1,    88,   131,   689,   130,\n",
      "          115,     1,  1722,   136,    88,   525,   689,   525,   241,   136,\n",
      "          131,   131,   131,     1,   525,     7,   525,     1,   131,   136,\n",
      "            1,     7,   525,     1,     7,     7,     7,     7,   115,   136,\n",
      "          241,   131,   131,   169,     7,     1,     7,    60,     7,     1,\n",
      "          131,     1,  1203,     1,     1,  1203,   136,     7,   689,     1,\n",
      "          689,   131,   136,   115,   131,   115,   131,   136,   131,   689,\n",
      "          689,   689,     1,     1,     1,   525,   136, 12072,     1,   115,\n",
      "            1,     1,     1,     7,  1203,     1,    93,   136,    29,   131,\n",
      "            7,   131,     1,   131,   136,   136,  1346,   471,   136,   689,\n",
      "           60,   525,  1203,   159,    93,   525,   471,   689,     1,   115,\n",
      "           93,     1,     7,    60,   131,    93,    29,     1,   525,   131,\n",
      "            1,     7,   131,   131,     7,   131,   131,     7,     1,     7,\n",
      "            1,     1,     1,     7,   136,     1,     7,   131,   525,   131,\n",
      "          241,     7,     1,   136,     1,   131,     7,     1,     1,  1722,\n",
      "            1,   115,     7,     1,     1,   131,     7,   131,   131,     1,\n",
      "          115,     7,     1,   525,     7,     7,     7,   115,     1,   136,\n",
      "            1,     1,   136,   689,     7,   689,   136,     1,     1,   115,\n",
      "            1,   115,   136,    93,    28,   241,   525,   241,     1,     7,\n",
      "          130,   525,   525,  1203,   241, 12072,     7,    93,    88,    93,\n",
      "            7,   115,   689,   525,   136,   872,    28,   159,  1203,   115,\n",
      "            1,    93,   115,   130,    88,     1,   136,    93,     7,    60])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([32, 24, 99, 69,  4,  7, 89, 18, 25, 71, 85,  7, 95,  8, 34, 44,  4, 73,\n",
      "        66, 14, 95, 24, 83, 61, 20, 76, 91, 69,  7, 66, 15, 20, 40,  4, 15, 23,\n",
      "        80, 32, 37, 14, 13, 73, 53, 10, 97, 74, 24, 43,  8, 15, 37, 39, 10, 14,\n",
      "        14, 50, 37, 59, 76, 86, 89,  0, 67,  9, 43, 85, 25, 60,  6, 93, 97,  8,\n",
      "        62, 87, 13, 37,  8, 79, 35,  7, 32, 87, 95, 34, 25, 95, 20, 19, 10, 12,\n",
      "        43, 85, 67, 61, 12, 16, 66, 80,  8, 73])\tToken idx: tensor([ 131,    1, 1732,    1,    1,  136,  525,  131,    7,  131,  241,  131,\n",
      "         525,    7,  131,  131,    7,    1,    7,  136,  689,  136,  131,  241,\n",
      "           7,    7,    1,    7,  525,    1,    1,    1,  131,  115,  525,  131,\n",
      "           7,    7,  136,    1,  131,    7,    7,    1,    1,  131,  131,   88,\n",
      "           1,  136,  525,    1,    7,  131,  525,    1,  689,    1,  115,    1,\n",
      "         131,    7,    7,    7,    7,   72,  241,  525,  131,    7, 1732,  115,\n",
      "         136,    7,  136,  169, 1722,  525,    1,  689,  136,    1,  130,  136,\n",
      "         131,  169,   93,  525,  115,    1,  115,   60,    1, 1203,    7,    1,\n",
      "         115,  115,  159,  131])\tRaw indices: indices_to_process=tensor([1067427,  800473, 3303679, 2301358,  133413,  233607, 2968942,  600485,\n",
      "         833832, 2368194, 2835246,  233602, 3169060,  266831, 1134133, 1467663,\n",
      "         133419, 2434770, 2201305,  467078, 3169224,  800608, 2768430, 2034774,\n",
      "         667067, 2534835, 3035124, 2301364,  233996, 2201299,  500296,  667061,\n",
      "        1334251,  133527,  500820,  767250, 2668247, 1067303, 1234197,  466943,\n",
      "         433720, 2434776, 1767716,  333531, 3235242, 2468253,  800603, 1434267,\n",
      "         266825,  500431, 1234586, 1300768,  333537,  467073,  467467, 1667651,\n",
      "        1234750, 1967828, 2534943, 2868359, 2968548,       7, 2234658,  300184,\n",
      "        1434186, 2835077,  834066, 2001705,  200249, 3101836, 3236973,  266939,\n",
      "        2068022, 2901718,  433725, 1234230,  268546, 2635412, 1167356,  234160,\n",
      "        1067432, 2901712, 3168665, 1134138,  833956, 3168704,  667153,  634232,\n",
      "         333645,  400237, 1434294, 2835065, 2234652, 2035736,  400243,  533649,\n",
      "        2201413, 2668355,  266983, 2434900])\n",
      "Beam idx: tensor([50, 33, 26, 12, 50,  6, 29,  6, 95, 26, 52, 56, 26,  6, 46,  6, 28, 95,\n",
      "        79, 83, 73, 29, 44, 21, 44, 87, 58, 52, 33, 33, 50, 52, 27, 33,  2, 55,\n",
      "        55, 82, 26, 95, 56, 34, 29, 95, 52, 46, 13, 10, 76, 95,  0, 95, 81, 84,\n",
      "        85, 80, 34,  8,  3, 39, 50, 99, 60, 27, 88, 97, 44, 56, 84, 50, 59,  6,\n",
      "         3, 36, 97,  8, 95, 16, 47, 29, 24, 12, 56, 97, 56, 95, 26, 46, 52, 24,\n",
      "        49, 70, 74, 45, 46, 52,  1, 46, 55, 56])\tToken idx: tensor([ 525,  131,  525,    1,  136,    7,    7,    1,  131, 1203,    1,  525,\n",
      "          60,  115,    1,   88,    1,    7,    7,    1, 1732,  525,    7,    1,\n",
      "           1,  131,    1,  115,  136,  169,  689,    7,    1,  525,    1,  131,\n",
      "         136,  525,  241,  525,   60,    7,  131,  241,   93,  115,    1,  136,\n",
      "         525,    1,    7,  136,    1,    1,  525,    1,    1,    7,    7,    7,\n",
      "         471,    1,  525,    7,    1,    1,   93,    1,    7,  131,    1, 1722,\n",
      "           1,    7,  131,    1,   93,    1,    7,    1,    7,    7,  130,  136,\n",
      "         136,  130,  130, 1722,  136,  115,    1,    1,  525,    1,   93,  159,\n",
      "           1,    7,  169,   93])\tRaw indices: indices_to_process=tensor([1668175, 1100780,  867703,  400237, 1667786,  200125,  967244,  200119,\n",
      "        3168666,  868381, 1734357, 1868293,  867238,  200233, 1534239,  200206,\n",
      "         933885, 3168542, 2634894, 2768300, 2436501,  967762, 1467539,  700414,\n",
      "        1467533, 2901842, 1934475, 1734471, 1100785, 1100818, 1668339, 1734363,\n",
      "         900532, 1101174,   66707, 1834546, 1834551, 2735471,  867419, 3169060,\n",
      "        1867828, 1134009,  967368, 3168776, 1734449, 1534353,  433590,  333666,\n",
      "        2535353, 3168536,       7, 3168671, 2701594, 2801653, 2835530, 2668241,\n",
      "        1134003,  266831,  100066, 1300774, 1668121, 3301948, 2001705,  900538,\n",
      "        2935065, 3235242, 1467625, 1867769, 2801659, 1667781, 1967828,  201840,\n",
      "         100060, 1200715, 3235372,  266825, 3168628,  533649, 1567598,  967238,\n",
      "         800479,  400243, 1867898, 3235377, 1867904, 3168665,  867308, 1535960,\n",
      "        1734492,  800587, 1634298, 2334711, 2468647, 1500886, 1534331, 1734515,\n",
      "          33354, 1534245, 1834584, 1867861])\n",
      "Beam idx: tensor([92, 80, 92, 58, 83, 94, 32,  3, 99, 99, 11,  6, 25, 58,  2, 20,  2, 58,\n",
      "        40, 99, 77, 73, 75, 79, 33, 33, 58, 86, 97, 12, 20, 82, 32, 79, 25, 30,\n",
      "        96,  2, 20,  2, 32,  6, 11, 94, 40, 58,  0, 38, 96, 35, 21, 92, 13, 80,\n",
      "        25, 91, 94, 90, 14, 40, 72, 73, 79, 63, 33, 95, 72, 91,  1, 82, 99, 80,\n",
      "        23,  5, 13, 26,  4, 58, 51, 16, 78, 40, 86, 33, 77, 77, 90, 40, 19,  2,\n",
      "        16, 10, 99, 99,  1, 95, 77, 21, 58, 96])\tToken idx: tensor([ 131,  131,  525,  525,    1,    1,  131,  131,  525,    1,    1,    1,\n",
      "         689, 1203,   88,    7,    7,   60,    7,  241,    1,    1,    1,    1,\n",
      "         136,  131,  131,  241,  131,  131,  115,    1,  136,    7,  136,    1,\n",
      "           1,  115,    1,    1,    1,    7,    7,  525,  131,  241,    7,    1,\n",
      "         136, 1732,  131, 1203,  136,  136,  131,    7,   60,   60,    7,    1,\n",
      "           7,  525,  131,    1,    1,    7,  115,    1,    7,  115,    7,    1,\n",
      "        1732,    7,    1,    7,    7,  136,  525,    1,  131,  115, 1203,  689,\n",
      "          60,  525, 1203,  136,  525, 1722,  115,    1,   72, 1203,    1,  689,\n",
      "         241,  136,    1,  131])\tRaw indices: indices_to_process=tensor([3068607, 2668371, 3069001, 1934999, 2768300, 3135183, 1067427,  100190,\n",
      "        3302472, 3301948,  366884,  200119,  834514, 1935677,   66794,  667067,\n",
      "          66713, 1934534, 1334127, 3302188, 2568182, 2434770, 2501476, 2634888,\n",
      "        1100785, 1100780, 1934605, 2868599, 3235372,  400367,  667175, 2734947,\n",
      "        1067432, 2634894,  833961, 1000591, 3201889,   66821,  667061,   66707,\n",
      "        1067297,  200125,  366890, 3135707, 1334251, 1934715,       7, 1267415,\n",
      "        3202024, 1169087,  700544, 3069679,  433725, 2668376,  833956, 3035130,\n",
      "        3135242, 3001830,  466949, 1334121, 2401423, 2435294, 2635018, 2101240,\n",
      "        1100650, 3168542, 2401531, 3035124,   33360, 2735061, 3301954, 2668241,\n",
      "         768851,  166772,  433590,  867185,  133419, 1934610, 1701528,  533649,\n",
      "        2601665, 1334235, 2869561, 1101338, 2568241, 2568706, 3002973, 1334256,\n",
      "         634232,   68428,  533763,  333531, 3302019, 3303150,   33354, 3169224,\n",
      "        2568422,  700549, 1934475, 3202019])\n",
      "Beam idx: tensor([17, 41, 57, 58,  9, 58, 22, 41, 94, 81, 57, 69, 13, 88, 62, 91, 49, 32,\n",
      "        47, 91, 47, 10, 33, 54,  5, 16, 78, 55, 84,  4, 79, 84, 10, 69, 61, 91,\n",
      "        69,  8, 69, 57, 34, 78, 81, 97, 57, 72, 57, 36, 69, 41, 80, 64, 41, 88,\n",
      "        32, 10, 84,  9, 21, 58,  5, 32, 37, 22, 32, 88, 88, 58, 58, 91, 54, 69,\n",
      "        41,  0,  4, 16,  4, 19,  5, 88, 69, 19,  2, 80, 49, 40, 72, 54, 18, 50,\n",
      "        41, 84, 29, 36, 80, 61, 14, 11, 81, 58])\tToken idx: tensor([  131,     1,   689,   136,     1,   525,   131,     7,   525,     1,\n",
      "          131,     1,     1,     1,     1,   131,     1,     1,   241,     1,\n",
      "            7,     1,     1,     1,     1,     7,   525,     1,     1,     7,\n",
      "            1,   115,     7,   136,   525,     7,   131,     7,     7,     7,\n",
      "            1,   131,     7,     1,    90,   136,   136,   131,   689,   689,\n",
      "            7,     1,  1346,   525,     7,   689,     7,   136,     1,   131,\n",
      "          689,   689,     1,   136,    93,    93,   131,   471, 12072,   136,\n",
      "            7,   169,    93,     7,   115,     1,     1,     7,     7,   169,\n",
      "           93,     1,     7,   115,     7,     7,   471,  1722,     7,   131,\n",
      "           90,    93,     1,   525,     1,     1,     7,   131,   115,   689])\tRaw indices: indices_to_process=tensor([ 567132, 1367474, 1901810, 1934610,  300178, 1934999,  733897, 1367480,\n",
      "        3135707, 2701594, 1901252, 2301358,  433590, 2935065, 2067887, 3035254,\n",
      "        1634298, 1067297, 1567832, 3035124, 1567598,  333531, 1100650, 1801063,\n",
      "         166766,  533655, 2602059, 1834416, 2801653,  133419, 2634888, 2801767,\n",
      "         333537, 2301493, 2035058, 3035130, 2301488,  266831, 2301364, 1901128,\n",
      "        1134003, 2601665, 2701600, 3235242, 1901211, 2401552, 1901257, 1200839,\n",
      "        2302046, 1368162, 2668247, 2134593, 1368819, 2935589, 1067303,  334219,\n",
      "        2801659,  300313,  700414, 1934605,  167454, 1067985, 1234062,  733902,\n",
      "        1067389, 2935157, 2935195, 1934945, 1946546, 3035259, 1801069, 2301526,\n",
      "        1367566,       7,  133527,  533649,  133413,  633714,  166772, 2935233,\n",
      "        2301450,  633708,   66713, 2668355, 1634304, 1334127, 2401887, 1802784,\n",
      "         600361, 1667781, 1367563, 2801745,  967238, 1201233, 2668241, 2034534,\n",
      "         466949,  367014, 2701708, 1935163])\n",
      "DEB: t.shape=torch.Size([10, 400])\n",
      "tensor=(\n",
      "\ttensor([ 235, 2416,  305,  305,   50,    7,    7,   60,  159,   60,  305,    7,\n",
      "           1,  118,    1,    1,   50,    7,    1,  131,    1, 2416,   60,    1,\n",
      "          60,  305,  235,  305,    7,    1,  447,   60,   50,   50,  447,   60,\n",
      "           7,  235,   50,  131,  305,    7, 2416,    7,  447,  118, 2416,   50,\n",
      "         118,  447,   50,   60,    7,  131,  131,    7,   50,  235,  305,  131,\n",
      "           7,    7,    7,  131,   50,  305,  159,   50,   60,    7,  447,  118,\n",
      "         159,    1,  305,   50,  118,    1,   50,    7,  235,    1,    1,    1,\n",
      "         159,    1,   60, 2416,    7,  305,   50,  305,    7,    1,  305,  131,\n",
      "           1,    7,  118,    7, 1346,  471,  998,    1, 1346,  119,    1,  119,\n",
      "         518,  998,  471,  998,  998,  119,  528,  119,  528,  518,  518,   93,\n",
      "         119,    1,    1,  518,    1,  119,  130,  471,  471,  471, 1346,  471,\n",
      "         528,  471,  343,  343,  343,   93,  998,  518,  998, 1722,    1,  518,\n",
      "         471,  528,  998,   29,  119,  518,    1,  518,  130,  518,   29,  119,\n",
      "        1722,  119,    1,  518, 1346,  130,    1,  528,  518,    1,    1,  998,\n",
      "         518, 1346,  998,  119,    1,    1,    1,  119,  518,  343,  343,    1,\n",
      "         119,    1,  998,    1,  998,  518,  998,  528,  471,  119,  518,  130,\n",
      "           1,  130,  528,  471,  471,  528,  343,  998,  525,  525,  525,  525,\n",
      "         525,  525,  525,  131,  136,  136,  131,  131,  131,  525,  525,  525,\n",
      "         525,  525,  431,  136,  131,  131,  136,  131,  131,  131,  525,  130,\n",
      "         131,  525,  525,  525,  525,  131,  131,  525,  136,  525,  525,  525,\n",
      "         525,  131,  131,  525,  431,  525,  525,  130,  136,  525,  131,  525,\n",
      "         525,  525,  131, 1491,  525,  525,  136,  431,  525,  131,  131,  130,\n",
      "         131,  131,  525, 1491,  525,  525,  136,  525,  525,  525,  525, 1491,\n",
      "         130,  525,  136,  525,  525,  431,  130,  131,  131,  131,  525,  431,\n",
      "         525,  525,  525,  426,  136,  136,  525,  131,  131,  131,  525,  136,\n",
      "         689,  525,  525,  525,  689,  525,  882,  525,  525,    8,  525,  525,\n",
      "         882,  261,    1,  525,  525,  525,  525,  525,  525,  882,  525,  261,\n",
      "         525,    1,    8,    1,  525,    8,  689,  525,  882,  525,  525,  525,\n",
      "         525,  525,  525,  525,    1,    8,    8,  882,  525,  525,  525,  525,\n",
      "         525,  525,  525,  525,  525,  261,  525,  882,  525,  689,  525,  525,\n",
      "         525,  525,  525,  882,  525,  261,  261,  525,  525,  525,  261,  525,\n",
      "         525,  882,    8,    1,    8,  525,  525,  261,  525,  525,    1,  525,\n",
      "         525,  882,  525,  261,  882,  689,  525,  525,    8,  525,  525,  525,\n",
      "         261,    1,    8,  525])\n",
      "\ttensor([ 525,  525,  525,  525,  525,  169,    1,  525,   93,  525,  525,  169,\n",
      "           1,  131,    1,    1,  525,  169,    1,  525,    1,  525,  525,    1,\n",
      "         525,  525,  525,  525,  169,    1,  525,  525,  525,  525,  525,  525,\n",
      "         169,  525,  525,  525,  525,  169,  525,    1,  525,  131,  525,  525,\n",
      "         131,  525,  525,  525,    1,  525,  525,  169,  525,  525,  525,  525,\n",
      "           1,  169,  169,  525,  525,  525,   93,  525,  525,  169,  525,  131,\n",
      "          93,    1,  525,  525,  131,    1,  525,  169,  525,    1,    1,    1,\n",
      "          93,    1,  525,  525,    1,  525,  525,  525,  169,    1,  525,  525,\n",
      "           1,  169,  131,  169,  525,  525,  525,  118,  525,  525, 1165,  525,\n",
      "         525,  525,  525,  525,  525,  525,  525,  525,  525,  525,  525,    1,\n",
      "         525, 1165,  118,  525,  118,  525,    1,  525,  525,  525,  525,  525,\n",
      "         525,  525,  525,  525,  525,    1,  525,  525,  525,  130, 1165,  525,\n",
      "         525,  525,  525,  115,  525,  525,  118,  525,    1,  525,  115,  525,\n",
      "         130,  525, 1165,  525,  525,    1,  118,  525,  525,  118,  118,  525,\n",
      "         525,  525,  525,  525, 1165,  118,  118,  525,  525,  525,  525, 1165,\n",
      "         525,  118,  525,  118,  525,  525,  525,  525,  525,  525,  525,    1,\n",
      "         118,    1,  525,  525,  525,  525,  525,  525, 1893,  426, 1893, 1893,\n",
      "         426, 1893,  169,    1,  131,  131,    1,    1,    1, 1893,  426,  115,\n",
      "         426, 1893,  525,  131,    1,    1,  131,    1,    1,    1, 1893,    1,\n",
      "           1,  115,  115,  426,  169,    1,    1, 1893,  131,  426,  115,  426,\n",
      "         169,    1,    1, 1893,  525, 1893,  426,    1,  131,  510,    1, 1893,\n",
      "         426,  426,    1,  525, 1893, 1893,  131,  525,  426,    1,    1,    1,\n",
      "           1,    1,  426,  525,  115,  426,  131,  426, 1893,  426,  426,  525,\n",
      "           1, 1893,  131,  115,  426,  525,    1,    1,    1,    1, 1893,  525,\n",
      "         426,  426,  115,  525,  131,  131,  115,    1,    1,    1, 1893,  131,\n",
      "         525, 1893,  510, 1893,  525, 1893,  525, 1893,   78,  525,  510,  704,\n",
      "         525,  525,    7,  579,   91,  131,  510,  579,  510,  525,  874,  525,\n",
      "          91,    7,  525,    7,   91,  525,  525,   91,  525,  704,  579,  579,\n",
      "         704,  510,  704,  510,    7,  525,  525,  525,  510,   78,  510,  510,\n",
      "         704, 1893,   60,  874, 1893,  525,  131,  525,   91,  525,   78, 1893,\n",
      "          91,  131,  874,  525,  131,  525,  525, 1893, 1893,  579,  525,  704,\n",
      "        1893,  525,  525,    7,  525,  518,   91,  525,  704,  518,    7,   60,\n",
      "          91,  525,   78,  525,  525,  525, 1893,   91,  525,  510,   60,  579,\n",
      "         525,    7,  525, 1893])\n",
      "\ttensor([  7, 525, 525, 525, 115,   7,   7,   1,   7,   1, 525,   7, 261, 525,\n",
      "        261, 261, 115,   7, 261, 525, 261, 525,   1, 261,   1, 525, 261, 525,\n",
      "          7, 261, 261,   1, 115, 115, 261,   1,   7,   7, 115, 525, 525,   7,\n",
      "        525,   7, 261, 525, 525, 115, 525, 261, 115,   1,   7, 525, 525, 115,\n",
      "        115,   7, 525, 525,   7,   7,   7, 525, 115, 525,   7, 115,   1,   7,\n",
      "        261, 525,   7, 261, 525, 115, 525, 261, 115,   7,   7, 261, 261, 261,\n",
      "          7, 261,   1, 525,   7, 525, 115, 525,   7, 261, 525, 525, 261,   7,\n",
      "        525,   7, 130,   7,   1,   7, 130,   7, 525,   7, 525,   1,   7,   1,\n",
      "          1,   7,   7,   7,   7, 525, 525, 115,   7, 525,   7, 525,   7,   7,\n",
      "        159,   7,   7,   7, 130,   7,   7,   7, 525, 525, 525, 115,   1, 525,\n",
      "          1, 525, 525, 525,   7,   7,   1, 115,   7, 525,   7, 525, 159, 525,\n",
      "        115,   7, 525,   7, 525, 525, 130, 159,   7,   7, 525,   7,   7,   1,\n",
      "        525, 130,   1,   7, 525,   7,   7,   7, 525, 525, 525, 525, 159,   7,\n",
      "          1,   7,   1, 525,   1,   7,   7, 159, 525, 159,   7, 159,   7,   7,\n",
      "          7,   7, 525,   1, 115,   7, 115, 115,   7, 115,   7, 130,   7,   7,\n",
      "        130, 130, 130, 115, 115, 131, 115, 115,   1,   7, 130, 525, 115, 130,\n",
      "        130, 130, 115, 115, 130, 131, 131,   7,   7, 130, 130, 115, 115, 115,\n",
      "        131, 115,   7, 130, 130, 115,   1, 115, 115, 115, 115,   1,   1, 115,\n",
      "          7,   7, 130, 525, 115, 115, 115,   1,   7, 525, 130, 115, 130, 130,\n",
      "          7, 525, 131,   7,   7,   7, 115,   7,   7, 525, 115, 115, 115, 131,\n",
      "          7,   1, 115, 130, 130, 130, 115,   1,   7, 115, 131, 525,   7,   7,\n",
      "        131, 130, 130,   1, 115, 115, 136, 525,   7, 525, 136, 525, 525, 525,\n",
      "          7, 525,   7, 525, 525, 525,   7,   7,   1, 525,   7,   7,   7, 525,\n",
      "          7, 525,   1,   7, 525,   7,   1, 525, 136,   1, 525, 525,   7,   7,\n",
      "        525,   7, 525,   7,   7, 525, 525, 525,   7,   7,   7,   7, 525, 525,\n",
      "        525,   7, 525, 525, 525, 525,   1, 136,   7, 525,   1, 525,   7, 525,\n",
      "        525, 525, 525, 525, 525,   7, 525, 525, 525, 525, 525,   7, 525, 525,\n",
      "          1, 525, 525, 525,   7, 525,   1, 525,   7, 525, 525, 136, 525,   1,\n",
      "        525,   7, 525,   7, 525,   7, 525, 525])\n",
      "\ttensor([  7,   7, 525, 525, 525, 131,   1, 131, 241, 131, 525, 131, 131, 131,\n",
      "        131, 131, 525, 131, 131,   1, 131,   7, 131, 131, 131, 525, 525, 525,\n",
      "        131, 131, 131, 131, 525, 525, 131, 131, 131,   7, 525,   1, 525, 131,\n",
      "          7,   1, 131, 131,   7, 525, 131, 131, 525, 131,   1,   1,   1,  93,\n",
      "        525,   7, 525,   1,   1, 131, 131,   1, 525, 525, 241, 525, 131, 131,\n",
      "        131, 131, 241, 131, 525, 525, 131, 131, 525, 131,   7, 131, 131, 131,\n",
      "        241, 131, 131,   7,   1, 525, 525, 525, 131, 131, 525,   1, 131, 131,\n",
      "        131, 131,   7, 115, 131,  28,   7,   7,   7,   7,   1, 131, 115, 131,\n",
      "        131,   7,   1,   7,   1,   1,   1, 525,   7,   7,  28,   1,  28,   7,\n",
      "        131, 115, 115, 115,   7, 115,   1, 115, 131, 131, 131, 525, 131,   1,\n",
      "        131,   7,   7,   1, 115,   1, 131, 525,   7,   1,  28,   1, 131,   1,\n",
      "        525,   7,   7,   7,   7,   1,   7, 131,  28,   1,   1,  28,  28, 131,\n",
      "          1,   7, 131,   7,   7,  28,  28,   7,   1, 131, 131,   7, 525,  28,\n",
      "        131,  28, 131,   1, 131,   1, 115, 525,   1, 131,  28, 131,   1, 115,\n",
      "        115,   1, 131, 131, 131, 131, 131, 131, 131, 131, 115,   1, 525, 525,\n",
      "          1,   1,   1, 131, 131,   7, 131, 131,  93, 525,   1, 241, 525,   1,\n",
      "          1,   1, 131, 525,   1,   7,   7, 131, 115,   1,   1, 131, 525, 131,\n",
      "          7, 131, 115,   1,   1, 131,  93, 131, 131, 525, 525, 525, 525, 131,\n",
      "        131, 131,   1, 131, 131, 131, 525,  93, 131, 241,   1, 525,   1,   1,\n",
      "        131, 131,   7, 131, 525, 131, 131, 131, 131, 131, 525, 131, 525,   7,\n",
      "        131,  93, 525,   1,   1,   1, 131,  93, 131, 131,   7, 131, 525, 525,\n",
      "          7,   1,   1, 525, 131, 525, 525,   7, 131,   7, 525,   7,  93,   7,\n",
      "        131,   1, 131,   7,   1, 525,   1, 131, 525, 131, 131, 131, 131,   1,\n",
      "        261, 525, 525,   1,   1,   1, 525,   1, 525, 525,   1,   7, 131, 131,\n",
      "          7, 131,   7, 131,   1,   1,   1,   1, 131, 131, 131, 131,   7,   7,\n",
      "          7, 261,   7, 525, 131,   1, 525, 525, 131,   7, 525, 131, 261,  93,\n",
      "        131, 525, 525,   7,   7, 131, 525,   7,   7,   1,   1,   1,   1, 525,\n",
      "        525, 525,   7, 525,   1,   7, 525,   1, 131, 525,   1, 525,   7, 525,\n",
      "          1, 131,   7, 131, 525,   1,   1,   7])\n",
      "\ttensor([   1,    1,  131,  131,    1,  525,    1,  136, 2443,  136,  131,  525,\n",
      "          60,   93,   60,   60,    1,  525,   60,  131,   60,    1,  136,   60,\n",
      "         136,  131,   60,  131,  525,   60,  131,  136,    1,    1,  131,  136,\n",
      "         525,    1,    1,  131,  131,  525,    1,    1,  131,   93,    1,    1,\n",
      "          93,  131,    1,  136,    1,  131,  131,    7,    1,    1,  131,  131,\n",
      "           1,  525,  525,  131,    1,  131, 2443,    1,  136,  525,  131,   93,\n",
      "        2443,   60,  131,    1,   93,  241,    1,  525,    1,   60,   60,   60,\n",
      "        2443,   60,  136,    1,    1,  131,    1,  131,  525,   60,  131,  131,\n",
      "          60,  525,   93,  525,  136,  131,    1,    1,  136,    7, 1203,    7,\n",
      "         131,    1,  131,    1,    1,    7,    7,    7,    7,  131,  131, 1203,\n",
      "           7, 1203,    1,  131,    1,    7,  131,  131,  131,  131,  136,  131,\n",
      "           7,  131,    1,    1,    1, 1203,    1,  131,    1, 1203, 1203,  131,\n",
      "         131,    7,    1,  131,    7,  131,    1,  131,  131,  131,  131,    7,\n",
      "        1203,    7, 1203,  131,  136,  131,    1,    7,  131,    1,    1,    1,\n",
      "         131,  136,    1,    7, 1203,    1,    1,    7,  131,    1,    1, 1203,\n",
      "           7,    1,    1,    1,    1,  131,    1,    7,  131,    7,  131,  131,\n",
      "           1,  131,    7,  131,  131,    7,    1,    1,  525,  261,  525,  525,\n",
      "         261,  525,    7,  115,  136,  136,  115,  115,  115,  525,  131,  131,\n",
      "         131,  525,  136,  136,  115,  131,    1,  115,  115,  115,  525,  131,\n",
      "         115,  131,  131,  261,    7,  115,  115,  525,    1,  131,  131,  131,\n",
      "           7,  115,  115,  525,  136,  525,  131,  131,    1,  131,    7,  525,\n",
      "         261,  261,  115,  131,  525,  525,    1,  136,   28,  131,  115,  131,\n",
      "         115,  115,   28,  131,  131,  261,  136,  261,  525,  261,  261,  131,\n",
      "         131,  525,    1,  131,   28,  136,  131,  115,  115,  115,  525,  136,\n",
      "         261,  131,  131,  131,  136,  136,  131,  115,  115,    7,  525,    1,\n",
      "         131,    1,  136,    1,  131,    1,  131,    1,  131,  131,  136,    7,\n",
      "           7,  131,    1,  131,  131,    1,  136,  131,  136,    7,  130,  131,\n",
      "         131,    1,  131,    1,  131,  131,  131,  131,    7,    7,  131,  131,\n",
      "           7,  169,    7,  136,    1,  131,  131,    7,  136,  131,  136,  169,\n",
      "           7,    1,  525,  130,    1,  131,    1,    7,  131,  131,  131,    1,\n",
      "         131,    1,  130,  131,    1,  131,  131,    1,    1,  131,  131,    7,\n",
      "           1,    7,  131,    1,  131,  136,  131,  131,    7,  136,    1,  525,\n",
      "         131,    7,  131,  131,    7,  131,    1,  131,  131,  169,  525,  131,\n",
      "         131,    1,  131,    1])\n",
      "\ttensor([ 131,    7,  525,  525,   28,  525,  525,   93,  115,    1,  525,  525,\n",
      "         136,    7,  136,  136,   28,  525,  136,    7,  136,    7,   93,  136,\n",
      "           1,  525, 1203,  525,  525,  136,    7,    1,   28,   28,    7,    1,\n",
      "         131,  131,   28,    7,  525,  525,    7,  525,    7,    7,    7,   28,\n",
      "           7,    7,   28,    1,  525,    7,    7,    7,   28,  131,  525,    7,\n",
      "         525,  525,  525,    7,   28,  525,  115,   28,   93,  136,    7,    7,\n",
      "         115,  136,  525,   28,    7,    1,   28,  525,  131,  136,  136,  136,\n",
      "         115,  136,    1,   93,  525,  525,   28,  525,  525,  136,  525,    7,\n",
      "         136,  131,    7,  525,  131,  131,  169,    1,  131,  131, 2034,  131,\n",
      "         241,  169,  131,    1,  169,  131,    1,  131,    1,  241,  241,  136,\n",
      "         131, 2034,    1,  241,    1,  525,  131,  131,  131,  131,  131,  131,\n",
      "           1,  131,  131,  131,  131,  136,  169,  241,    1,  689, 2034,  241,\n",
      "         131,    1,    1,  131,  131,  241,    1,  241,  131,  241,  131,  131,\n",
      "         689,  131, 2034,  241,  131,  131,    1,    1,  241,    1,    1,    1,\n",
      "         241,  131,    1,  131, 2034,    1,    1,  131,  241,  131,  131, 2034,\n",
      "         131,    1,    1,    1,    1,  241,  169,    1,  131,  131,  241,  131,\n",
      "           1,  131,    1,  131,  131,    1,  131,    1,    1,  136,    1,    1,\n",
      "         136,    1,  241,  525,    1,    1,  525,  525,  525,    1,    1,  136,\n",
      "           1,    1,  131,    1,  525,  525,    1,  525,  525,  525,    1,  131,\n",
      "         525,  136,  136,  136,  241,  525,  525,    1,    1,    1,  136,    1,\n",
      "         241,  525,  525,    1,  131,    1,    1,  131,    1,    1,    7,    1,\n",
      "         136,  136,  525,  115,    1,    1,    1,  131,    1,  525,  525,  131,\n",
      "         525,  525,    1,  115,  136,  136,    1,  136,    1,  136,  136,  115,\n",
      "         131,    1,    1,  136,    1,  131,  131,  525,  525,  525,    1,  131,\n",
      "         136,    1,  136,    7,    1,    1,  136,  525,  525,    7,    1,    1,\n",
      "        1203,    1,  131,    1, 1203,    1,    1,    1,  115,  525,  131,    7,\n",
      "         131,  131,  136,  241,  525,    1,  131,  241,  131,  131,    1,  131,\n",
      "         525,  136,  525,  136,  525,  525, 1203,  525,  131,    7,  241,  241,\n",
      "           7,    7,    7,  131,  136,  525,  525,  131,  131,  115,  131,    7,\n",
      "           7,    1,  131,    1,    1,  131,    1,  131,  525, 1203,  115,    1,\n",
      "         525,    1,    1,    1,    1,  131,  131,    1,    1,  241,  131,    7,\n",
      "           1,  131,  525,  136,  525,    1,  525,  131,    7,    1,  136,  131,\n",
      "         525,  131,  115,  131,  131, 1203,    1,  525,  525,    7,  131,  241,\n",
      "         131,  136,  525,    1])\n",
      "\ttensor([   7,  689,  525,  525,  130,    7,    1, 1203,  136,  131,  131,    7,\n",
      "           1,  131,    1,    1,  130,    7,    1,  131,    1,  689, 1203,    1,\n",
      "         131,  525,    7,  525,    7,    1,    1,  131,  136,  130,    1,  131,\n",
      "           1,    7,  130,  131,  525,    7,  136,    1,    1,  131,  689,  136,\n",
      "         131,    1,  130,  131,    1,  131,  131,  131,  130,    7,  525,  131,\n",
      "           1,    7,    7,  131,  136,  131,  136,  130,  131,  136,    1,  131,\n",
      "         136,    1,  525,  130,  131,   60,  130,    7,    7,    1,    1,    1,\n",
      "         136,    1,  131,  525,    1,  131,  136,  131,    7,    1,  131,  131,\n",
      "           1,    1,  131,    7,    7,    1,  525,    1,    7,    1,    7,    1,\n",
      "         131,  525,    1,    1,  525,    1,    7,    1,    7,  131,  131,  131,\n",
      "           7,    7,    1,  131,    1,    7,  131,    1,    1,    1,    7,    1,\n",
      "           7,    1,  115,    7,    7,  136,  525,  131,    1,  131,    7,  131,\n",
      "           1,    7,    1,  136,    1,  131,    1,  131,  136,  131,  525,    1,\n",
      "         131,    1,    7,   93,    7,  136,    1,    7,   93,    1,    1,    1,\n",
      "         131,    7,    1,    1,    7,    1,    1,    1,  131,    1,    7,    7,\n",
      "           1,    1,    1,    1,    1,  131,  525,    7,    1,    1,  131,  136,\n",
      "           1,  131,    7,    1,    1,    7,    7,    1,  689,    7,  689,  525,\n",
      "           7,  525,  525,    7,  525,  525,    7,    7,    7,  525,    7,  131,\n",
      "           7,  525,  525,  525,    7,  115,   93,    7,    7,    7,  525,  136,\n",
      "           7,  131,  131,    7,  525,    7,    7,  131,   93,    7,  131,    7,\n",
      "         525,    7,    7,  525,  525,  525,    7,  136,   93,    7,  131,  689,\n",
      "           7,    7,    7,    7,  525,  525,   93,  525,  525,  115,    7,  131,\n",
      "           7,    7,  525,    7,  131,    7,  525,    7,  525,    7,    7,    7,\n",
      "         136,  525,   93,  131,  525,  525,  136,    7,    7,    7,  525,  525,\n",
      "           7,    7,  131,  131,  525,  525,  131,    7,    7,  131,  525,   93,\n",
      "         131,  136,    1,  525,  131,  525,    1,  136,  131,  131,    1,    7,\n",
      "           7,  525,  131,  115,    7,    7,    1,  115,    1,    7,  131,  131,\n",
      "         872,  131,  131,  131,  872,  131,  131,  872,    7,    7,  115,  115,\n",
      "           7,  131,    7,    1,  131,  131,  131,    7,    1,  131,    1,  131,\n",
      "           7,  136,  131,  131,  136,  525,    7,    7,  872,  131,  131,  525,\n",
      "         872,    7,  131,    1,    7,  525,  525,  525,  525,  115,  131,    7,\n",
      "         136,    7,  131,  131,  131,   60,  872,  525,    7,   60,  131,  131,\n",
      "           7,    7,  131,  131,    7,  131,  136,  872,  131,  131,  131,  115,\n",
      "         136,  136,  131,  525])\n",
      "\ttensor([ 136,    1,    1,  115,  525,    1,  136,    7,  136,    7,    1,    1,\n",
      "           1,  131,  169,  136,  525,  115,  136,  131,    1,    1,    7,  169,\n",
      "           1,  115,  131,  115,    1,  136,    7,    1, 3460,  525,    7,  115,\n",
      "           1,  136,  525,  131,    1,  115,  426,  136,    7,  131,    1,   60,\n",
      "         131,    7,  525,  115,  136,  131,  131,  241,  525,  136,  115,  131,\n",
      "         136,  115,    1,  131,   60,    1,  136,  525,  525,  115,    7,  131,\n",
      "         131,  131,    1,  525,  131,  525,  525,    1,  136,  131,    1,  169,\n",
      "         136,    1,    1,  131,  136,  115,   60,    1,    1,  169,  115,  131,\n",
      "         136,    1,  131,  115,    1,    1,    1,    7,    1,  130,   93,  130,\n",
      "           1,    1,    1,  136,    1,  130,  131,  130,  131,    1,    1,    1,\n",
      "         131,   93,    7,    1,    7,  136,    1,    1,    1,    1,    1,    1,\n",
      "         131,    1,   60,  136,  136, 1383,    1,    1,  136,    7,   93,    1,\n",
      "           1,  131,  131,  131,  130,    1,    1,    1,  131,    1,    7,  130,\n",
      "           7,  130,    1, 1722,    1,    7,    7,  131, 1722,    1,    7,  136,\n",
      "           1,    1,  136,  130,    1,    7,    1,  130,    1,    7,  131,   93,\n",
      "         131,    7,  136,    1,  136,    1,    1,  131,    1,  131,    1,  131,\n",
      "           7,    1,  131,    1,    7,  131,  136,  136,  131,    1,  131,    7,\n",
      "          88,    7,  115, 1722,  136,  136, 1722, 1469,   88,    7,   93,  115,\n",
      "          93,    7,    1,  136,  159,    1,    1,   28,   28,   28,    7, 1203,\n",
      "         159,    7,  115,    1,  115,   28,   88,  115,    1,   93,  115,   93,\n",
      "         115, 1469, 1722,    7,    1,    7,    1, 1203,    1,   93,  115,  131,\n",
      "          88,    1,   88,    7,    7,    7,  115,    1,  525,    1,   28,    7,\n",
      "          28,   28,  525,    7,    7,    1,  136,    1,    7,   88,   88,    7,\n",
      "        1203,    7,    1,  115,  131,    1, 1203,   28,  159,  159,    7,    1,\n",
      "        1722,   93,  115,  131,  136,  136,    7,   28,  159,  115,    7,    1,\n",
      "           1,  131,  131,    1,   93,    1,    1,  131,  115,    1,  131,  115,\n",
      "           1,  115,    1,    1,  131,    1,  131,    1,  131,  136,  115,    7,\n",
      "         136,  115,    1,  115,  136,    1,   93,  136,  136,  115,    1,    1,\n",
      "         115,  115,  115,  131,  115,    1,    1,  525,  131,    7,  131,  115,\n",
      "         115,  131,    7,  115,  131,  115,    1,  136,  136,   93,  115,    1,\n",
      "         136,    1,    7,    1,    1,  115,  115,    1,    1,    1,    7,  115,\n",
      "         131,    1,    1,  115,    1,  525,  136,  115,  115,  525,    1,    7,\n",
      "         131,  525,    7,    7,  136,   93,  131,  136,    1,  115,    7,    1,\n",
      "           1,    1,    1,    1])\n",
      "\ttensor([ 261,  525,   93,    7,  525,    7,    1,    7,    7,    1,    7,    7,\n",
      "          93,    1,  525,  525,  525,  241,  241, 1722,   93,  525,  115,  136,\n",
      "         131,    1,  525,    7,    7,  241,    7,  131,    1,  525,    7, 1203,\n",
      "           1,  261,    7, 1722,    1,  241,  525,  115,  131,  136,  525,    1,\n",
      "           1,    7,    7,   60,  115, 1722, 1722,  115,    7,    7,    1,  159,\n",
      "           1,    7,   88,   28,    1,    7,    7,  115,    1,  136,  131,    1,\n",
      "           1,    1,    1,    7,    1,    1,    1,    7,  261,    1,   93,  525,\n",
      "           7,   93,  131,  131,  115,  525,    1,    7,   88,  136,  525,    7,\n",
      "         241,    1,    1,  241,  131,    1,    1,  131,  131,    1,    7,    1,\n",
      "         525,    1,    7,   28,    1,    1,  115,    1,    1,  525,  689,  525,\n",
      "           7,    7,  136,    7,  136,  131,    7,    7,    1,    1,  131,    7,\n",
      "           7,    1,    1,  115,  115,  115,    1,  525,   28,    7,    7,  525,\n",
      "           7,  115,    1,    7,  130,  525,    1,  525,  471,    1,    1,  426,\n",
      "           7,    7,    1,    1,  131,  136,    7,    7,  689,  525,  136,   28,\n",
      "           1,  131,    7,    1,    1,    1,  525,    7,  525,  241,    7,    7,\n",
      "         131,  131,   28,  525,   28,  525,    1,  115,    7,  131,  261,    1,\n",
      "          93,    1,  115,    7,    1,  115,  115,   28,    7,   60,    7,    1,\n",
      "         159,  131, 1203,    1,  131,  131,    7,  525,    1,    1,  131,  241,\n",
      "         131,    1,  689,  131,  471,  131,  136,  689,    1,    1,    1,  471,\n",
      "           1,  525,  241, 1203, 1203,  689,    1,  131,   29,  131,  241,  131,\n",
      "        1203,  525,    7,  131,  689,    1,  131,  689,   29, 1203,  131,    7,\n",
      "           1,   60,    1,    1,  131,   93,    7,  689,    1,  131,  689,    7,\n",
      "           1,   93,    1,    1,  131, 1203,  131,   60,  169,    7,    1,    7,\n",
      "         525,    1,  689,    7,  136,  689,  471,    1,  471,  471,   93,  689,\n",
      "         136,  131,    7,    1,  131,  131,  131,   93,  471,  131,    1,   29,\n",
      "         131,    7,    1,    1,    7,    1,    1,    7,   88,  115,    1,    7,\n",
      "           7, 1203,  136,   93,  136,    7,  115,   93,  115,    1,    1,    7,\n",
      "         131,    7,   88,  689,  136,    7,   93,  136,    1,    7,  115,   93,\n",
      "           7,    1,    7,    1,    1,   88,  115,   93,    1,  525,    1,    7,\n",
      "           7,    7,    7,   28,    7, 1203,    7,    1,  136,    7,    7,    1,\n",
      "         131,    7,  131,    1,    7, 1203, 1203,    1,    1,   93,    7,    7,\n",
      "           7,    1,    7,    7,    7,  131,  131, 1203,    7,  131,  131,    7,\n",
      "         136,  115,  525,    7,  525,    1,    7,  136, 1722,    7,    7,  115,\n",
      "         136,    1,  115,    1])\n",
      "\ttensor([  131,     1,  1732,     1,     1,   136,   525,   131,     7,   131,\n",
      "          241,   131,   525,     7,   131,   131,     7,     1,     7,   136,\n",
      "          689,   136,   131,   241,     7,     7,     1,     7,   525,     1,\n",
      "            1,     1,   131,   115,   525,   131,     7,     7,   136,     1,\n",
      "          131,     7,     7,     1,     1,   131,   131,    88,     1,   136,\n",
      "          525,     1,     7,   131,   525,     1,   689,     1,   115,     1,\n",
      "          131,     7,     7,     7,     7,    72,   241,   525,   131,     7,\n",
      "         1732,   115,   136,     7,   136,   169,  1722,   525,     1,   689,\n",
      "          136,     1,   130,   136,   131,   169,    93,   525,   115,     1,\n",
      "          115,    60,     1,  1203,     7,     1,   115,   115,   159,   131,\n",
      "          525,   131,   525,     1,   136,     7,     7,     1,   131,  1203,\n",
      "            1,   525,    60,   115,     1,    88,     1,     7,     7,     1,\n",
      "         1732,   525,     7,     1,     1,   131,     1,   115,   136,   169,\n",
      "          689,     7,     1,   525,     1,   131,   136,   525,   241,   525,\n",
      "           60,     7,   131,   241,    93,   115,     1,   136,   525,     1,\n",
      "            7,   136,     1,     1,   525,     1,     1,     7,     7,     7,\n",
      "          471,     1,   525,     7,     1,     1,    93,     1,     7,   131,\n",
      "            1,  1722,     1,     7,   131,     1,    93,     1,     7,     1,\n",
      "            7,     7,   130,   136,   136,   130,   130,  1722,   136,   115,\n",
      "            1,     1,   525,     1,    93,   159,     1,     7,   169,    93,\n",
      "          131,   131,   525,   525,     1,     1,   131,   131,   525,     1,\n",
      "            1,     1,   689,  1203,    88,     7,     7,    60,     7,   241,\n",
      "            1,     1,     1,     1,   136,   131,   131,   241,   131,   131,\n",
      "          115,     1,   136,     7,   136,     1,     1,   115,     1,     1,\n",
      "            1,     7,     7,   525,   131,   241,     7,     1,   136,  1732,\n",
      "          131,  1203,   136,   136,   131,     7,    60,    60,     7,     1,\n",
      "            7,   525,   131,     1,     1,     7,   115,     1,     7,   115,\n",
      "            7,     1,  1732,     7,     1,     7,     7,   136,   525,     1,\n",
      "          131,   115,  1203,   689,    60,   525,  1203,   136,   525,  1722,\n",
      "          115,     1,    72,  1203,     1,   689,   241,   136,     1,   131,\n",
      "          131,     1,   689,   136,     1,   525,   131,     7,   525,     1,\n",
      "          131,     1,     1,     1,     1,   131,     1,     1,   241,     1,\n",
      "            7,     1,     1,     1,     1,     7,   525,     1,     1,     7,\n",
      "            1,   115,     7,   136,   525,     7,   131,     7,     7,     7,\n",
      "            1,   131,     7,     1,    90,   136,   136,   131,   689,   689,\n",
      "            7,     1,  1346,   525,     7,   689,     7,   136,     1,   131,\n",
      "          689,   689,     1,   136,    93,    93,   131,   471, 12072,   136,\n",
      "            7,   169,    93,     7,   115,     1,     1,     7,     7,   169,\n",
      "           93,     1,     7,   115,     7,     7,   471,  1722,     7,   131,\n",
      "           90,    93,     1,   525,     1,     1,     7,   131,   115,   689])\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9]])"
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_lens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T21:23:11.801792800Z",
     "start_time": "2024-04-15T21:23:11.779194900Z"
    }
   },
   "execution_count": 353
  },
  {
   "cell_type": "code",
   "source": [
    "for start_tokens_elem, start_tokens_lens_elem, decoded_tokens_elem, decoded_lens_elem, log_probas_elem in zip(\n",
    "    start_tokens.T, start_tokens_lens,\n",
    "    decoded_tokens.permute(1, 2, 0), decoded_lens.permute(0, 1), log_probas.permute(0, 1)\n",
    "):\n",
    "    start_tokens_elem = start_tokens_elem[:start_tokens_lens_elem].tolist()\n",
    "    start_words = np.array(lm_model.vocab.get_itos())[np.array(start_tokens_elem)]\n",
    "    \n",
    "    print(' '.join(start_words))\n",
    "    start_text = ' '.join(start_words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{start_text}</b></div>'))\n",
    "    \n",
    "    for idx, (hyp, hyp_len, hyp_log_prob) in enumerate(zip(decoded_tokens_elem, decoded_lens_elem, log_probas_elem)):\n",
    "        if idx >= 3:\n",
    "            break\n",
    "            \n",
    "        hyp = hyp[:hyp_len].tolist()\n",
    "        # print(f\"Tokens: {tokens}\")\n",
    "        hyp_words = np.array(lm_model.vocab.get_itos())[np.array(hyp)]\n",
    "        print(f\"Text: {' '.join(hyp_words)}, log proba: {hyp_log_prob:.4f}\")\n",
    "        # hyp_text = ' '.join(hyp_words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "        # display(Markdown(\n",
    "        #     f'<div class=\"alert alert-block alert-success\"> <b>{hyp_log_prob:.3f}: {hyp_text}</b></div>'\n",
    "        # ))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T21:23:11.910684Z",
     "start_time": "2024-04-15T21:23:11.785794400Z"
    }
   },
   "execution_count": 354,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>\n",
      "Text: liked ever movie movie <unk> seen movie made saw, log proba: -18.0324\n",
      "Text: seeing ever ever movie <unk> movie bad <unk> ever, log proba: -15.7798\n",
      "Text: got ever ever ever seen ever ever <unk> one, log proba: -16.8093\n",
      "<sos> <unk> favorite movie\n",
      "Text: life ever ive movie made seen movie <unk> seen, log proba: -18.2851\n",
      "Text: based ever movie film seen seen <unk> <unk> <unk>, log proba: -18.6186\n",
      "Text: classic ever <unk> seen <unk> like ever <unk> <unk>, log proba: -18.0889\n",
      "<sos> <unk> best movie\n",
      "Text: ever spoilers film seen ever <unk> bad seen movie, log proba: -17.8790\n",
      "Text: ever good movie seen saw made movie <unk> best, log proba: -18.3225\n",
      "Text: ever spoilers film seen ever <unk> bad seen movie, log proba: -18.2945\n",
      "<sos> <unk> worst movie\n",
      "Text: bad ever made ever seen worst seen <unk> seen, log proba: -18.1796\n",
      "Text: ever spoilers ever movie <unk> <unk> made seen movie, log proba: -17.1323\n",
      "Text: ever never movie seen made seen <unk> seen <unk>, log proba: -18.2516\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуйте выполнить декодинг для разных `beam_size`. Убедитесь, что при `beam_search=1` семплирование совпадает с top-1 (greedy decoding) подходом. \n",
    "\n",
    "Сравните результаты Beam Search с top-k семплированием и жадным декодированием. Опишите ваши наблюдения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:40:39.335632700Z",
     "start_time": "2024-04-15T20:40:39.265113400Z"
    }
   },
   "execution_count": 258,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Бонус. Существенное улучшение качества (до 6 баллов)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:04:51.678260Z",
     "start_time": "2021-04-02T15:04:51.673587Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Та модель, которая использовалась в предыдущей части во многом заимствует улучшения LSTM из статьи [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf). Вы можете попробовать применить другие варианты регуляризации из данной статьи для существенного улучшения качества LM.\n",
    "\n",
    "Например:\n",
    "1. Dropout для эмбеддингов **(+0.25)**\n",
    "2. Dropout входов и выходов RNN **(+0.25)**\n",
    "3. Регуляризация активаций (AR/TAR) **(+1.0)**\n",
    "4. NT-ASGD **(+1.5)**\n",
    "5. Tied веса эмбеддингов и софтмакса **(+1.0)**\n",
    "6. Attention **(+2.0)**\n",
    "\n",
    "**Полные баллы ставятся только при наличии качественного и количественного сравнения с бейзлайном.**\n",
    "\n",
    "**Для эксперимента с Attention необходимо изобразить Attention Maps для нескольких примеров.**"
   ],
   "metadata": {}
  }
 ]
}
