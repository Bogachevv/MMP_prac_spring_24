{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "804px",
    "left": "148px",
    "top": "50px",
    "width": "555.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "313px",
    "left": "926px",
    "right": "27px",
    "top": "120px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30683,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# `Практикум по программированию на языке Python`\n",
    "\n",
    "## `Задание 03. Рекуррентные Нейронные Сети. Dropout. LM`\n",
    "\n",
    "#### Фамилия, имя: Богачев Владимир\n",
    "\n",
    "Дата выдачи: <span style=\"color:red\">__30 марта 23:59__</span>.\n",
    "\n",
    "Мягкий дедлайн: <span style=\"color:red\">__13 апреля 23:59__</span>.\n",
    "\n",
    "Стоимость: __10 баллов__ (основная часть заданий) + __7 баллов__ (дополнительные задания).\n",
    "\n",
    "<span style=\"color:red\">__В ноутбуке все клетки должны выполняться без ошибок при последовательном их выполнении.__</span>\n",
    "\n",
    "#### `Москва, 2024`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Данное задание будет состоять из двух частей:\n",
    "1. Применение рекуррентной сети для решения задачи классификации текста. Более конкретно -- предсказания рейтинга отзыва фильма.\n",
    "2. Простейшая лингвистическая модель для генерации текста на основе LSTM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "При выполнении задания вы обучите LSTM с разным уровнем \"коробочности\", а также познакомитесь с различными способами применения DropOut к рекуррентным архитектурам. В рекуррентных архитектурах вариантов, куда можно наложить бинарную маску шума, гораздо больше, чем в нейросетях прямого прохода.\n",
    "\n",
    "Во второй части вы попробуете реализовать простейший рекуррентный декодер для генерации текстов.\n",
    "\n",
    "Задание сделано так, чтобы его можно было выполнять на CPU, однако RNN - это ресурсоёмкая вещь, поэтому на GPU с ними работать приятнее. Можете попробовать использовать [https://colab.research.google.com](https://colab.research.google.com) - бесплатное облако с GPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для корректного отображения картинок, вам может понадобится сделать ноутбук доверенным (Trusted) в правом верхнем углу**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 0. Загрузка и предобработка данных (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Рекомендуемые гиперпараметры`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "max_length = 200\n",
    "top_n_words = 5000\n",
    "\n",
    "hidden_dim = 128\n",
    "embedding_dim = 32\n",
    "\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:22:46.468794Z",
     "iopub.execute_input": "2024-04-11T17:22:46.469179Z",
     "iopub.status.idle": "2024-04-11T17:22:46.476062Z",
     "shell.execute_reply.started": "2024-04-11T17:22:46.469149Z",
     "shell.execute_reply": "2024-04-11T17:22:46.474329Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:06.616378600Z",
     "start_time": "2024-04-14T09:56:06.384115100Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первое, что нужно сделать — скачать, предобработать данные и организовать их таким образом, чтобы их можно было подавать в нейронную сеть.\n",
    "\n",
    "Для обеих частей задания мы будем использовать [**Large Movie Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Загрузка и предобработка данных`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузите данные по ссылке выше. (**tip**: используйте `wget`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:19.592935Z",
     "iopub.execute_input": "2024-04-11T17:25:19.593392Z",
     "iopub.status.idle": "2024-04-11T17:25:38.191124Z",
     "shell.execute_reply.started": "2024-04-11T17:25:19.593355Z",
     "shell.execute_reply": "2024-04-11T17:25:38.189556Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:06.619893200Z",
     "start_time": "2024-04-14T09:56:06.389115800Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Распакуйте скачанные данные в папку `aclImdb` (**tip:** используйте `tar`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!tar -xzf aclImdb_v1.tar.gz"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:38.194362Z",
     "iopub.execute_input": "2024-04-11T17:25:38.194831Z",
     "iopub.status.idle": "2024-04-11T17:25:49.331609Z",
     "shell.execute_reply.started": "2024-04-11T17:25:38.194788Z",
     "shell.execute_reply": "2024-04-11T17:25:49.330257Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:56.493223200Z",
     "start_time": "2024-04-14T09:56:06.408541100Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрите в файле `./aclImdb/README` как организованы данные:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!cat ./aclImdb/train/pos/10003_8.txt"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:49.333046Z",
     "iopub.execute_input": "2024-04-11T17:25:49.333428Z",
     "iopub.status.idle": "2024-04-11T17:25:50.441878Z",
     "shell.execute_reply.started": "2024-04-11T17:25:49.333392Z",
     "shell.execute_reply": "2024-04-11T17:25:50.440493Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:56.512540600Z",
     "start_time": "2024-04-14T09:56:56.494221400Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_data_path = './aclImdb/test/'\n",
    "train_data_path = './aclImdb/train/'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:50.446497Z",
     "iopub.execute_input": "2024-04-11T17:25:50.447293Z",
     "iopub.status.idle": "2024-04-11T17:25:50.452552Z",
     "shell.execute_reply.started": "2024-04-11T17:25:50.447247Z",
     "shell.execute_reply": "2024-04-11T17:25:50.451165Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:56.532715500Z",
     "start_time": "2024-04-14T09:56:56.512540600Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:50.454104Z",
     "iopub.execute_input": "2024-04-11T17:25:50.455185Z",
     "iopub.status.idle": "2024-04-11T17:25:57.205948Z",
     "shell.execute_reply.started": "2024-04-11T17:25:50.455150Z",
     "shell.execute_reply": "2024-04-11T17:25:57.204408Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:59.239236Z",
     "start_time": "2024-04-14T09:56:56.514713500Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vladimir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Стандартной предобработкой данных является токенизация текстов. Полученные токены можно будет закодировать и затем подавать на вход нейронной сети. Ключевым моментом, который влияет на скорость работы нейросети и её размер в памяти — размер словаря, используемого при токенизации. Для задачи классификации мы можем убрать часть слов (стоп слова, редкие слова), ускорив обучение без потери в качестве."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:57.207709Z",
     "iopub.execute_input": "2024-04-11T17:25:57.208275Z",
     "iopub.status.idle": "2024-04-11T17:25:57.223033Z",
     "shell.execute_reply.started": "2024-04-11T17:25:57.208243Z",
     "shell.execute_reply": "2024-04-11T17:25:57.221565Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:59.270818700Z",
     "start_time": "2024-04-14T09:56:59.239236Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:57.225050Z",
     "iopub.execute_input": "2024-04-11T17:25:57.225517Z",
     "iopub.status.idle": "2024-04-11T17:25:58.312143Z",
     "shell.execute_reply.started": "2024-04-11T17:25:57.225470Z",
     "shell.execute_reply": "2024-04-11T17:25:58.311031Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:59.322109500Z",
     "start_time": "2024-04-14T09:56:59.266825600Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуйте функцию для токенизации текста. Выполнять токенизацию можно по-разному, но в данном задании предлагается это делать следующим образом:\n",
    "1. Привести текст к нижнему регистру\n",
    "2. Убрать html разметку из текстов (`<br />`, ...)\n",
    "3. Убрать все символы кроме латинских букв\n",
    "4. Разбить строку по пробелам\n",
    "5. Убрать стоп слова"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.313717Z",
     "iopub.execute_input": "2024-04-11T17:25:58.314345Z",
     "iopub.status.idle": "2024-04-11T17:25:58.696667Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.314303Z",
     "shell.execute_reply": "2024-04-11T17:25:58.695352Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:59.346335400Z",
     "start_time": "2024-04-14T09:56:59.270818700Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param str text: Input text \n",
    "    :return List[str]: List of words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    text = text.translate(translator)\n",
    "    text = word_tokenize(text, language='english')\n",
    "    text = list(filter(lambda w: w not in STOPWORDS, text))\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.698220Z",
     "iopub.execute_input": "2024-04-11T17:25:58.698862Z",
     "iopub.status.idle": "2024-04-11T17:25:58.708538Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.698824Z",
     "shell.execute_reply": "2024-04-11T17:25:58.706709Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:59.363382800Z",
     "start_time": "2024-04-14T09:56:59.314164600Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenize('1. Hello <br /> words!! <br />')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.714426Z",
     "iopub.execute_input": "2024-04-11T17:25:58.715170Z",
     "iopub.status.idle": "2024-04-11T17:25:58.748140Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.715128Z",
     "shell.execute_reply": "2024-04-11T17:25:58.746737Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:59.377382Z",
     "start_time": "2024-04-14T09:56:59.319111800Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['hello', 'words']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь мы можем создать словарь, с помощью которого мы будем численно кодировать токены из текста и наоборот.\n",
    "\n",
    "Удобной обёрткой для создания словарей является класс `torchtext.vocab.Vocab` и фабрика для создания таких классов `torchtext.vocab.vocab`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torchtext.vocab.vocab??"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.749880Z",
     "iopub.execute_input": "2024-04-11T17:25:58.750793Z",
     "iopub.status.idle": "2024-04-11T17:25:58.849637Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.750745Z",
     "shell.execute_reply": "2024-04-11T17:25:58.847692Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:56:59.473381300Z",
     "start_time": "2024-04-14T09:56:59.327337300Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы создать такой словарь, сначала нужно создать словарь со всеми токенами в тексте и их частотами встречаемости:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:51:55.300753Z",
     "start_time": "2021-04-01T19:51:55.275188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "counter = defaultdict(int)\n",
    "\n",
    "for path in ['./aclImdb/test/neg', './aclImdb/test/pos', './aclImdb/train/neg', './aclImdb/train/pos']:\n",
    "    for file_path in os.listdir(path):\n",
    "        text = open(os.path.join(path, file_path), 'r', encoding='utf-8', errors='ignore').read().strip()\n",
    "        for token in tokenize(text):\n",
    "            counter[token] += 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.852596Z",
     "iopub.execute_input": "2024-04-11T17:25:58.853845Z",
     "iopub.status.idle": "2024-04-11T17:28:46.997932Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.853790Z",
     "shell.execute_reply": "2024-04-11T17:28:46.996330Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:42.946710500Z",
     "start_time": "2024-04-14T09:56:59.358382400Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir\\AppData\\Local\\Temp\\ipykernel_14412\\3422549362.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text).get_text()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для работы с текстами нам необходимо зарезервировать два специальных токена:\n",
    "1. `<pad>` для токена означающего паддинг\n",
    "2. `<unk>` для токенов, которые отсутствуют в словаре"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# specials = ['<pad>', '<unk>']\n",
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for special in specials:\n",
    "    counter[special] = 0"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:46.999768Z",
     "iopub.execute_input": "2024-04-11T17:28:47.000149Z",
     "iopub.status.idle": "2024-04-11T17:28:47.005985Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.000119Z",
     "shell.execute_reply": "2024-04-11T17:28:47.004988Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:42.956297200Z",
     "start_time": "2024-04-14T09:57:42.949661100Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте словарь из словаря частот `counter`. Наименьшие *id* отдайте под специальные токены. \n",
    "\n",
    "Отбросьте низкочастотные слова, оставив только `top_n_words` слов. Можете использовать любой способ реализации этого условия, например:\n",
    "1. Оставить в словаре `counter` нужное число слов\n",
    "2. Подобрать параметр `min_freq`, чтобы оставшееся число слов было близко к необходимому порогу"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torchtext.__version__"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.007190Z",
     "iopub.execute_input": "2024-04-11T17:28:47.008227Z",
     "iopub.status.idle": "2024-04-11T17:28:47.028116Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.008192Z",
     "shell.execute_reply": "2024-04-11T17:28:47.026746Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:42.968295800Z",
     "start_time": "2024-04-14T09:57:42.951176900Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.16.0+cpu'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab = torchtext.vocab.vocab(\n",
    "    counter,\n",
    "    min_freq=145,\n",
    "    specials=specials,\n",
    ")\n",
    "\n",
    "# vocab.append_token('<pad>')\n",
    "# vocab.append_token('<unk>')\n",
    "# vocab.append_token('<sos>')\n",
    "# vocab.append_token('<eos>')\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.029902Z",
     "iopub.execute_input": "2024-04-11T17:28:47.030270Z",
     "iopub.status.idle": "2024-04-11T17:28:47.199189Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.030230Z",
     "shell.execute_reply": "2024-04-11T17:28:47.197799Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.045969100Z",
     "start_time": "2024-04-14T09:57:42.955297Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab, len(vocab)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.200582Z",
     "iopub.execute_input": "2024-04-11T17:28:47.201012Z",
     "iopub.status.idle": "2024-04-11T17:28:47.208352Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.200984Z",
     "shell.execute_reply": "2024-04-11T17:28:47.206947Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.048972500Z",
     "start_time": "2024-04-14T09:57:42.988640400Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(Vocab(), 5037)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab.lookup_indices(['<pad>', '<unk>'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.210021Z",
     "iopub.execute_input": "2024-04-11T17:28:47.210648Z",
     "iopub.status.idle": "2024-04-11T17:28:47.223755Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.210601Z",
     "shell.execute_reply": "2024-04-11T17:28:47.222380Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.063013700Z",
     "start_time": "2024-04-14T09:57:42.993156600Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab.lookup_indices(['this', 'film', 'was', 'awful'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.225104Z",
     "iopub.execute_input": "2024-04-11T17:28:47.225510Z",
     "iopub.status.idle": "2024-04-11T17:28:47.238847Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.225476Z",
     "shell.execute_reply": "2024-04-11T17:28:47.237637Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.066012Z",
     "start_time": "2024-04-14T09:57:42.996786Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 98, 1, 422]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь мы готовы создать обёртку-датасет для наших данных. \n",
    "\n",
    "Необходимо добавить несколько опции, которые понадобятся во второй части задания:\n",
    "1. Ограничение на максимальную длину текста в токенах. Если текст оказывается длиннее, то последние токены отбрасываются\n",
    "2. Возможность добавить в специальные токены `<sos>`, `<eos>` в начало и конец токенизированного текста\n",
    "    \n",
    "**tips:**\n",
    "1. Обратите особое внимание, что у длинных текстов не должен обрезаться паддинг\n",
    "2. В исходных данных рейтинг закодирован в названии файла в виде числа от $1$ до $10$. Для удобства, вычтите $1$, чтобы рейтинг был от $0$ до $9$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import re"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.241391Z",
     "iopub.execute_input": "2024-04-11T17:28:47.242132Z",
     "iopub.status.idle": "2024-04-11T17:28:47.253250Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.242077Z",
     "shell.execute_reply": "2024-04-11T17:28:47.251777Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.073012600Z",
     "start_time": "2024-04-14T09:57:43.000349500Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "re_rating = re.compile('_[0-9]*')\n",
    "re.search(re_rating, './aclImdb/train/pos/10003_10.txt').group(0)[1:]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.255370Z",
     "iopub.execute_input": "2024-04-11T17:28:47.256186Z",
     "iopub.status.idle": "2024-04-11T17:28:47.271859Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.256139Z",
     "shell.execute_reply": "2024-04-11T17:28:47.270616Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.077012800Z",
     "start_time": "2024-04-14T09:57:43.005352100Z"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'10'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_raiting(path: str, re_rating = None):\n",
    "    re_rating = '_[0-9]*' if re_rating is None else re_rating\n",
    "    \n",
    "    rating = re.search(re_rating, path).group(0)[1:]\n",
    "    return int(rating) - 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.273661Z",
     "iopub.execute_input": "2024-04-11T17:28:47.274645Z",
     "iopub.status.idle": "2024-04-11T17:28:47.284077Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.274598Z",
     "shell.execute_reply": "2024-04-11T17:28:47.282140Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.080011800Z",
     "start_time": "2024-04-14T09:57:43.008288500Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class LargeMovieReviewDataset(Dataset):\n",
    "    def __init__(self, data_path, vocab, max_len, pad_sos=False, pad_eos=False):\n",
    "        \"\"\"\n",
    "        :param str data_path: Path to folder with one of the data splits (train or test)\n",
    "        :param torchtext.vocab.Vocab vocab: dictionary with lookup_indices method\n",
    "        :param int max_len: Maximum length of tokenized text\n",
    "        :param bool pad_sos: If True pad sequence at the beginning with <sos> \n",
    "        :param bool pad_eos: If True pad sequence at the end with <eos>         \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pad_sos = pad_sos\n",
    "        if self.pad_sos:\n",
    "            self.sos_id = vocab.lookup_indices(['<sos>'])[0]\n",
    "        self.pad_eos = pad_eos\n",
    "        if self.pad_eos:\n",
    "            self.eos_id = vocab.lookup_indices(['<eos>'])[0]\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.data_path = data_path\n",
    "        self.negative_path = os.path.join(data_path, 'neg')\n",
    "        self.positive_path = os.path.join(data_path, 'pos')\n",
    "        \n",
    "        self.negative_paths = []\n",
    "        self.positive_paths = []\n",
    "\n",
    "        for file_path in os.listdir(self.negative_path):\n",
    "            self.negative_paths.append(os.path.join(self.negative_path, file_path))\n",
    "\n",
    "        for file_path in os.listdir(self.positive_path):\n",
    "            self.positive_paths.append(os.path.join(self.positive_path, file_path))\n",
    "        \n",
    "        self.texts = []\n",
    "        self.tokens = []\n",
    "        self.ratings = []\n",
    "        self.labels = [0] * len(self.negative_paths) + [1] * len(self.positive_paths)\n",
    "        \n",
    "        # Read each file in data_path, tokenize it, get tokens ids, its rating and store\n",
    "        re_rating = re.compile('_[0-9]*')\n",
    "        for path in self.negative_paths + self.positive_paths:\n",
    "            # YOUR CODE HERE\n",
    "            with open(path, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read().strip()\n",
    "            self.texts.append(text)\n",
    "            \n",
    "            txt_tokens = vocab.lookup_indices(tokenize(text))[0:self.max_len]\n",
    "            if self.pad_sos:\n",
    "                txt_tokens.insert(0, vocab['<sos>'])\n",
    "            if self.pad_eos:\n",
    "                txt_tokens.append(vocab['<eos>'])\n",
    "            \n",
    "            self.tokens.append(txt_tokens)\n",
    "            self.ratings.append(get_raiting(path, re_rating=re_rating))\n",
    "        \n",
    "        self.ratings = torch.LongTensor(self.ratings)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        self.tokens = [torch.LongTensor(tls) for tls in self.tokens]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :param int idx: index of object in dataset\n",
    "        :return dict: Dictionary with all useful object data \n",
    "            {\n",
    "                'text' str: unprocessed text,\n",
    "                'label' torch.Tensor(dtype=torch.long): sentiment of the text (0 for negative, 1 for positive)\n",
    "                'rating' torch.Tensor(dtype=torch.long): rating of the text\n",
    "                'tokens' torch.Tensor(dtype=torch.long): tensor of tokens ids for the text\n",
    "                'tokens_len' torch.Tensor(dtype=torch.long): number of tokens\n",
    "            }\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        res = {\n",
    "            'text': self.texts[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'rating': self.ratings[idx],\n",
    "            'tokens': self.tokens[idx],\n",
    "            'tokens_len': self.tokens[idx].shape[0]\n",
    "        }\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return int: number of objects in dataset \n",
    "        \"\"\"\n",
    "        return len(self.tokens)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.285804Z",
     "iopub.execute_input": "2024-04-11T17:28:47.286395Z",
     "iopub.status.idle": "2024-04-11T17:28:47.306007Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.286361Z",
     "shell.execute_reply": "2024-04-11T17:28:47.305049Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:57:43.083015200Z",
     "start_time": "2024-04-14T09:57:43.014599500Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте датасеты для тестовой и обучающей выборки. \n",
    "\n",
    "Обратите внимание, что для задачи классификации нам не потребуется паддинг с помощью `<sos>`, `<eos>`. \n",
    "\n",
    "Не забудьте обрезать длинные тексты, передав параметр `max_length`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset = LargeMovieReviewDataset(test_data_path, vocab, max_len=max_length)\n",
    "train_dataset = LargeMovieReviewDataset(train_data_path, vocab, max_len=max_length)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.307497Z",
     "iopub.execute_input": "2024-04-11T17:28:47.308587Z",
     "iopub.status.idle": "2024-04-11T17:31:33.855929Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.308543Z",
     "shell.execute_reply": "2024-04-11T17:31:33.854622Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:20.415323800Z",
     "start_time": "2024-04-14T09:57:43.019970100Z"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir\\AppData\\Local\\Temp\\ipykernel_14412\\3422549362.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text).get_text()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим, как выглядит объект в датасете:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "for d in train_dataset:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\"\n",
    "    \n",
    "for d in test_dataset:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\""
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:33.858064Z",
     "iopub.execute_input": "2024-04-11T17:31:33.858477Z",
     "iopub.status.idle": "2024-04-11T17:31:34.788422Z",
     "shell.execute_reply.started": "2024-04-11T17:31:33.858426Z",
     "shell.execute_reply": "2024-04-11T17:31:34.787016Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:20.895883200Z",
     "start_time": "2024-04-14T09:58:20.415323800Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset[-2]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.808174Z",
     "iopub.execute_input": "2024-04-11T17:31:34.808645Z",
     "iopub.status.idle": "2024-04-11T17:31:34.828293Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.808610Z",
     "shell.execute_reply": "2024-04-11T17:31:34.826392Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:20.905612300Z",
     "start_time": "2024-04-14T09:58:20.896882500Z"
    }
   },
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"This movie, with all its complexity and subtlety, makes for one of the most thought-provoking short films I have ever seen. The topics it addresses are ugly, cynical, and at times, even macabre, but the film remains beautiful in its language, artful with its camera angles, and gorgeous in its style, skillfully recreating the short story of the same name written by a master of short stories, Tobias Wolff.<br /><br />Not wishing to spoil anything of the movie, I won't go into any details, other than to say that this movie is magnificent in and of itself. It takes pride in what it does, and does it well. It shows the most important memories of life, all of which can be topped by the single most elusive feeling: unexpected bliss. This movie, of its own volition, has created in me the same feelings the main character (Tom Noonan) felt when words transformed his very existence, and that is one impressive feat.\",\n 'label': tensor(1),\n 'rating': tensor(9),\n 'tokens': tensor([   6, 3680, 3314,   97,   79, 4861,  603,   61,  419,  111,    1,    1,\n          188, 1967,   74,  198,    1,   98,  760, 1733, 2546,    1,  128, 1951,\n         2747, 1365,    1,    1,  603,  301, 1457, 1533, 2836,  603,  819,    1,\n            1,  361, 1227,  665,    6,  252,   80, 2067,  542,    6, 4399, 1115,\n         3588,   42,   37, 1880, 3975, 1012,    1,  153,    1,  258, 4448,    1,\n            6,    1, 2124,  975,  232,   20,  950,    1,  578, 2419,    1, 2329,\n           79, 2513,    1]),\n 'tokens_len': 75}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь нам нужно создать `DataLoader` для наших данных. `DataLoader` умеет из коробки объединять список объектов из датасета в один батч, даже когда датасет возвращает словарь тензоров. Однако, это работает только в случае когда все эти тензоры имеют один и тот же размер во всех батчах. В нашем случае, это не так, так как разные тексты могут иметь разную длину.\n",
    "\n",
    "Чтобы обойти эту проблему у `DataLoader` есть параметр `collate_fn`, который позволяет задать функцию для объединения списка объектов в один батч."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы объединить несколько тензоров разной длины в один можно использовать функцию `torch.nn.utils.rnn.pad_sequence`\n",
    "\n",
    "Обратите внимание на её аргументы:\n",
    "1. `batch_first` определяет по какой оси \"складывать\" тензоры. Предпочтительнее использовать `batch_first=False` так как это может упростить выполнение задания в дальнейшем \n",
    "2. `padding_value` — число, которое будет использоваться в качестве паддинга, чтобы сделать все тензоры одинаковой длины"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.nn.utils.rnn.pad_sequence([\n",
    "    torch.tensor([1, 2, 3]),\n",
    "    torch.tensor([4, 5]),\n",
    "    torch.tensor([6, 7, 8, 9])\n",
    "], batch_first=False, padding_value=-1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.829776Z",
     "iopub.execute_input": "2024-04-11T17:31:34.830242Z",
     "iopub.status.idle": "2024-04-11T17:31:34.871668Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.830197Z",
     "shell.execute_reply": "2024-04-11T17:31:34.869604Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:20.908612900Z",
     "start_time": "2024-04-14T09:58:20.901387400Z"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  4,  6],\n        [ 2,  5,  7],\n        [ 3, -1,  8],\n        [-1, -1,  9]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch, padding_value, batch_first=False):\n",
    "    \"\"\"\n",
    "    :param List[Dict] batch: List of objects from dataset\n",
    "    :param int padding_value: Value that will be used to pad tokens\n",
    "    :param bool batch_first: If True resulting tensor with tokens must have shape [B, T] otherwise [T, B]\n",
    "    :return dict: Dictionary with all data collated\n",
    "        {\n",
    "            'ratings' torch.Tensor(dtype=torch.long): rating of the text for each object in batch\n",
    "            'labels' torch.Tensor(dtype=torch.long): sentiment of the text for each object in batch\n",
    "            \n",
    "            'texts' List[str]: All texts in one list\n",
    "            'tokens' torch.Tensor(dtype=torch.long): tensor of tokens ids padded with @padding_value\n",
    "            'tokens_lens' torch.Tensor(dtype=torch.long): number of tokens for each object in batch\n",
    "        }\n",
    "    \"\"\"\n",
    "    ratings = torch.LongTensor(size=(len(batch), ))\n",
    "    labels = torch.LongTensor(size=(len(batch), ))\n",
    "    texts = []\n",
    "    tokens = []\n",
    "    tokens_lens = torch.LongTensor(size=(len(batch), ))\n",
    "    \n",
    "    for i, batch_dir in enumerate(batch):\n",
    "        ratings[i] = batch_dir['rating']\n",
    "        labels[i] = batch_dir['label']\n",
    "        texts.append(batch_dir['text'])\n",
    "        tokens.append(batch_dir['tokens'])\n",
    "        tokens_lens[i] = batch_dir['tokens_len']\n",
    "    \n",
    "    tokens = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=batch_first, padding_value=padding_value)\n",
    "    \n",
    "    res = {\n",
    "            'texts': texts,\n",
    "            'labels': labels,\n",
    "            'ratings': ratings,\n",
    "            'tokens': tokens,\n",
    "            'tokens_lens': tokens_lens,\n",
    "        }\n",
    "    \n",
    "    return res"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.873549Z",
     "iopub.execute_input": "2024-04-11T17:31:34.873934Z",
     "iopub.status.idle": "2024-04-11T17:31:34.895676Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.873901Z",
     "shell.execute_reply": "2024-04-11T17:31:34.891939Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:20.934173800Z",
     "start_time": "2024-04-14T09:58:20.904613100Z"
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте даталоадеры с использованием `collate_fn`.\n",
    "\n",
    "**tips**:\n",
    "1. Передать в `collate_fn` правильное значение паддинга можно, например, с помощью `functools.partial`\n",
    "2. Если вы работаете в Google Colab, то, возможно, вам будет необходимо установить `num_workers=0` во избежание падения ноутбука."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import functools"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.905939Z",
     "iopub.execute_input": "2024-04-11T17:31:34.907603Z",
     "iopub.status.idle": "2024-04-11T17:31:34.913490Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.907545Z",
     "shell.execute_reply": "2024-04-11T17:31:34.911716Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:20.937474600Z",
     "start_time": "2024-04-14T09:58:20.909613800Z"
    }
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "collate_fn_ = functools.partial(collate_fn, padding_value=vocab['<pad>'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0, collate_fn=collate_fn_)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.917306Z",
     "iopub.execute_input": "2024-04-11T17:31:34.918343Z",
     "iopub.status.idle": "2024-04-11T17:31:34.932951Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.918275Z",
     "shell.execute_reply": "2024-04-11T17:31:34.931536Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:20.944172500Z",
     "start_time": "2024-04-14T09:58:20.913603900Z"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for d in train_dataloader:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\"\n",
    "\n",
    "for d in test_dataloader:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\""
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.935437Z",
     "iopub.execute_input": "2024-04-11T17:31:34.935969Z",
     "iopub.status.idle": "2024-04-11T17:31:36.315937Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.935933Z",
     "shell.execute_reply": "2024-04-11T17:31:36.314514Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:21.746420500Z",
     "start_time": "2024-04-14T09:58:20.917145300Z"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим на какой-нибудь батч:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "batch.keys(), batch['labels'], batch['ratings'], batch['tokens'], batch['tokens_lens']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.317864Z",
     "iopub.execute_input": "2024-04-11T17:31:36.318299Z",
     "iopub.status.idle": "2024-04-11T17:31:36.345228Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.318263Z",
     "shell.execute_reply": "2024-04-11T17:31:36.343273Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:21.756144Z",
     "start_time": "2024-04-14T09:58:21.746420500Z"
    }
   },
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(dict_keys(['texts', 'labels', 'ratings', 'tokens', 'tokens_lens']),\n tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n tensor([1, 3, 0, 2, 2, 1, 1, 1, 3, 3, 2, 2, 1, 0, 0, 3, 1, 3, 2, 0, 0, 0, 3, 0,\n         2, 3, 3, 2, 1, 2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 3, 0, 3, 1, 0, 0,\n         0, 1, 0, 3, 0, 0, 0, 3, 3, 1, 3, 3, 2, 0, 0, 0]),\n tensor([[   4,   58,  136,  ..., 1984,  514,    1],\n         [   1,   59,  137,  ...,  284,    1,  274],\n         [   5,   60,  138,  ...,  349,    1, 2002],\n         ...,\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0]]),\n tensor([ 74, 128, 108, 168, 137,  52,  74,  74,  72,  98,  59, 143, 134,  52,\n         104, 112,  67, 116, 189,  47,  36,  96, 200, 200, 136, 111, 105, 200,\n         200, 144,  75,  82, 184,  99, 156, 132, 131,  56, 182, 106,  67,  61,\n          86, 200, 113,  66, 200,  55, 115,  77,  56, 145, 130,  29,  64,  60,\n         200,  16, 151,  87,  64,  71,  83,  87]))"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 1. Классификация текстов (4 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Сборка и обучение RNN в pytorch (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим переменные для device-agnostic кода:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dtype, device, cuda_device_id = torch.float32, None, 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{0}'.format(str(cuda_device_id) if cuda_device_id is not None else '')\n",
    "if cuda_device_id is not None and torch.cuda.is_available():\n",
    "    device = 'cuda:{0:d}'.format(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}, dtype: {dtype}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.346749Z",
     "iopub.execute_input": "2024-04-11T17:31:36.347147Z",
     "iopub.status.idle": "2024-04-11T17:31:36.356022Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.347114Z",
     "shell.execute_reply": "2024-04-11T17:31:36.354771Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:21.775458600Z",
     "start_time": "2024-04-14T09:58:21.754429500Z"
    }
   },
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0, dtype: torch.float32\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Наша нейросеть будет обрабатывать входную последовательность по словам (word level). Мы будем использовать простую и стандартную рекуррентную архитектуру для классификации:\n",
    "1. Слой представлений, превращающий id токена в вектор-эмбеддинг этого слова\n",
    "2. Слой LSTM\n",
    "3. Полносвязный слой, предсказывающий выход по последнему скрытому состоянию\n",
    "\n",
    "Ниже дан код для сборки и обучения нашей нейросети."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите класс-обёртку над LSTM для задачи классификации. \n",
    "**Не используйте циклы.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:59:16.467178Z",
     "start_time": "2021-04-01T20:59:16.441112Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.357634Z",
     "iopub.execute_input": "2024-04-11T17:31:36.358609Z",
     "iopub.status.idle": "2024-04-11T17:31:36.371926Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.358572Z",
     "shell.execute_reply": "2024-04-11T17:31:36.370373Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:21.781458300Z",
     "start_time": "2024-04-14T09:58:21.758143200Z"
    }
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "emb_dump = None\n",
    "inp_dump = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:21.784457500Z",
     "start_time": "2024-04-14T09:58:21.760233Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim, hidden_dim, output_size, vocab,\n",
    "        rec_layer=torch.nn.LSTM, dropout=None, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Create a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        #    Use torch.nn.Embedding. Do not forget specify padding_idx!\n",
    "        # YOUR CODE HERE\n",
    "        self.word_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=len(vocab),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab['<pad>'],\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        if dropout is not None:\n",
    "            self.rnn = rec_layer(\n",
    "                input_size=embedding_dim, \n",
    "                hidden_size=hidden_dim, \n",
    "                device=device,\n",
    "                dropout=dropout,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = rec_layer(\n",
    "                input_size=embedding_dim, \n",
    "                hidden_size=hidden_dim, \n",
    "                device=device,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        # Create linear layer for classification\n",
    "        # YOUR CODE HERE\n",
    "        self.output = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_size),\n",
    "        )\n",
    "        # self.output = nn.Sequential(\n",
    "        #     nn.Linear(hidden_dim, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm1d(64),\n",
    "        #     nn.Linear(64, output_size),\n",
    "        # )\n",
    "    \n",
    "    def forward(self, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor(dtype=torch.long) tokens: Batch of texts represented with tokens.\n",
    "        :param torch.Tensor(dtype=torch.long) tokens_lens: Number of non-padding tokens for each object in batch.\n",
    "        :return torch.Tensor(dtype=torch.long): Vector representation for each sequence in batch\n",
    "        \"\"\"\n",
    "        # Evaluate embeddings\n",
    "        # DEBUG: store last input in globals\n",
    "        global inp_dump\n",
    "        global emb_dump\n",
    "        \n",
    "        # DEBUG: store last input in globals\n",
    "        inp_dump = tokens.detach()\n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Tokens has NaNs\"\n",
    "        \n",
    "        x = self.word_embeddings(tokens)\n",
    "        # DEBUG: store last input in globals\n",
    "        emb_dump = x.detach()\n",
    "        \n",
    "        assert not torch.any(torch.isnan(x)), f\"Embeddings has NaNs\"\n",
    "        \n",
    "        # Make forward pass through recurrent network\n",
    "        # YOUR CODE HERE\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Pass output from rnn to linear layer \n",
    "        # Note: each object in batch has its own length \n",
    "        #     so we must take rnn hidden state after the last token for each text in batch        \n",
    "        x = x[tokens_lens - 1, torch.arange(0, x.shape[1]), :]\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.373584Z",
     "iopub.execute_input": "2024-04-11T17:31:36.374996Z",
     "iopub.status.idle": "2024-04-11T17:31:36.393418Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.374920Z",
     "shell.execute_reply": "2024-04-11T17:31:36.392097Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:21.870457600Z",
     "start_time": "2024-04-14T09:58:21.768242900Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Исходный код LSTM](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите функции для обучения и оценки модели:\n",
    "\n",
    "**tip:**\n",
    "1. В функции `evaluate` при подсчёте метрик учитывайте, что батчи могут иметь разный размер. (в частности последний батч)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.394775Z",
     "iopub.execute_input": "2024-04-11T17:31:36.395929Z",
     "iopub.status.idle": "2024-04-11T17:31:37.839947Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.395887Z",
     "shell.execute_reply": "2024-04-11T17:31:37.837984Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.298695200Z",
     "start_time": "2024-04-14T09:58:21.771457700Z"
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def set_global_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "#     torch.use_deterministic_algorithms(True) # если нужно гарантировать 1000% воспроизводимость\n",
    "\n",
    "    # Для Dataloader\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Для каждого woerker в Daaloader\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:37.842763Z",
     "iopub.execute_input": "2024-04-11T17:31:37.843189Z",
     "iopub.status.idle": "2024-04-11T17:31:37.852490Z",
     "shell.execute_reply.started": "2024-04-11T17:31:37.843154Z",
     "shell.execute_reply": "2024-04-11T17:31:37.850826Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.309672900Z",
     "start_time": "2024-04-14T09:58:22.300694600Z"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Make optimizer step\n",
    "        \n",
    "        # labels = data['labels'].to(device)\n",
    "        ratings = data['ratings'].to(device)\n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Eval epoch {idx} has NaNs\"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    for idx, data in enumerate(dataloader):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Evaluate accuracy\n",
    "        \n",
    "        # labels = data['labels'].to(device)\n",
    "        ratings = data['ratings'].to(device)\n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Eval epoch {idx} has NaNs\"\n",
    "        \n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, ratings)\n",
    "        total_loss += loss\n",
    "        \n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        total_accuracy += torch.sum(pred == ratings).item()\n",
    "        \n",
    "    return total_loss / len(dataloader.dataset), total_accuracy / len(dataloader.dataset)\n",
    "    \n",
    "\n",
    "def train(\n",
    "    train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs, name,\n",
    "):\n",
    "    wandb.init(project=\"MMP_prac_rnn_deb\", name=name)\n",
    "    \n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    rng = tqdm(range(num_epochs))\n",
    "    \n",
    "    for epoch in rng:\n",
    "        train_epoch(train_loader, model, loss_fn, optimizer, device)\n",
    "        \n",
    "        train_loss, train_acc = evaluate(train_loader, model, loss_fn, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss, test_acc = evaluate(test_loader, model, loss_fn, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        wandb.log({\"eval/loss\": test_loss, \"eval/accuracy\": test_acc}, step=epoch)\n",
    "        wandb.log({\"train/loss\": train_loss, \"train/accuracy\": train_acc}, step=epoch)\n",
    "        \n",
    "        print(\n",
    "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. Accuracy (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
    "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.177562Z",
     "iopub.execute_input": "2024-04-11T17:34:50.178008Z",
     "iopub.status.idle": "2024-04-11T17:34:50.203391Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.177974Z",
     "shell.execute_reply": "2024-04-11T17:34:50.201917Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.323816400Z",
     "start_time": "2024-04-14T09:58:22.307506200Z"
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.486380Z",
     "iopub.execute_input": "2024-04-11T17:34:50.487487Z",
     "iopub.status.idle": "2024-04-11T17:34:50.498593Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.487434Z",
     "shell.execute_reply": "2024-04-11T17:34:50.497242Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:46:21.014666300Z",
     "start_time": "2024-04-14T08:46:20.985111Z"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=torch.nn.LSTM, dropout=None\n",
    ").to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.646098Z",
     "iopub.execute_input": "2024-04-11T17:34:50.647421Z",
     "iopub.status.idle": "2024-04-11T17:34:50.661479Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.647372Z",
     "shell.execute_reply": "2024-04-11T17:34:50.659595Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:46:24.923655200Z",
     "start_time": "2024-04-14T08:46:21.001667500Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим класс для подсчёта функции потерь и оптимизатор:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:52.169426Z",
     "iopub.execute_input": "2024-04-11T17:34:52.169838Z",
     "iopub.status.idle": "2024-04-11T17:34:52.177144Z",
     "shell.execute_reply.started": "2024-04-11T17:34:52.169807Z",
     "shell.execute_reply": "2024-04-11T17:34:52.175838Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:46:25.213590600Z",
     "start_time": "2024-04-14T08:46:24.923655200Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем обучить модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"Basic RNN ratings\",\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:57.048782Z",
     "iopub.execute_input": "2024-04-11T17:34:57.049265Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:47:55.873096900Z",
     "start_time": "2024-04-14T08:46:25.213590600Z"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbogachevv\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240414_114627-b5aij9ql</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">Basic RNN ratings</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd97d54cef9d46d79c1c3d93df5f018b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b9a35d153da41718eed73459b47ac6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.272/0.262\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc2d260de0dc4da3be41b341d1e44160"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.027/0.028. Accuracy (Train/Test): 0.358/0.332\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60e6ae419ca344d0bfcaa8aada6bb63e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.385/0.324\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9b7c1ffe2e346cca0bf2226604d2360"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.025/0.028. Accuracy (Train/Test): 0.392/0.335\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "465f4edcd9204f03bb3064a561ac95e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.023/0.027. Accuracy (Train/Test): 0.435/0.360\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec3c0addbc5546ef93534a869f7e2b3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.022/0.027. Accuracy (Train/Test): 0.467/0.366\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7283d55f8524e30ad62133f486a397f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.516/0.374\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5dd6bfd2ed04b6a912e18be6572dc12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.019/0.026. Accuracy (Train/Test): 0.552/0.353\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87f5f774cf4e44f4b817b4ff672949e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.017/0.027. Accuracy (Train/Test): 0.585/0.355\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2561f1bcdd24009bf88a5cd7b3508da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.016/0.028. Accuracy (Train/Test): 0.613/0.337\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e63eafcaab34dedaf7e03781951f526"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.015/0.029. Accuracy (Train/Test): 0.656/0.348\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c0204bb7d1e44ed918b60484bd4e71f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.013/0.031. Accuracy (Train/Test): 0.679/0.361\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d0310a8c9aa43a3977e317b02b01f5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.012/0.032. Accuracy (Train/Test): 0.746/0.322\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30e5ecbd9500487bbb97079864a6e101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.010/0.035. Accuracy (Train/Test): 0.776/0.343\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ec5ffd18b1e416597c22ac24a78d4c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.009/0.037. Accuracy (Train/Test): 0.817/0.317\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33ef01845d1b42d89b9e0fa03ca3d7ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▅▆▇▇█▇▇▆▆▇▅▆▄</td></tr><tr><td>eval/loss</td><td>▄▂▂▂▁▁▁▁▂▂▃▄▅▆█</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▃▃▄▅▅▅▆▆▇▇█</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▅▄▄▄▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.31728</td></tr><tr><td>eval/loss</td><td>0.03729</td></tr><tr><td>train/accuracy</td><td>0.81728</td></tr><tr><td>train/loss</td><td>0.00877</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">Basic RNN ratings</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240414_114627-b5aij9ql\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нерегуляризованные LSTM часто быстро переобучаются (и мы это видим по точности на контроле). Чтобы с этим бороться, часто используют *L2-регуляризацию* и *дропаут*.\n",
    "Однако способов накладывать дропаут на рекуррентный слой достаточно много, и далеко не все хорошо работают. По [ссылке](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b) доступен хороший обзор дропаутов для RNN.\n",
    "\n",
    "Мы реализуем два варианта DropOut для RNN (и третий дополнительно). Заодно увидим, что для реализации различных усовершенствований рекуррентной архитектуры приходится \"вскрывать\" слой до различной \"глубины\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Гала и Гарамани. Variational Dropout (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Начнем с дропаута, описанного в [статье Гала и Гарамани](https://arxiv.org/abs/1512.05287).\n",
    "Для этого нам потребуется перейти от использования слоя `torch.nn.LSTM`, полностью скрывающего от нас рекуррентную логику, к использованию слоя `torch.nn.LSTMCell`, обрабатывающего лишь один временной шаг нашей последовательности (а всю логику вокруг придется реализовать самостоятельно). \n",
    "\n",
    "Допишите класс `RNNLayer`. При `dropout=0` ваш класс должен работать как обычный слой LSTM, а при `dropout > 0` накладывать бинарную маску на входной и скрытый вектор на каждом временном шаге, причем эта маска должна быть одинаковой во все моменты времени.\n",
    "\n",
    "Дропаут Гала и Гарамани в виде формул (m обозначает маску дропаута):\n",
    "\n",
    "$$\n",
    "h_{t-1} = h_{t-1}*m_h, \\, x_t = x_t * m_x\n",
    "$$\n",
    "\n",
    "Далее обычный шаг рекуррентной архитектуры, например, LSTM:\n",
    "\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Union, Optional, Tuple, List"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:38:38.240225600Z",
     "start_time": "2024-04-12T11:38:38.229479300Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def init_h0_c0(num_objects: int, hidden_size: int, some_existing_tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    return h0 and c0, use some_existing_tensor.new_zeros() to gen them\n",
    "    h0 shape: num_objects x hidden_size\n",
    "    c0 shape: num_objects x hidden_size\n",
    "    \"\"\"\n",
    "    h0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    c0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    \n",
    "    return h0, c0"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:38:38.252362400Z",
     "start_time": "2024-04-12T11:38:38.232604800Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def gen_dropout_mask(input_size, hidden_size, is_training, p, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    is_training: if True, gen masks from Bernoulli\n",
    "                 if False, gen masks consisting of (1-p)\n",
    "    \n",
    "    return dropout masks of size input_size, hidden_size if p is not None\n",
    "    return one masks if p is None\n",
    "    \"\"\"\n",
    "    if p is None:\n",
    "        return some_existing_tensor.new_ones((input_size, hidden_size))\n",
    "    \n",
    "    if is_training:\n",
    "        return some_existing_tensor.new_empty((input_size, hidden_size)).bernoulli_(1 - p)\n",
    "    else:\n",
    "        return some_existing_tensor.new_full((input_size, hidden_size), fill_value=1 - p)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:38:38.259361Z",
     "start_time": "2024-04-12T11:38:38.237762300Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите класс-обёртку над `LSTMCell` для реализации Variational Dropout. **Используйте только цикл по времени**"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:09:12.282613Z",
     "start_time": "2021-04-01T21:09:12.256019Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout: Optional[float] = None, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device if device is not None else torch.device('cpu')\n",
    "        \n",
    "        self.rnn_cell = torch.nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize h_0, c_0\n",
    "        h, c = init_h0_c0(\n",
    "            num_objects=x.shape[1],\n",
    "            hidden_size=self.hidden_size,\n",
    "            some_existing_tensor=x\n",
    "        )\n",
    "        \n",
    "        h = h.to(self.device)\n",
    "        c = c.to(self.device)\n",
    "        \n",
    "        # Gen masks for input and hidden state\n",
    "        p = self.dropout\n",
    "        \n",
    "        input_mask = gen_dropout_mask(\n",
    "            input_size=x.shape[1],\n",
    "            hidden_size=self.input_size,\n",
    "            is_training=self.training,\n",
    "            p=p,\n",
    "            some_existing_tensor=x,\n",
    "        ).to(device)\n",
    "        \n",
    "        hidden_st_mask = gen_dropout_mask(\n",
    "            input_size=x.shape[1],\n",
    "            hidden_size=self.hidden_size,\n",
    "            is_training=self.training,\n",
    "            p=p,\n",
    "            some_existing_tensor=x,\n",
    "        ).to(device)\n",
    "                \n",
    "        # Implement recurrent logic and return what nn.LSTM returns\n",
    "        # Do not forget to apply generated dropout masks!\n",
    "        \n",
    "        rs = x.new_empty((x.shape[0], x.shape[1], self.hidden_size)).to(self.device)\n",
    "        \n",
    "        for idx in range(x.shape[0]):\n",
    "            # print(f\"DEB: {x.shape=}\\t{h.shape=}\\t{hidden_st_mask.shape=}\\t{input_mask.shape=}\")\n",
    "#             inp = x[idx] * input_mask.broadcast_to((x.shape[1], x.shape[2]))\n",
    "#             h = h * hidden_st_mask.broadcast_to((x.shape[1], self.hidden_size))\n",
    "            \n",
    "            inp = x[idx] * input_mask\n",
    "            h = h * hidden_st_mask\n",
    "    \n",
    "            h, c = self.rnn_cell(inp, (h, c))\n",
    "            rs[idx] = h\n",
    "        \n",
    "        return rs, (h, c)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:38:38.271360300Z",
     "start_time": "2024-04-12T11:38:38.241225600Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте реализованную модель с выключенным дропаутом (слой `RNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Сильно ли оно увеличилось по сравнению с `torch.nn.LSTM` (LSTM \"из коробки\")?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:39:51.746874900Z",
     "start_time": "2024-04-12T11:39:51.731876100Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=RNNLayer, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:39:52.200061400Z",
     "start_time": "2024-04-12T11:39:52.183332Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"DEB RNN ratings DO=0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:40:05.893103700Z",
     "start_time": "2024-04-12T11:39:52.826026400Z"
    }
   },
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_143952-nq67sl7m</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace' target=\"_blank\">DEB RNN ratings DO=0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0bc9e0d10594b4cbae49d71b92c94a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bcb8b67433b42838dd4aa12922f0f3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте полученную модель с `dropout=0.25`, вновь замерив время обучения. Получилось ли побороть переобучение? Сильно ли дольше обучается данная модель по сравнению с предыдущей? (доп. время тратится на генерацию масок дропаута)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=RNNLayer, dropout=0.25,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T15:30:11.356544800Z",
     "start_time": "2024-04-06T15:30:11.293802500Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"RNN ratings DO=0.25\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T16:01:38.201037Z",
     "start_time": "2024-04-06T15:30:11.341544200Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Гала и Гарамани. Дубль 2 (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<начало взлома pytorch>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "При разворачивании цикла по времени средствами python обучение рекуррентной нейросети сильно замедляется. Однако для реализации дропаута Гала и Гарамани необязательно явно задавать в коде умножение нейронов на маски. Можно схитрить и обойтись использованием слоя `torch.nn.LSTM`: перед вызовом `forward` слоя `torch.nn.LSTM` подменять его веса на веса, домноженные по строкам на маски. А обучаемые веса хранить отдельно. Именно так этот дропаут реализован в библиотеке `fastai`, код из которой использован в ячейке ниже.\n",
    "\n",
    "Такой слой реализуется в виде обертки над `torch.nn.LSTM`. Допишите класс:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:45:36.905451100Z",
     "start_time": "2024-04-12T11:45:36.847265200Z"
    }
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FastRNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, layers_dropout=0.0, num_layers=1, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.layers_dropout = layers_dropout\n",
    "        self.module = torch.nn.LSTM(input_size, hidden_size, dropout=layers_dropout, num_layers=num_layers, device=device)\n",
    "\n",
    "        self.layer_names = []\n",
    "        self.layer_name_tuples = []\n",
    "        for layer_n in range(self.num_layers):\n",
    "            self.layer_names += [f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}']\n",
    "            self.layer_name_tuples.append((f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}'))\n",
    "        \n",
    "        for layer in self.layer_names:\n",
    "            # Get torch.nn.Parameter with weights from torch.nn.LSTM instance\n",
    "            w = getattr(self.module, layer)\n",
    "\n",
    "            # Remove it from model\n",
    "            delattr(self.module, layer)\n",
    "\n",
    "            # And create new torch.nn.Parameter with the same data but different name\n",
    "            self.register_parameter(f'{layer}_raw', torch.nn.Parameter(w.data))\n",
    "\n",
    "            # Note. In torch.nn.LSTM.forward parameter with name `layer` will be used\n",
    "            #     so we must initialize it using `layer_raw` before forward pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _setweights(self, x):\n",
    "        \"\"\"\n",
    "            Apply dropout to the raw weights.\n",
    "        \"\"\"\n",
    "        p = 0 if self.dropout is None else self.dropout\n",
    "        \n",
    "        for layer in self.layer_names:\n",
    "            # Generate mask\n",
    "            \n",
    "            # Get torch.nn.Parameter with weights\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            \n",
    "            # Apply dropout mask\n",
    "            if 'ih' in layer:\n",
    "                input_mask = gen_dropout_mask(\n",
    "                    input_size=1,\n",
    "                    hidden_size=self.input_size,\n",
    "                    is_training=self.training,\n",
    "                    p=p,\n",
    "                    some_existing_tensor=x,\n",
    "                ).view((-1, ))\n",
    "                \n",
    "                # print(f\"DEB: {raw_w.shape=}\\t{input_mask.shape=}\")\n",
    "                \n",
    "                masked_raw_w = raw_w * input_mask\n",
    "            elif 'hh' in layer:\n",
    "                hidden_st_mask = gen_dropout_mask(\n",
    "                    input_size=1,\n",
    "                    hidden_size=self.hidden_size,\n",
    "                    is_training=self.training,\n",
    "                    p=p,\n",
    "                    some_existing_tensor=x,\n",
    "                ).view((-1, ))\n",
    "                \n",
    "                # print(f\"DEB: {raw_w.shape=}\\t{hidden_st_mask.shape=}\")\n",
    "                \n",
    "                masked_raw_w = raw_w * hidden_st_mask\n",
    "            else:\n",
    "                assert False, \"WHF???\"\n",
    "            \n",
    "            # Set modified weights in its place\n",
    "            setattr(self.module, layer, masked_raw_w)\n",
    "\n",
    "    def forward(self, x, h_c: Optional[Tuple[torch.Tensor, torch.Tensor]]=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor containing the features of the input sequence.\n",
    "        :param Optional[Tuple[torch.Tensor, torch.Tensor]] h_c: initial hidden state and initial cell state\n",
    "        \"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            # To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # Set new weights of self.module and call its forward\n",
    "            # Pass h_c with x if it is not None. Otherwise pass only x\n",
    "           \n",
    "            self._setweights(x)  # set weights from layer_raw to layer\n",
    "            \n",
    "            if h_c is not None:\n",
    "                return self.module.forward(x, h_c)\n",
    "            else:\n",
    "                return self.module.forward(x)\n",
    "            \n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'):\n",
    "            self.module.reset()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:46:49.716712800Z",
     "start_time": "2024-04-12T11:46:49.593391700Z"
    }
   },
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте реализованную модель с выключенным дропаутом (слой `FastRNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Убедитесь, что модель выдаёт такое же качество, как и оригинальная реализация LSTM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=FastRNNLayer, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:46:56.736930Z",
     "start_time": "2024-04-12T11:46:50.588073100Z"
    }
   },
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a2f36aea9dc4d1981caa9ef4969a290"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▃▄▆▇▇▇█</td></tr><tr><td>eval/loss</td><td>██▇▆▃▂▂▂▁</td></tr><tr><td>train/accuracy</td><td>▁▂▃▄▅▆▇▇█</td></tr><tr><td>train/loss</td><td>█▇▇▆▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.37148</td></tr><tr><td>eval/loss</td><td>0.02561</td></tr><tr><td>train/accuracy</td><td>0.43556</td></tr><tr><td>train/loss</td><td>0.02251</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">FastRNN ratings DO=0</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/pza55zzo/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/pza55zzo/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240412_144544-pza55zzo\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"FastRNN ratings DO=0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:48:30.599231600Z",
     "start_time": "2024-04-12T11:46:56.729929100Z"
    }
   },
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2defc673b94d47d2987e891dff4cf707"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_144656-1rdcvo1m</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/1rdcvo1m/workspace' target=\"_blank\">FastRNN ratings DO=0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/1rdcvo1m/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/1rdcvo1m/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33dfd545c9b34c5f85d87c0fc1a1ec6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b226040f1e9403aab0da47f9b86ea24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.031/0.032. Accuracy (Train/Test): 0.234/0.232\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52d5d927ba1c45df9f7b99d74d1fc63a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.031/0.031. Accuracy (Train/Test): 0.256/0.247\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7440c7e6e14c40e79124056c91ae470c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.281/0.267\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5280554773654cba9f409adda3ba100c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.029/0.030. Accuracy (Train/Test): 0.312/0.297\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb51c74720954a3c818d7deaa6af6d7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.026/0.028. Accuracy (Train/Test): 0.363/0.339\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbf9045aaab045b890a72b76a05dfa54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.389/0.351\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4967a6ad7f25413c95b2a7a434ca1d8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.024/0.026. Accuracy (Train/Test): 0.402/0.352\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43e6e67781bb43acb742b5d08d7b22de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.024/0.026. Accuracy (Train/Test): 0.408/0.350\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ddfba696f3740f38bd59a377679e6e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.023/0.026. Accuracy (Train/Test): 0.436/0.371\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d70b283eab5546e68a5fa2cdb63e717b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.022/0.025. Accuracy (Train/Test): 0.452/0.371\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58e1ebac72204c08b0487188cdebb8d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.462/0.365\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c83a7ad3659044f79d17b3850ab39aaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.464/0.374\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "434a32bc4b4c411d94d318d7d4d12853"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.487/0.379\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72a547123ffd406f9a0e345fec68629d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.493/0.381\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2d18d02654041bb9ef527fb540c9b32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.501/0.381\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6438bd095b5480992f98c44b277df27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▃▄▆▇▇▇██▇████</td></tr><tr><td>eval/loss</td><td>██▇▆▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▄▅▅▆▆▇▇▇███</td></tr><tr><td>train/loss</td><td>██▇▆▅▄▄▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.38128</td></tr><tr><td>eval/loss</td><td>0.02588</td></tr><tr><td>train/accuracy</td><td>0.50056</td></tr><tr><td>train/loss</td><td>0.0199</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">FastRNN ratings DO=0</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/1rdcvo1m/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/1rdcvo1m/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240412_144656-1rdcvo1m\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте полученный слой (вновь подставив его в `RNNClassifier` в качестве `rec_layer`) с `dropout=0.25`. Сравните время обучения с предыдущими моделями. Проследите, чтобы качество получилось такое же, как при первой реализации этого дропаута."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=FastRNNLayer, dropout=0.25,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:49:00.594023600Z",
     "start_time": "2024-04-12T11:49:00.549057900Z"
    }
   },
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"FastRNN ratings DO=0.25\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:50:33.929680700Z",
     "start_time": "2024-04-12T11:49:00.784086Z"
    }
   },
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_144900-c7c94pr2</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/c7c94pr2/workspace' target=\"_blank\">FastRNN ratings DO=0.25</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/c7c94pr2/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/c7c94pr2/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97c258f9437c40178320b0f2a1fd8de8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c65a04f0f8b4930b32ddc467cced8c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.031/0.032. Accuracy (Train/Test): 0.227/0.227\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4845b2ccb6243688a64f20d47ec1bdc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.031/0.031. Accuracy (Train/Test): 0.244/0.242\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71481497d5614912aa6e387bbb33d9ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.031/0.031. Accuracy (Train/Test): 0.261/0.256\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd2446a6ac3141d9b07fb817aa1870bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.030/0.030. Accuracy (Train/Test): 0.284/0.273\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff096a2b704f4e6bb87a26ab55d758b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.028/0.029. Accuracy (Train/Test): 0.319/0.302\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3793af55546e419c9c3ecae7e61df918"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.027/0.028. Accuracy (Train/Test): 0.342/0.329\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f314410911c4425c828f03ae203c1313"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.026/0.027. Accuracy (Train/Test): 0.369/0.345\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce4c55e94b6f4fb8981b4424a1c78fed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.025/0.026. Accuracy (Train/Test): 0.387/0.354\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f37a90fc51d0484cb225a7e1fcfa5c3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.024/0.025. Accuracy (Train/Test): 0.402/0.371\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11cc3c7e679244c8aa3d758d0f917510"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.024/0.026. Accuracy (Train/Test): 0.407/0.364\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ad5d200bf9d43cd98e45e9574970708"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.023/0.025. Accuracy (Train/Test): 0.426/0.381\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30fe84032bab4fa2afbcd9e3c0b0bb90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.023/0.025. Accuracy (Train/Test): 0.436/0.388\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1198698c02ee49669105d1eba0457b1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.023/0.025. Accuracy (Train/Test): 0.426/0.380\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e6edfe316dc421a8b4cc0e42c51697f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.022/0.024. Accuracy (Train/Test): 0.451/0.389\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0341c5d022942df815396aca03853e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.022/0.024. Accuracy (Train/Test): 0.460/0.394\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5a16406c73f44abaa34483916784670"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▂▃▄▅▆▆▇▇▇█▇██</td></tr><tr><td>eval/loss</td><td>██▇▇▆▅▄▃▂▂▂▁▂▁▁</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▄▄▅▆▆▆▇▇▇██</td></tr><tr><td>train/loss</td><td>██▇▇▆▅▄▃▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.39444</td></tr><tr><td>eval/loss</td><td>0.02426</td></tr><tr><td>train/accuracy</td><td>0.45956</td></tr><tr><td>train/loss</td><td>0.02161</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">FastRNN ratings DO=0.25</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/c7c94pr2/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/c7c94pr2/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240412_144900-c7c94pr2\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "</конец взлома pytorch>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Семениуты и др. (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Перейдем к реализации дропаута для LSTM по статье [Semeniuta et al](http://www.aclweb.org/anthology/C16-1165). \n",
    "\n",
    "Этот метод применения дропаута не менее популярен, чем предыдущий. Его особенность состоит в том, что он придуман специально для гейтовых архитектур. В контексте LSTM этот дропаут накладывается только на информационный поток ($m_h$ — маска дропаута):\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot g \\odot {\\bf m_h} \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$\n",
    "На входы $x_t$ маска накладывается как в предыдущем дропауте. Впрочем, на входы маску можно наложить вообще до вызова рекуррентного слоя.\n",
    "\n",
    "Согласно статье, маска дропаута может быть как одинаковая, так и разная для всех моментов времени. Мы сделаем одинаковую для всех моментов времени.\n",
    "\n",
    "Для реализации этого дропаута можно: \n",
    "1. самостоятельно реализовать LSTM (интерфейса LSTMCell не хватит) \n",
    "2. снова воспользоваться трюком с установкой весов (но тут мы опираемся на свойство $tanh(0)=0$, к тому же, трюк в данном случае выглядит менее тривиально, чем с дропаутом Гала). \n",
    "\n",
    "Предлагается реализовать дропаут по сценарию 1. Допишите класс:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class HandmadeLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device('cpu') if device is None else device\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_weights = torch.nn.Linear(input_size, 4 * hidden_size).to(self.device)\n",
    "        self.hidden_weights = torch.nn.Linear(hidden_size, 4 * hidden_size).to(self.device)\n",
    "        self.activations = nn.ModuleList([\n",
    "            nn.ReLU(), # i\n",
    "            nn.ReLU(), # o\n",
    "            nn.ReLU(), # f\n",
    "            nn.Tanh(), # g\n",
    "            nn.Tanh(), # h\n",
    "        ]).to(self.device)\n",
    "        \n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        \"\"\"\n",
    "        Initialization as in Pytorch. \n",
    "        Do not forget to call this method!\n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert not torch.any(torch.isnan(x)), f\"x has nan: {x=}\"\n",
    "        # Use functions init_h0_c0 and gen_dropout_masks defined above\n",
    "        seq_len = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        input_dim = x.shape[2]\n",
    "        \n",
    "        h, c = init_h0_c0(\n",
    "            num_objects=batch_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            some_existing_tensor=x,\n",
    "        )\n",
    "        \n",
    "        # input_mask, hidden_st_mask = gen_dropout_mask(\n",
    "        #     input_size=input_dim,\n",
    "        #     hidden_size=hidden_dim,\n",
    "        #     is_training=self.training,\n",
    "        #     p=self.dropout,\n",
    "        #     some_existing_tensor=x,\n",
    "        # )\n",
    "        \n",
    "        # print(f\"DEB: {torch.all(input_mask == 1.0)}\\t{torch.all(hidden_st_mask == 1.0)}\")\n",
    "        \n",
    "        # Implement recurrent logic to mimic torch.nn.LSTM\n",
    "        # Do not forget to apply dropout mask\n",
    "        rs = x.new_empty((seq_len, batch_size, self.hidden_size))\n",
    "        for idx in range(seq_len):\n",
    "            # x ~ [L, B, F], h ~ [H]\n",
    "            # print(f\"DEB: {x.shape=}\\t{h.shape=}\\t{c.shape=}\")\n",
    "            \n",
    "            # DEB: disable masks\n",
    "            # inp = x[idx, :, :] * input_mask # x --> [B, F]\n",
    "            inp = x[idx, :, :]\n",
    "            assert not torch.any(torch.isnan(inp)), f\"inp has nan: idx={idx}\\n{inp=}\"\n",
    "            \n",
    "            inp4 = self.input_weights(inp)  # x4 ~ [B, 4H]\n",
    "            h4 = self.hidden_weights(h)  # h4 ~ [4H]\n",
    "            assert not torch.any(torch.isnan(inp4)), f\"inp4 has nan: idx={idx}\\n{inp4=}\"\n",
    "            assert not torch.any(torch.isnan(h4)), f\"h4 has nan: idx={idx}\\n{h4=}\"\n",
    "            \n",
    "            y = inp4 + h4.broadcast_to((batch_size, 4 * self.hidden_size))\n",
    "            assert not torch.any(torch.isnan(y)), f\"y has nan: idx={idx}\\n{y=}\"\n",
    "            \n",
    "            i = self.activations[0](y[:, 0:self.hidden_size])\n",
    "            o = self.activations[1](y[:, 1*self.hidden_size:2*self.hidden_size])\n",
    "            f = self.activations[2](y[:, 2*self.hidden_size:3*self.hidden_size])\n",
    "            g = self.activations[3](y[:, 3*self.hidden_size:4*self.hidden_size])\n",
    "            \n",
    "            assert not torch.any(torch.isnan(i)), f\"i has nan: idx={idx}\\n{i=}\"\n",
    "            assert not torch.any(torch.isnan(o)), f\"o has nan: idx={idx}\\n{o=}\"\n",
    "            assert not torch.any(torch.isnan(f)), f\"f has nan: idx={idx}\\n{f=}\"\n",
    "            assert not torch.any(torch.isnan(g)), f\"g has nan: idx={idx}\\n{g=}\"\n",
    "            \n",
    "            # DEB: disable masks\n",
    "            # c = f * c + i * g * hidden_st_mask\n",
    "            c = f * c + i * g\n",
    "            h = o * self.activations[4](c)\n",
    "            assert not torch.any(torch.isnan(c)), f\"c has nan: idx={idx}\\n{c=}\"\n",
    "            assert not torch.any(torch.isnan(h)), f\"h has nan: idx={idx}\\n{h=}\"\n",
    "            \n",
    "            rs[idx, :, :] = h\n",
    "        \n",
    "        assert not torch.any(torch.isnan(rs)), f\"rs has nan: {rs=}\"\n",
    "        return rs, (h, c)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:47.175776300Z",
     "start_time": "2024-04-12T11:56:47.121999800Z"
    }
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте вашу реализацию без дропаута (проконтролируйте качество и сравните время обучения с временем обучения `torch.nn.LSTM` и `RNNLayer`), а также с `dropout=0.25`. Сравните качество модели с таким дропаутом с качеством модели с дропаутом Гала и Гарамани."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:48.427670500Z",
     "start_time": "2024-04-12T11:56:48.365671500Z"
    }
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=HandmadeLSTM, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:48.935714200Z",
     "start_time": "2024-04-12T11:56:48.890122200Z"
    }
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"HandmadeLSTM ratings DO=0.0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:09:13.369299700Z",
     "start_time": "2024-04-12T11:56:49.600384700Z"
    }
   },
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01145555555555499, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6a25206998d45aba4d7eec256bf2312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_145649-57fgbyzk</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace' target=\"_blank\">HandmadeLSTM ratings DO=0.0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7127aa3a73084c9d85403fb4f76f17cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bebf6b51d52945bd93d60276aebb160e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.030/0.032. Accuracy (Train/Test): 0.264/0.233\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a955bc0791124d4884a1592a89fd63d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.029/0.031. Accuracy (Train/Test): 0.293/0.242\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac264afed37a46399e18cb26076b9785"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Embeddings has NaNs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHandmadeLSTM ratings DO=0.0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[36], line 66\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs, name)\u001B[0m\n\u001B[0;32m     63\u001B[0m rng \u001B[38;5;241m=\u001B[39m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs))\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m rng:\n\u001B[1;32m---> 66\u001B[0m     \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m evaluate(train_loader, model, loss_fn, device)\n\u001B[0;32m     69\u001B[0m     train_accuracies\u001B[38;5;241m.\u001B[39mappend(train_acc)\n",
      "Cell \u001B[1;32mIn[36], line 17\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(dataloader, model, loss_fn, optimizer, device)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(torch\u001B[38;5;241m.\u001B[39misnan(tokens)), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEval epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 17\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens_lens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits, ratings)\n\u001B[0;32m     19\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[33], line 71\u001B[0m, in \u001B[0;36mRNNClassifier.forward\u001B[1;34m(self, tokens, tokens_lens)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# DEBUG: store last input in globals\u001B[39;00m\n\u001B[0;32m     69\u001B[0m emb_dump \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(torch\u001B[38;5;241m.\u001B[39misnan(x)), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbeddings has NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# Make forward pass through recurrent network\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# YOUR CODE HERE\u001B[39;00m\n\u001B[0;32m     75\u001B[0m x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(x)\n",
      "\u001B[1;31mAssertionError\u001B[0m: Embeddings has NaNs"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([   0,    1,    4,  ..., 5001, 5004, 5016])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dump = inp_dump.cpu()\n",
    "\n",
    "torch.unique(inp_dump)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:12:56.040351100Z",
     "start_time": "2024-04-12T12:12:55.983535100Z"
    }
   },
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:15:24.836069800Z",
     "start_time": "2024-04-12T12:15:24.780492400Z"
    }
   },
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Token: 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(vocab)):\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(\n\u001B[0;32m      3\u001B[0m         torch\u001B[38;5;241m.\u001B[39misnan(\n\u001B[0;32m      4\u001B[0m             model\u001B[38;5;241m.\u001B[39mword_embeddings(\n\u001B[0;32m      5\u001B[0m                 torch\u001B[38;5;241m.\u001B[39mLongTensor([i])\n\u001B[0;32m      6\u001B[0m             )       \n\u001B[0;32m      7\u001B[0m         )\n\u001B[0;32m      8\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToken: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Token: 1"
     ]
    }
   ],
   "source": [
    "for i in range(len(vocab)):\n",
    "    assert not torch.any(\n",
    "        torch.isnan(\n",
    "            model.word_embeddings(\n",
    "                torch.LongTensor([i])\n",
    "            )       \n",
    "        )\n",
    "    ), f\"Token: {i}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:24.317561300Z",
     "start_time": "2024-04-12T12:16:24.250852100Z"
    }
   },
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        [ 1.1044, -1.1028,  0.5543,  ..., -0.6463,  0.0749,  0.1939],\n        ...,\n        [ 1.0837, -0.6411,  1.0597,  ...,  1.3895, -0.3897,  0.6304],\n        [ 0.4510, -0.0567, -1.1788,  ..., -0.0786,  2.0267,  0.6129],\n        [-0.6576, -0.7521,  0.2827,  ..., -0.0446,  1.5368, -0.1816]],\n       requires_grad=True)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embeddings.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:45.419413500Z",
     "start_time": "2024-04-12T12:16:45.392862300Z"
    }
   },
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.387698Z",
     "start_time": "2024-03-30T22:22:19.019606Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Сравнение всех предложенных моделей (1 балл)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T23:33:28.831346Z",
     "start_time": "2021-04-01T23:33:28.810453Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Используя замеры времени заполните табличку с временем работы четырёх реализованных моделей в следующей ячейке:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T23:48:05.361634Z",
     "start_time": "2021-04-01T23:48:05.333901Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| torch.nn.LSTM | RNNLayer | FastRNNLayer | HandmadeLSTM |\n",
    "|---------------|----------|--------------|--------------|\n",
    "| 2m 35s        | 14m 16s  | 2m 41s       | 31m 44s      |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.566217Z",
     "start_time": "2024-03-30T22:43:31.389958Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Крайне желательно рисовать графики в векторном формате. \n",
    "\n",
    "Если по каким-то причинам, отрисовка не будет работать, закомментируйте следующую ячейку."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib_inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.573342Z",
     "start_time": "2024-03-30T22:43:31.567690Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нарисуйте два графика — функция потерь и качество на обучающей и тестовой выборке для всех 7 моделей обученных выше."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "...\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_title('CrossEntropy Loss')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_title('Accuracy')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:32.290598Z",
     "start_time": "2024-03-30T22:43:31.575031Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сделайте итоговые выводы о качестве работы моделей с разными реализациями DropOut:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Бонус. Zoneout (0.5 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Это еще одна модификация идеи дропаута применительно к рекуррентным нейросетям. В Zoneout на каждом временном шаге с вероятностью $p$ компонента скрытого состояния обновляется, а с вероятностью $1-p$ берется с предыдущего шага. \n",
    "В Виде формул ($m^t_h$ - бинарная маска):\n",
    " \n",
    "(сначала обычный рекуррентный переход, например LSTM)\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$\n",
    "Затем Zoneout:\n",
    "$$\n",
    "h_t = h_t * m_h^t + h_{t-1}*(1-m_h^t)\n",
    "$$\n",
    "В этом методе маска уже должна быть разная во все моменты времени (иначе метод упрощается до дропаута Гала и Гарамани). На входы $x_t$ вновь можно накладывать маску до начала работы рекуррентного слоя.  \n",
    "\n",
    "Если у вас осталось время, вы можете реализовать этот метод. Выберите основу из трех рассмотренных случаев самостоятельно.\n",
    "\n",
    "**Полный балл ставится только при наличии качественного и количественного сравнения с предыдущими моделями.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 2. Language Modeling с помощью LSTM (5 баллов)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:05:00.702763Z",
     "start_time": "2021-03-31T16:05:00.674835Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Во второй части мы попробуем обучить модель для генерации отзывов по их началу."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Концептуально модель будет выглядеть следующим образом:\n",
    "    \n",
    "![image info](https://blog.feedly.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-12.08.35-PM.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В процессе обучения будем тренировать сеть предсказывать вероятность следующего символа при условии всех предыдущих. Эту вероятность можно моделировать с помощью скрытого состояния $h^{(t)}$ пропуская его через линейный слой с выходной размерностью равной размерности словаря:\n",
    "$$\n",
    "p(x^{t}|x^{t-1}, ..., x^{1}) = SoftMax(Linear(h^{(t)}))\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обратите внимание, что для вычисления $p(x^{t}|x^{t-1}, ..., x^{1})$ для всех моментов времени достаточно сделать один проход по RNN, а затем применить линейное преобразование ко всем скрытым состояниям."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве функции потерь необходимо использовать `CrossEntropy`."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T00:37:56.100520Z",
     "start_time": "2021-04-02T00:37:56.072747Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим другой важный момент. Для того, чтобы решить данную задачу, модель должна уметь определять момент начала генерации предложения и оповещать о завершении генерации — конце предложения. Для этого добавим в словарь вспомогательные токены `<sos>`, `<eos>`. Добавив `<sos>` в начало каждого предложения и `<eos>` в конец.\n",
    "\n",
    "Модель сможет начинать генерацию как только ей будет передан токен `<sos>` и заканчивать генерацию, как только на очередном месте самым вероятным токеном оказывается `<eos>`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для решения этой задачи мы воспользуемся уже реализованной LSTM с дропаутом `FastRNNLayer` и классом `RNNClassifier`, то есть архитектура сети принципиально не поменяется. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация модели и цикла обучения (2 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Не используйте циклы в `RNNLM`, `LMCrossEntropyLoss`, `LMAccuracy`**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNLM(RNNClassifier):\n",
    "    def __init__(\n",
    "        self, embedding_dim, hidden_dim, vocab, dropout=0.5, layers_dropout=0.5, num_layers=1\n",
    "    ):\n",
    "        # super().__init__(\n",
    "        #     embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(vocab), vocab=vocab,\n",
    "        #     rec_layer=FastRNNLayer, dropout=dropout, layers_dropout=layers_dropout, num_layers=num_layers\n",
    "        # )\n",
    "    \n",
    "        super().__init__(\n",
    "            embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(vocab), vocab=vocab,\n",
    "            rec_layer=nn.LSTM, dropout=None, num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, len(vocab))\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor(dtype=torch.long) tokens: \n",
    "            Batch of texts represented with tokens. Shape: [T, B]\n",
    "        :param torch.Tensor(dtype=torch.long) tokens_lens: \n",
    "            Number of non-padding tokens for each object in batch. Shape: [B]\n",
    "        :return torch.Tensor: \n",
    "            Distribution of next token for each time step. Shape: [T, B, V], V — size of vocabulary\n",
    "        \"\"\"\n",
    "        # Make embeddings for all tokens\n",
    "        x = self.word_embeddings(tokens)\n",
    "        \n",
    "        # Forward pass embeddings through network\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Take all hidden states from the last layer of LSTM for each step and perform linear transformation\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.335940200Z",
     "start_time": "2024-04-14T09:58:22.311673200Z"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуем функцию потерь для данной задачи. \n",
    "\n",
    "Моменты на которые нужно обратить внимание:\n",
    "1. Распределение вероятности следующего токена для последнего токена в последовательности не участвует в подсчёте функции потерь.\n",
    "2. Необходимо учитывать, что в одном батче могут быть тексты разной длины."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для решения второй проблемы можно воспользоваться функцией `torch.nn.utils.rnn.pack_padded_sequence`. \n",
    "\n",
    "Принимая на вход батч тензоров и длину каждого тензора без учёта паддинга эта функция позволяет получить все элементы в тензорах, которые не относятся к паддингу в виде плоского массива:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "padded_tensors = torch.tensor([\n",
    "    [[1, 11, 111], [2, 22, 222], [3, 33, 333]],\n",
    "    [[4, 44, 444], [5, 55, 555], [6, 66, 666]],\n",
    "    [[7, 77, 777], [0, 0, 0], [8, 88, 888]],\n",
    "    [[9, 99, 999], [0, 0, 0], [0, 0, 0]]\n",
    "])\n",
    "tensors_lens = torch.tensor([4, 2, 3])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.338938700Z",
     "start_time": "2024-04-14T09:58:22.319430600Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обратите внимание, что `torch.nn.utils.rnn.pack_padded_sequence` автоматически переупорядочивает тензоры в батче по убыванию их длины."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.nn.utils.rnn.pack_padded_sequence(padded_tensors, tensors_lens, batch_first=False, enforce_sorted=False)[0].shape"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.349620700Z",
     "start_time": "2024-04-14T09:58:22.321816300Z"
    }
   },
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([9, 3])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class LMCrossEntropyLoss(torch.nn.CrossEntropyLoss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, outputs, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n",
    "        :param torch.Tensor tokens: Batch of tokens. Shape: [T, B]\n",
    "        :param torch.Tensor tokens_lens: Length of each sequence in batch\n",
    "        :return torch.Tensor: CrossEntropyLoss between corresponding logits and tokens\n",
    "        \"\"\"\n",
    "        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n",
    "        # Do not forget specify enforce_sorted=False and correct value of batch_first \n",
    "        \n",
    "        # print(f\"DEB: {outputs.shape=}\\t{tokens.shape=}\")\n",
    "        \n",
    "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "        \n",
    "        # print(f\"DEB: {packed_outputs.shape=}\\t{packed_tokens.shape=}\")\n",
    "        \n",
    "        # Use super().forward(..., ...) to compute CrossEntropyLoss\n",
    "        return super().forward(\n",
    "            input=packed_outputs[:-1, :],\n",
    "            target=packed_tokens[1:]\n",
    "        )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.442014400Z",
     "start_time": "2024-04-14T09:58:22.328306800Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для оценки качества нам также необходимо вычислять долю правильно предсказанных токенов. Реализуйте класс для вычисления точности."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class LMAccuracy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, outputs, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n",
    "        :param torch.Tensor tokens: Batch of tokens. Shape: [T, B]\n",
    "        :param torch.Tensor tokens_lens: Length of each sequence in batch\n",
    "        :return torch.Tensor: Accuracy for given logits and tokens\n",
    "        \"\"\"\n",
    "        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n",
    "        # Do not forget specify enforce_sorted=False and correct value of batch_first \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "    \n",
    "        preds = torch.argmax(packed_outputs, dim=1)\n",
    "    \n",
    "        return torch.sum(\n",
    "            preds[:-1] == packed_tokens[1:]\n",
    "        )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.446015Z",
     "start_time": "2024-04-14T09:58:22.330676600Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Модифицируйте функции `train_epoch`, `evaluate`, `train` для обучения LM.\n",
    "\n",
    "**При вычислении точности, обратите внимание на то, что мы не предсказываем первый токен в каждой последовательности и токены, относящиеся к паддингу.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch_lm(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Make optimizer step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, tokens, tokens_lens)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_lm(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    accuracy_fn = LMAccuracy()  # DEB: return 0.0\n",
    "    \n",
    "    for idx, data in enumerate(dataloader):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Evaluate accuracy\n",
    "        \n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, tokens, tokens_lens)\n",
    "        acc = accuracy_fn(logits, tokens, tokens_lens)\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += torch.sum(tokens_lens)\n",
    "        total_accuracy += acc\n",
    "        \n",
    "            \n",
    "    return total_loss / total_tokens, total_accuracy / total_tokens\n",
    "\n",
    "def train_lm(\n",
    "    train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs\n",
    "):\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_epoch_lm(train_loader, model, loss_fn, optimizer, device)\n",
    "        \n",
    "        train_loss, train_acc = evaluate_lm(train_loader, model, loss_fn, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss, test_acc = evaluate_lm(test_loader, model, loss_fn, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(\n",
    "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. Accuracy (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
    "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.474547Z",
     "start_time": "2024-04-14T09:58:22.334939Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь у нас всё готово для обучения модели."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим словарь с `<sos>`, `<eos>` токенами.\n",
    "\n",
    "Обратите внимание, что в отличие от классификации текстов нам необходимо значительно увеличить размер словаря, чтобы доля `<unk>` токенов была не велика.\n",
    "\n",
    "Так же, так как задача генерации значительно сложнее задачи классификации текстов будем обучать модель только на префиксах рецензий длины $20$. Это позволяет значительно ускорить обучение."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T01:06:12.736180Z",
     "start_time": "2021-04-02T01:06:12.708814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for special in specials:\n",
    "    counter[special] = 0\n",
    "# min_freq=8 is approximately equivalent to max_size=30000. \n",
    "#   You can lower min_freq in order to make model vocabulary more diverse \n",
    "lm_vocab = torchtext.vocab.vocab(counter, specials=specials, special_first=True, min_freq=8)\n",
    "lm_vocab.set_default_index(vocab['<unk>'])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:22.487528700Z",
     "start_time": "2024-04-14T09:58:22.342620700Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_test_dataset = LargeMovieReviewDataset(test_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)\n",
    "lm_train_dataset = LargeMovieReviewDataset(train_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:59.703532900Z",
     "start_time": "2024-04-14T09:58:22.382548900Z"
    }
   },
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir\\AppData\\Local\\Temp\\ipykernel_14412\\3422549362.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text).get_text()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим даталоадеры для тестовой и обучающей выборок:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "lm_test_dataloader = DataLoader(\n",
    "    lm_test_dataset, batch_size=196, shuffle=False, num_workers=0, \n",
    "    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n",
    ")\n",
    "lm_train_dataloader = DataLoader(\n",
    "    lm_train_dataset, batch_size=196, shuffle=True, num_workers=0, \n",
    "    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:59.713049300Z",
     "start_time": "2024-04-14T09:58:59.705533800Z"
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Убедитесь, что все предложения имеют в начале `<sos>` токен, а в конце — `<eos>` токен."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch = next(iter(lm_train_dataloader))\n",
    "batch['tokens'], batch['tokens_lens']"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:59.734264100Z",
     "start_time": "2024-04-14T09:58:59.709050Z"
    }
   },
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[    2,     2,     2,  ...,     2,     2,     2],\n         [ 5712,  2596,     7,  ...,   118,    50, 15429],\n         [ 1323,  1211,     1,  ...,   515,  3519,  4339],\n         ...,\n         [11201,  3732,  3536,  ...,  7189,  5530,    43],\n         [    1,   251,  5254,  ...,  2502,   709,     1],\n         [    3,     3,     3,  ...,     3,     3,     3]]),\n tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 16, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 15, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]))"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим модель, функцию потерь и оптимизатор: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:59.737263400Z",
     "start_time": "2024-04-14T09:58:59.716513700Z"
    }
   },
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "3598"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:58:59.830796300Z",
     "start_time": "2024-04-14T09:58:59.720265400Z"
    }
   },
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": [
    "# lm_model = RNNLM(\n",
    "#     embedding_dim=512, hidden_dim=512, vocab=lm_vocab, dropout=0.6, layers_dropout=0.6, num_layers=2\n",
    "# ).to(device=device)\n",
    "\n",
    "lm_model = RNNLM(\n",
    "    embedding_dim=512, hidden_dim=512, vocab=lm_vocab\n",
    ").to(device=device)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:59:02.355771Z",
     "start_time": "2024-04-14T09:58:59.829796Z"
    }
   },
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_loss_fn = LMCrossEntropyLoss(reduction='mean')\n",
    "lm_optimizer = torch.optim.Adam(lm_model.parameters(), lr=0.005, weight_decay=1.2e-6)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:59:02.591412300Z",
     "start_time": "2024-04-14T09:59:02.356770200Z"
    }
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучим модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# lm_model = torch.compile(lm_model)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T09:59:02.612411500Z",
     "start_time": "2024-04-14T09:59:02.591412300Z"
    }
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_train_losses, lm_train_accuracies, lm_test_losses, lm_test_accuracies = train_lm(\n",
    "    lm_train_dataloader, lm_test_dataloader, lm_model, lm_loss_fn, lm_optimizer, device, 10\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T10:06:11.594758600Z",
     "start_time": "2024-04-14T09:59:02.595411200Z"
    }
   },
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b76a57654384f8e865a6b366c43229d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c3fbb09de744176ba326d1b6eea6541"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.128\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "429ac88386d6441cbb44193eb5e55240"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52aa168356da4f94858db53976f38800"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a215a493fd046d09b265df80c2073c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d4b18ab36824ffe9f4de7966f98ee5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19161d3e4b9444728996883b5dde8c6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9df90da8d42a40f4953d980320e56a10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49184743219a4b1bba4b8f6dd38dbbdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "564e22bdde874735863a5ea11a10324e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ea563ea33014bbab515018ed56637f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10. Loss (Train/Test): 0.002/0.002. Accuracy (Train/Test): 0.129/0.129\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pred_0 = lm_model(batch['tokens'].to(device), batch['tokens_lens'].to(device))[:, 0, :].cpu()\n",
    "pred_0 = torch.argmax(pred_0, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:43:36.760911400Z",
     "start_time": "2024-04-14T09:43:36.429042300Z"
    }
   },
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([2, 7, 7, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]),\n tensor([   2, 2449,    7, 2073,  261,    7,  517, 1757,  223,  463, 1002,  707,\n          952,   97, 1885,   93, 5919,  502,   93,   60,  130,    3]))"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_0, batch['tokens'][:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T09:44:22.264745200Z",
     "start_time": "2024-04-14T09:44:22.128879300Z"
    }
   },
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация декодера (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь, реализуем последнюю деталь — декодирование с использованием обученной модели.\n",
    "Есть несколько вариантов. Рассмотрим два самых простых:\n",
    "1. **Жадное декодирование.** На каждом шаге мы выбираем токен с максимальной вероятностью и используем его для обновления скрытого состояния RNN.\n",
    "2. **Top-k sampling.** На очередном шаге рассматриваются $k$ токенов с самыми большими вероятностями. Остальные токены игнорируются. Из выбранных токенов семплируется следующий токен пропорционально их вероятностям.\n",
    "\n",
    "Прочитать подробнее про разные варианты декодирования можно по ссылкам:\n",
    "1. [От huggingface](https://huggingface.co/blog/how-to-generate)\n",
    "2. [На towardsdatascience](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Существенным в процессе декодирования является критерий останова. Как только очередной самый вероятный символ оказался `<eos>`, то данная последовательность считается сгенерированной. Однако, может так оказаться, что `<eos>` никогда не будет выбран, тогда необходимо прекратить генерацию, как только длина последовательности перейдёт порог `max_generated_len`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def decode(model, start_tokens, start_tokens_lens, max_generated_len=20, top_k=None):\n",
    "    \"\"\"\n",
    "    :param RNNLM model: Model\n",
    "    :param torch.Tensor start_tokens: Batch of seed tokens. Shape: [T, B]\n",
    "    :param torch.Tensor start_tokens_lens: Length of each sequence in batch. Shape: [B]\n",
    "    :param int max_generated_len: Maximum lenght of generated samples\n",
    "    :param Optional[int] top_k: Number of tokens with the largest probability to sample from\n",
    "    :return Tuple[torch.Tensor, torch.Tensor]. \n",
    "        Newly predicted tokens and length of generated part. Shape [T*, B], [B]\n",
    "    \"\"\"\n",
    "    # Get embedding for start_tokens\n",
    "    embedding = model.word_embeddings(start_tokens)\n",
    "    \n",
    "    # Pass embedding through rnn and collect hidden states and cell states for each time moment\n",
    "    all_h, all_c = [], []\n",
    "    h = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    c = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    for time_step in range(start_tokens.shape[0]):\n",
    "        _, (h, c) = model.rnn(embedding[time_step][None, :, :], (h, c))\n",
    "        all_h.append(h)\n",
    "        all_c.append(c)\n",
    "    \n",
    "    all_h = torch.stack(all_h, dim=1)\n",
    "    all_c = torch.stack(all_c, dim=1)\n",
    "    # Take final hidden state and cell state for each start sequence in batch\n",
    "    # We will use them as h_0, c_0 for generation new tokens\n",
    "    h = all_h[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n",
    "    c = all_c[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n",
    "    \n",
    "    # List of predicted tokens for each time step\n",
    "    predicted_tokens = []\n",
    "    # Length of generated part for each object in the batch\n",
    "    decoded_lens = torch.zeros_like(start_tokens_lens, dtype=torch.long)\n",
    "    # Boolean mask where we store if the sequence has already generated\n",
    "    # i.e. `<eos>` was selected on any step\n",
    "    is_finished_decoding = torch.zeros_like(start_tokens_lens, dtype=torch.bool)\n",
    "    \n",
    "    # Stop when all sequences in the batch are finished\n",
    "    while not torch.all(is_finished_decoding) and torch.max(decoded_lens) < max_generated_len:\n",
    "        # Evaluate next token distribution using hidden state h.\n",
    "        # Note. Over first dimension h has hidden states for each layer of LSTM.\n",
    "        #     We must use hidden state from the last layer\n",
    "        # logits, (h, c) = model.rnn(h, (h, c))\n",
    "        # logits = model.linear(logits)\n",
    "        \n",
    "        logits = model.linear(h)\n",
    "        # print(f\"DEB: {logits.shape=}\")\n",
    "        \n",
    "        if top_k is not None:\n",
    "            # Top-k sampling. Use only top-k most probable logits to sample next token\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            # Mask non top-k logits\n",
    "            logits[indices_to_remove] = -1e10\n",
    "            # Sample next_token. \n",
    "            # YOUR CODE HERE\n",
    "            next_token = ...\n",
    "        else:\n",
    "            # Select most probable token\n",
    "            next_token = torch.argmax(logits, dim=2)\n",
    "            # print(f\"DEB: {next_token.shape=}\")\n",
    "            \n",
    "        predicted_tokens.append(next_token)\n",
    "        \n",
    "        decoded_lens += (~is_finished_decoding)\n",
    "        is_finished_decoding |= (next_token[0, :] == torch.tensor(model.vocab.lookup_indices(['<eos>'])[0]))\n",
    "\n",
    "        # Compute embedding for next token\n",
    "        embedding = model.word_embeddings(next_token)\n",
    "        \n",
    "        # Update hidden and cell states\n",
    "        _, (h, c) = model.rnn(embedding, (h, c))\n",
    "        \n",
    "    return torch.stack(predicted_tokens)[:, 0, :], decoded_lens"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:10:32.686653500Z",
     "start_time": "2024-04-14T12:10:32.611687400Z"
    }
   },
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем сгенерировать продолжения для нескольких префиксов:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T01:38:06.232189Z",
     "start_time": "2021-04-02T01:38:06.205413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "start_tokens = torch.tensor([\n",
    "    lm_model.vocab.lookup_indices(['<sos>', '<pad>', '<pad>', '<pad>']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'my', 'favorite', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'best', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'worst', 'movie']),\n",
    "]).T\n",
    "\n",
    "start_tokens_lens = torch.tensor([1, 4, 4, 4])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:10:33.303846400Z",
     "start_time": "2024-04-14T12:10:33.269521Z"
    }
   },
   "execution_count": 109,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_model = lm_model.cpu()\n",
    "lm_model.eval()\n",
    "# decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=5)\n",
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=None)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:10:33.584216300Z",
     "start_time": "2024-04-14T12:10:33.481036300Z"
    }
   },
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 4]), torch.Size([4]))"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens.shape, decoded_lens.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T12:10:38.882051800Z",
     "start_time": "2024-04-14T12:10:38.802769900Z"
    }
   },
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "source": [
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist() + decoded_text_tokens.tolist()\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:11:21.213472400Z",
     "start_time": "2024-04-14T12:11:21.149246100Z"
    }
   },
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> <sos> movie movie movie movie <unk> <unk> <unk> <unk> <unk>\n",
      "<sos> <unk> favorite movie movie movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<sos> <unk> best movie movie movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<sos> <unk> worst movie movie movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуйте выполнить семплирование для разных $k$. Сравните результаты top-k семплирования с жадным декодированием. Опишите ваши наблюдения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:46:28.556188Z",
     "start_time": "2024-03-30T22:46:28.553997Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Beam Search (2 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим более продвинутый алгоритм для декодирования. Реализуйте алгоритм Beam Search."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Несколько замечаний по имплементации:\n",
    "\n",
    "1. При больших размерах `beam_size` число гипотез ($B \\times \\text{beam\\_size}$) на очередном шаге может быть слишком большим. Поэтому может потребоваться разбить все гипотезы на отдельные батчи и делать forward-pass в несколько итераций. Используйте [`torch.split`](https://pytorch.org/docs/stable/generated/torch.split.html)\n",
    "2. Для выбора лучших гипотез используйте [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html). Обратите внимание на индексы, которые возвращает эта функция (может пригодиться метод [`torch.remainder`](https://pytorch.org/docs/stable/generated/torch.remainder.html))\n",
    "3. Можно отслеживать, какие элементы в батче (или какие гипотезы) закончили генерацию. Делая forward-pass только для незавершённых гипотез, можно ускорить декодинг, однако, это усложнит реализацию"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def decode_beam_search(model, start_tokens, start_tokens_lens, max_generated_len=20, beam_size=5):\n",
    "    \"\"\"\n",
    "    :param RNNLM model: Model\n",
    "    :param torch.Tensor start_tokens: Batch of seed tokens. Shape: [T, B]\n",
    "    :param torch.Tensor start_tokens_lens: Length of each sequence in batch. Shape: [B]\n",
    "    :param int max_generated_len: Maximum length of generated samples\n",
    "    :param int beam_size: Size of beam\n",
    "    :return Tuple[torch.Tensor, torch.Tensor, torch.Tensor]. \n",
    "        Newly predicted tokens, lengths of generated parts and log probabilities for each hypotheses \n",
    "        Shape [T*, B, beam_size], [T*, beam_size], [T*, beam_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    # L — number of RNN layers in the model, H — hidden size, BS — beam size\n",
    "    #\n",
    "    # 1. Make forward pass of start_tokens through the model. \n",
    "    #      Obtain the last cell and hidden state for each element in the batch \n",
    "    #          (i.e. tensors of shape [L, B, H])\n",
    "    #      Use those states as the initialization for each hypotheses in the beam \n",
    "    #          (i.e. tensors of shape [L, B * BS, H])\n",
    "    #      Initialize probabilities for each hypotheses in the beam with 1.0\n",
    "    #          (i.e. tensor of shape [B * BS])\n",
    "    #      Initialize vector that show whether hypothesis is finished\n",
    "    #          (i.e. tensor of shape [B * BS])\n",
    "    # 2. While all sequences do not end with <eos> and their length less than max_generated_len\n",
    "    #      1. Get probabilities for the next token for each hypothesis \n",
    "    #          (i.e. tensor of shape [B * BS, V])\n",
    "    #      2. Use those probabilities to compute probability for each extension of each hypothesis\n",
    "    #          (i.e. tensor of shape [B * BS, V])\n",
    "    #      3. For each element in the batch select new BS best hypotheses\n",
    "    #          Note, that some of the hypotheses on the previous step have been finished\n",
    "    #            so their probability should not change. So you have to select BS best hypotheses\n",
    "    #            among all extension of unfinished hypotheses and finished hypotheses\n",
    "    #          As a result you will have a new token for best extensions of unfinished hypotheses\n",
    "    #          For simplisity you can use <EOS> token if you select finished hypothesis in the beam\n",
    "    #            i.e. tensor of shape [B * BS] of indices for selected hypotheses and\n",
    "    #                 tensor of shape [B * BS] of extension tokens for each hypothesis\n",
    "    #      4. Update probabilities for each hypotheses and is_finished state for each hypothesis\n",
    "    #          Concat new tokens to the existing prefixes\n",
    "    #      5. Update hidden and cell state to correspond to the selected hypothesis\n",
    "\n",
    "    eos_idx = model.vocab.get_stoi()['<eos>']\n",
    "    \n",
    "    # Get embeddings for the start tokens\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    # Make forward pass through the RNN and \n",
    "    #   obtain the last cell and hidden state for each element in the batch\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "\n",
    "    start_h = ... # [L, B, H]\n",
    "    start_c = ... # [L, B, H]\n",
    "\n",
    "    # Use those states as the initialization for each hypotheses in the beam\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "    h = ... # [L, B * BS, H]\n",
    "    c = ... # [L, B * BS, H]\n",
    "    \n",
    "    # Select initial tokens for each hypotheses in the beam\n",
    "    #   Compute log probabilities and select top-beam_size tokens for each element\n",
    "    #   Use them to initialize beam search state\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    new_tokens = ... # [B * BS]\n",
    "    log_probas = ... # [B * BS]\n",
    "    hypotesis = ... # [1, B * BS]\n",
    "    \n",
    "    is_finished = ... # [B * BS]\n",
    "    decoded_lens = ... # [B * BS]\n",
    "\n",
    "    while not torch.all(is_finished) and hypotesis.shape[0] < max_generated_len:\n",
    "        # Get probabilities for the next token for each hypothesis\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "\n",
    "        next_token_log_probas = ... # [B * BS, V]\n",
    "        \n",
    "        # Use those probabilities to compute probability for each extension of each hypothesis\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "        extension_log_probas = ... # [B * BS, V]\n",
    "\n",
    "        # For each element in the batch select new BS best hypotheses\n",
    "        #   You can use loop over different beams\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "\n",
    "        # Update probabilities for each hypotheses and is_finished state and decoded_lens for each hypothesis\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "\n",
    "        # Concat new tokens to the existing prefixes\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "        # Update hidden and cell state to correspond to the selected hypothesis\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "    return (\n",
    "        hypotesis.view(-1, start_tokens.shape[1], beam_size), \n",
    "        decoded_lens.view(start_tokens.shape[1], beam_size),\n",
    "        log_probas.view(start_tokens.shape[1], beam_size)\n",
    "    )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:46:29.834960Z",
     "start_time": "2024-03-30T22:46:29.824795Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start_tokens = torch.tensor([\n",
    "    lm_model.vocab.lookup_indices(['<sos>', '<pad>', '<pad>', '<pad>']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'my', 'favorite', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'best', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'worst', 'movie']),\n",
    "]).T\n",
    "\n",
    "start_tokens_lens = torch.tensor([1, 4, 4, 4])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:46:30.504262Z",
     "start_time": "2024-03-30T22:46:30.501331Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_model.to(device).eval()\n",
    "start_tokens = start_tokens.to(device)\n",
    "start_tokens_lens = start_tokens_lens.to(device)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:46:31.081687Z",
     "start_time": "2024-03-30T22:46:30.966597Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "beam_size = 100\n",
    "decoded_tokens, decoded_lens, log_probas = decode_beam_search(\n",
    "    lm_model, start_tokens, start_tokens_lens, max_generated_len=10, beam_size=beam_size\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:46:31.910807Z",
     "start_time": "2024-03-30T22:46:31.646253Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for start_tokens_elem, start_tokens_lens_elem, decoded_tokens_elem, decoded_lens_elem, log_probas_elem in zip(\n",
    "    start_tokens.T, start_tokens_lens,\n",
    "    decoded_tokens.permute(1, 2, 0), decoded_lens.permute(0, 1), log_probas.permute(0, 1)\n",
    "):\n",
    "    start_tokens_elem = start_tokens_elem[:start_tokens_lens_elem].tolist()\n",
    "    start_words = np.array(lm_model.vocab.get_itos())[np.array(start_tokens_elem)]\n",
    "    \n",
    "    start_text = ' '.join(start_words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{start_text}</b></div>'))\n",
    "    \n",
    "    for idx, (hyp, hyp_len, hyp_log_prob) in enumerate(zip(decoded_tokens_elem, decoded_lens_elem, log_probas_elem)):\n",
    "        if idx >= 3:\n",
    "            break\n",
    "            \n",
    "        hyp = hyp[:hyp_len].tolist()\n",
    "        hyp_words = np.array(lm_model.vocab.get_itos())[np.array(hyp)]\n",
    "        hyp_text = ' '.join(hyp_words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "        display(Markdown(\n",
    "            f'<div class=\"alert alert-block alert-success\"> <b>{hyp_log_prob:.3f}: {hyp_text}</b></div>'\n",
    "        ))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:46:32.320606Z",
     "start_time": "2024-03-30T22:46:32.169949Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуйте выполнить декодинг для разных `beam_size`. Убедитесь, что при `beam_search=1` семплирование совпадает с top-1 (greedy decoding) подходом. \n",
    "\n",
    "Сравните результаты Beam Search с top-k семплированием и жадным декодированием. Опишите ваши наблюдения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:00:59.682820Z",
     "start_time": "2024-03-30T22:00:59.682812Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Бонус. Существенное улучшение качества (до 6 баллов)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:04:51.678260Z",
     "start_time": "2021-04-02T15:04:51.673587Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Та модель, которая использовалась в предыдущей части во многом заимствует улучшения LSTM из статьи [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf). Вы можете попробовать применить другие варианты регуляризации из данной статьи для существенного улучшения качества LM.\n",
    "\n",
    "Например:\n",
    "1. Dropout для эмбеддингов **(+0.25)**\n",
    "2. Dropout входов и выходов RNN **(+0.25)**\n",
    "3. Регуляризация активаций (AR/TAR) **(+1.0)**\n",
    "4. NT-ASGD **(+1.5)**\n",
    "5. Tied веса эмбеддингов и софтмакса **(+1.0)**\n",
    "6. Attention **(+2.0)**\n",
    "\n",
    "**Полные баллы ставятся только при наличии качественного и количественного сравнения с бейзлайном.**\n",
    "\n",
    "**Для эксперимента с Attention необходимо изобразить Attention Maps для нескольких примеров.**"
   ],
   "metadata": {}
  }
 ]
}
