{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "804px",
    "left": "148px",
    "top": "50px",
    "width": "555.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "313px",
    "left": "926px",
    "right": "27px",
    "top": "120px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30683,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# `Практикум по программированию на языке Python`\n",
    "\n",
    "## `Задание 03. Рекуррентные Нейронные Сети. Dropout. LM`\n",
    "\n",
    "#### Фамилия, имя: Богачев Владимир\n",
    "\n",
    "Дата выдачи: <span style=\"color:red\">__30 марта 23:59__</span>.\n",
    "\n",
    "Мягкий дедлайн: <span style=\"color:red\">__13 апреля 23:59__</span>.\n",
    "\n",
    "Стоимость: __10 баллов__ (основная часть заданий) + __7 баллов__ (дополнительные задания).\n",
    "\n",
    "<span style=\"color:red\">__В ноутбуке все клетки должны выполняться без ошибок при последовательном их выполнении.__</span>\n",
    "\n",
    "#### `Москва, 2024`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Данное задание будет состоять из двух частей:\n",
    "1. Применение рекуррентной сети для решения задачи классификации текста. Более конкретно -- предсказания рейтинга отзыва фильма.\n",
    "2. Простейшая лингвистическая модель для генерации текста на основе LSTM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "При выполнении задания вы обучите LSTM с разным уровнем \"коробочности\", а также познакомитесь с различными способами применения DropOut к рекуррентным архитектурам. В рекуррентных архитектурах вариантов, куда можно наложить бинарную маску шума, гораздо больше, чем в нейросетях прямого прохода.\n",
    "\n",
    "Во второй части вы попробуете реализовать простейший рекуррентный декодер для генерации текстов.\n",
    "\n",
    "Задание сделано так, чтобы его можно было выполнять на CPU, однако RNN - это ресурсоёмкая вещь, поэтому на GPU с ними работать приятнее. Можете попробовать использовать [https://colab.research.google.com](https://colab.research.google.com) - бесплатное облако с GPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для корректного отображения картинок, вам может понадобится сделать ноутбук доверенным (Trusted) в правом верхнем углу**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 0. Загрузка и предобработка данных (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Рекомендуемые гиперпараметры`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "max_length = 200\n",
    "top_n_words = 5000\n",
    "\n",
    "hidden_dim = 128\n",
    "embedding_dim = 32\n",
    "\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:22:46.468794Z",
     "iopub.execute_input": "2024-04-11T17:22:46.469179Z",
     "iopub.status.idle": "2024-04-11T17:22:46.476062Z",
     "shell.execute_reply.started": "2024-04-11T17:22:46.469149Z",
     "shell.execute_reply": "2024-04-11T17:22:46.474329Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:54:32.808717600Z",
     "start_time": "2024-04-15T18:54:32.770724Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первое, что нужно сделать — скачать, предобработать данные и организовать их таким образом, чтобы их можно было подавать в нейронную сеть.\n",
    "\n",
    "Для обеих частей задания мы будем использовать [**Large Movie Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Загрузка и предобработка данных`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузите данные по ссылке выше. (**tip**: используйте `wget`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:19.592935Z",
     "iopub.execute_input": "2024-04-11T17:25:19.593392Z",
     "iopub.status.idle": "2024-04-11T17:25:38.191124Z",
     "shell.execute_reply.started": "2024-04-11T17:25:19.593355Z",
     "shell.execute_reply": "2024-04-11T17:25:38.189556Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:54:32.878717600Z",
     "start_time": "2024-04-15T18:54:32.774843500Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Распакуйте скачанные данные в папку `aclImdb` (**tip:** используйте `tar`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!tar -xzf aclImdb_v1.tar.gz"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:38.194362Z",
     "iopub.execute_input": "2024-04-11T17:25:38.194831Z",
     "iopub.status.idle": "2024-04-11T17:25:49.331609Z",
     "shell.execute_reply.started": "2024-04-11T17:25:38.194788Z",
     "shell.execute_reply": "2024-04-11T17:25:49.330257Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:20.540608400Z",
     "start_time": "2024-04-15T18:54:32.792717600Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрите в файле `./aclImdb/README` как организованы данные:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!cat ./aclImdb/train/pos/10003_8.txt"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:49.333046Z",
     "iopub.execute_input": "2024-04-11T17:25:49.333428Z",
     "iopub.status.idle": "2024-04-11T17:25:50.441878Z",
     "shell.execute_reply.started": "2024-04-11T17:25:49.333392Z",
     "shell.execute_reply": "2024-04-11T17:25:50.440493Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:20.562558100Z",
     "start_time": "2024-04-15T18:55:20.542608200Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_data_path = './aclImdb/test/'\n",
    "train_data_path = './aclImdb/train/'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:50.446497Z",
     "iopub.execute_input": "2024-04-11T17:25:50.447293Z",
     "iopub.status.idle": "2024-04-11T17:25:50.452552Z",
     "shell.execute_reply.started": "2024-04-11T17:25:50.447247Z",
     "shell.execute_reply": "2024-04-11T17:25:50.451165Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:20.619063800Z",
     "start_time": "2024-04-15T18:55:20.562037400Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:50.454104Z",
     "iopub.execute_input": "2024-04-11T17:25:50.455185Z",
     "iopub.status.idle": "2024-04-11T17:25:57.205948Z",
     "shell.execute_reply.started": "2024-04-11T17:25:50.455150Z",
     "shell.execute_reply": "2024-04-11T17:25:57.204408Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.301159400Z",
     "start_time": "2024-04-15T18:55:20.563552400Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060]\n",
      "[nltk_data]     Попытка установить соединение была безуспешной, т.к.\n",
      "[nltk_data]     от другого компьютера за требуемое время не получен\n",
      "[nltk_data]     нужный отклик, или было разорвано уже установленное\n",
      "[nltk_data]     соединение из-за неверного отклика уже подключенного\n",
      "[nltk_data]     компьютера>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Стандартной предобработкой данных является токенизация текстов. Полученные токены можно будет закодировать и затем подавать на вход нейронной сети. Ключевым моментом, который влияет на скорость работы нейросети и её размер в памяти — размер словаря, используемого при токенизации. Для задачи классификации мы можем убрать часть слов (стоп слова, редкие слова), ускорив обучение без потери в качестве."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:57.207709Z",
     "iopub.execute_input": "2024-04-11T17:25:57.208275Z",
     "iopub.status.idle": "2024-04-11T17:25:57.223033Z",
     "shell.execute_reply.started": "2024-04-11T17:25:57.208243Z",
     "shell.execute_reply": "2024-04-11T17:25:57.221565Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.382137100Z",
     "start_time": "2024-04-15T18:55:42.261671100Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:57.225050Z",
     "iopub.execute_input": "2024-04-11T17:25:57.225517Z",
     "iopub.status.idle": "2024-04-11T17:25:58.312143Z",
     "shell.execute_reply.started": "2024-04-11T17:25:57.225470Z",
     "shell.execute_reply": "2024-04-11T17:25:58.311031Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.386136300Z",
     "start_time": "2024-04-15T18:55:42.286644600Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуйте функцию для токенизации текста. Выполнять токенизацию можно по-разному, но в данном задании предлагается это делать следующим образом:\n",
    "1. Привести текст к нижнему регистру\n",
    "2. Убрать html разметку из текстов (`<br />`, ...)\n",
    "3. Убрать все символы кроме латинских букв\n",
    "4. Разбить строку по пробелам\n",
    "5. Убрать стоп слова"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.313717Z",
     "iopub.execute_input": "2024-04-11T17:25:58.314345Z",
     "iopub.status.idle": "2024-04-11T17:25:58.696667Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.314303Z",
     "shell.execute_reply": "2024-04-11T17:25:58.695352Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.432138500Z",
     "start_time": "2024-04-15T18:55:42.292157800Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param str text: Input text \n",
    "    :return List[str]: List of words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    text = text.translate(translator)\n",
    "    text = word_tokenize(text, language='english')\n",
    "    text = list(filter(lambda w: w not in STOPWORDS, text))\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.698220Z",
     "iopub.execute_input": "2024-04-11T17:25:58.698862Z",
     "iopub.status.idle": "2024-04-11T17:25:58.708538Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.698824Z",
     "shell.execute_reply": "2024-04-11T17:25:58.706709Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.435151200Z",
     "start_time": "2024-04-15T18:55:42.338403800Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenize('1. Hello <br /> words!! <br />')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.714426Z",
     "iopub.execute_input": "2024-04-11T17:25:58.715170Z",
     "iopub.status.idle": "2024-04-11T17:25:58.748140Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.715128Z",
     "shell.execute_reply": "2024-04-11T17:25:58.746737Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.450152100Z",
     "start_time": "2024-04-15T18:55:42.340579700Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['hello', 'words']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь мы можем создать словарь, с помощью которого мы будем численно кодировать токены из текста и наоборот.\n",
    "\n",
    "Удобной обёрткой для создания словарей является класс `torchtext.vocab.Vocab` и фабрика для создания таких классов `torchtext.vocab.vocab`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torchtext.vocab.vocab??"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.749880Z",
     "iopub.execute_input": "2024-04-11T17:25:58.750793Z",
     "iopub.status.idle": "2024-04-11T17:25:58.849637Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.750745Z",
     "shell.execute_reply": "2024-04-11T17:25:58.847692Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:55:42.570669800Z",
     "start_time": "2024-04-15T18:55:42.348621400Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы создать такой словарь, сначала нужно создать словарь со всеми токенами в тексте и их частотами встречаемости:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:51:55.300753Z",
     "start_time": "2021-04-01T19:51:55.275188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "counter = defaultdict(int)\n",
    "\n",
    "for path in ['./aclImdb/test/neg', './aclImdb/test/pos', './aclImdb/train/neg', './aclImdb/train/pos']:\n",
    "    for file_path in os.listdir(path):\n",
    "        text = open(os.path.join(path, file_path), 'r', encoding='utf-8', errors='ignore').read().strip()\n",
    "        for token in tokenize(text):\n",
    "            counter[token] += 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:25:58.852596Z",
     "iopub.execute_input": "2024-04-11T17:25:58.853845Z",
     "iopub.status.idle": "2024-04-11T17:28:46.997932Z",
     "shell.execute_reply.started": "2024-04-11T17:25:58.853790Z",
     "shell.execute_reply": "2024-04-11T17:28:46.996330Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.239438200Z",
     "start_time": "2024-04-15T18:55:42.380136300Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir\\AppData\\Local\\Temp\\ipykernel_12668\\3422549362.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text).get_text()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для работы с текстами нам необходимо зарезервировать два специальных токена:\n",
    "1. `<pad>` для токена означающего паддинг\n",
    "2. `<unk>` для токенов, которые отсутствуют в словаре"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# specials = ['<pad>', '<unk>']\n",
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for special in specials:\n",
    "    counter[special] = 0"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:46.999768Z",
     "iopub.execute_input": "2024-04-11T17:28:47.000149Z",
     "iopub.status.idle": "2024-04-11T17:28:47.005985Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.000119Z",
     "shell.execute_reply": "2024-04-11T17:28:47.004988Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.249047600Z",
     "start_time": "2024-04-15T18:56:27.239438200Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте словарь из словаря частот `counter`. Наименьшие *id* отдайте под специальные токены. \n",
    "\n",
    "Отбросьте низкочастотные слова, оставив только `top_n_words` слов. Можете использовать любой способ реализации этого условия, например:\n",
    "1. Оставить в словаре `counter` нужное число слов\n",
    "2. Подобрать параметр `min_freq`, чтобы оставшееся число слов было близко к необходимому порогу"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torchtext.__version__"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.007190Z",
     "iopub.execute_input": "2024-04-11T17:28:47.008227Z",
     "iopub.status.idle": "2024-04-11T17:28:47.028116Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.008192Z",
     "shell.execute_reply": "2024-04-11T17:28:47.026746Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.261046100Z",
     "start_time": "2024-04-15T18:56:27.242694400Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.16.0+cpu'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab = torchtext.vocab.vocab(\n",
    "    counter,\n",
    "    min_freq=145,\n",
    "    specials=specials,\n",
    ")\n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.029902Z",
     "iopub.execute_input": "2024-04-11T17:28:47.030270Z",
     "iopub.status.idle": "2024-04-11T17:28:47.199189Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.030230Z",
     "shell.execute_reply": "2024-04-11T17:28:47.197799Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.347669300Z",
     "start_time": "2024-04-15T18:56:27.247047100Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab, len(vocab)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.200582Z",
     "iopub.execute_input": "2024-04-11T17:28:47.201012Z",
     "iopub.status.idle": "2024-04-11T17:28:47.208352Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.200984Z",
     "shell.execute_reply": "2024-04-11T17:28:47.206947Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.368671300Z",
     "start_time": "2024-04-15T18:56:27.282679900Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(Vocab(), 5037)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab.lookup_indices(['<pad>', '<unk>'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.210021Z",
     "iopub.execute_input": "2024-04-11T17:28:47.210648Z",
     "iopub.status.idle": "2024-04-11T17:28:47.223755Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.210601Z",
     "shell.execute_reply": "2024-04-11T17:28:47.222380Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.372669800Z",
     "start_time": "2024-04-15T18:56:27.286902700Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab.lookup_indices(['this', 'film', 'was', 'awful'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.225104Z",
     "iopub.execute_input": "2024-04-11T17:28:47.225510Z",
     "iopub.status.idle": "2024-04-11T17:28:47.238847Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.225476Z",
     "shell.execute_reply": "2024-04-11T17:28:47.237637Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.389669900Z",
     "start_time": "2024-04-15T18:56:27.289902600Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 98, 1, 422]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь мы готовы создать обёртку-датасет для наших данных. \n",
    "\n",
    "Необходимо добавить несколько опции, которые понадобятся во второй части задания:\n",
    "1. Ограничение на максимальную длину текста в токенах. Если текст оказывается длиннее, то последние токены отбрасываются\n",
    "2. Возможность добавить в специальные токены `<sos>`, `<eos>` в начало и конец токенизированного текста\n",
    "    \n",
    "**tips:**\n",
    "1. Обратите особое внимание, что у длинных текстов не должен обрезаться паддинг\n",
    "2. В исходных данных рейтинг закодирован в названии файла в виде числа от $1$ до $10$. Для удобства, вычтите $1$, чтобы рейтинг был от $0$ до $9$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import re"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.241391Z",
     "iopub.execute_input": "2024-04-11T17:28:47.242132Z",
     "iopub.status.idle": "2024-04-11T17:28:47.253250Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.242077Z",
     "shell.execute_reply": "2024-04-11T17:28:47.251777Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.393669600Z",
     "start_time": "2024-04-15T18:56:27.293663300Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "re_rating = re.compile('_[0-9]*')\n",
    "re.search(re_rating, './aclImdb/train/pos/10003_10.txt').group(0)[1:]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.255370Z",
     "iopub.execute_input": "2024-04-11T17:28:47.256186Z",
     "iopub.status.idle": "2024-04-11T17:28:47.271859Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.256139Z",
     "shell.execute_reply": "2024-04-11T17:28:47.270616Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.405670700Z",
     "start_time": "2024-04-15T18:56:27.296475100Z"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'10'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_raiting(path: str, re_rating = None):\n",
    "    re_rating = '_[0-9]*' if re_rating is None else re_rating\n",
    "    \n",
    "    rating = re.search(re_rating, path).group(0)[1:]\n",
    "    return int(rating) - 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.273661Z",
     "iopub.execute_input": "2024-04-11T17:28:47.274645Z",
     "iopub.status.idle": "2024-04-11T17:28:47.284077Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.274598Z",
     "shell.execute_reply": "2024-04-11T17:28:47.282140Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.408669600Z",
     "start_time": "2024-04-15T18:56:27.301888600Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class LargeMovieReviewDataset(Dataset):\n",
    "    def __init__(self, data_path, vocab, max_len, pad_sos=False, pad_eos=False):\n",
    "        \"\"\"\n",
    "        :param str data_path: Path to folder with one of the data splits (train or test)\n",
    "        :param torchtext.vocab.Vocab vocab: dictionary with lookup_indices method\n",
    "        :param int max_len: Maximum length of tokenized text\n",
    "        :param bool pad_sos: If True pad sequence at the beginning with <sos> \n",
    "        :param bool pad_eos: If True pad sequence at the end with <eos>         \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pad_sos = pad_sos\n",
    "        if self.pad_sos:\n",
    "            self.sos_id = vocab.lookup_indices(['<sos>'])[0]\n",
    "        self.pad_eos = pad_eos\n",
    "        if self.pad_eos:\n",
    "            self.eos_id = vocab.lookup_indices(['<eos>'])[0]\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.data_path = data_path\n",
    "        self.negative_path = os.path.join(data_path, 'neg')\n",
    "        self.positive_path = os.path.join(data_path, 'pos')\n",
    "        \n",
    "        self.negative_paths = []\n",
    "        self.positive_paths = []\n",
    "\n",
    "        for file_path in os.listdir(self.negative_path):\n",
    "            self.negative_paths.append(os.path.join(self.negative_path, file_path))\n",
    "\n",
    "        for file_path in os.listdir(self.positive_path):\n",
    "            self.positive_paths.append(os.path.join(self.positive_path, file_path))\n",
    "        \n",
    "        self.texts = []\n",
    "        self.tokens = []\n",
    "        self.ratings = []\n",
    "        self.labels = [0] * len(self.negative_paths) + [1] * len(self.positive_paths)\n",
    "        \n",
    "        # Read each file in data_path, tokenize it, get tokens ids, its rating and store\n",
    "        re_rating = re.compile('_[0-9]*')\n",
    "        for path in self.negative_paths + self.positive_paths:\n",
    "            # YOUR CODE HERE\n",
    "            with open(path, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read().strip()\n",
    "            self.texts.append(text)\n",
    "            \n",
    "            txt_tokens = vocab.lookup_indices(tokenize(text))[0:self.max_len]\n",
    "            if self.pad_sos:\n",
    "                txt_tokens.insert(0, vocab['<sos>'])\n",
    "            if self.pad_eos:\n",
    "                txt_tokens.append(vocab['<eos>'])\n",
    "            \n",
    "            self.tokens.append(txt_tokens)\n",
    "            self.ratings.append(get_raiting(path, re_rating=re_rating))\n",
    "        \n",
    "        self.ratings = torch.LongTensor(self.ratings)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        self.tokens = [torch.LongTensor(tls) for tls in self.tokens]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :param int idx: index of object in dataset\n",
    "        :return dict: Dictionary with all useful object data \n",
    "            {\n",
    "                'text' str: unprocessed text,\n",
    "                'label' torch.Tensor(dtype=torch.long): sentiment of the text (0 for negative, 1 for positive)\n",
    "                'rating' torch.Tensor(dtype=torch.long): rating of the text\n",
    "                'tokens' torch.Tensor(dtype=torch.long): tensor of tokens ids for the text\n",
    "                'tokens_len' torch.Tensor(dtype=torch.long): number of tokens\n",
    "            }\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        res = {\n",
    "            'text': self.texts[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'rating': self.ratings[idx],\n",
    "            'tokens': self.tokens[idx],\n",
    "            'tokens_len': self.tokens[idx].shape[0]\n",
    "        }\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return int: number of objects in dataset \n",
    "        \"\"\"\n",
    "        return len(self.tokens)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:28:47.285804Z",
     "iopub.execute_input": "2024-04-11T17:28:47.286395Z",
     "iopub.status.idle": "2024-04-11T17:28:47.306007Z",
     "shell.execute_reply.started": "2024-04-11T17:28:47.286361Z",
     "shell.execute_reply": "2024-04-11T17:28:47.305049Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.412669600Z",
     "start_time": "2024-04-15T18:56:27.308889700Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте датасеты для тестовой и обучающей выборки. \n",
    "\n",
    "Обратите внимание, что для задачи классификации нам не потребуется паддинг с помощью `<sos>`, `<eos>`. \n",
    "\n",
    "Не забудьте обрезать длинные тексты, передав параметр `max_length`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pathlib"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:27.415670Z",
     "start_time": "2024-04-15T18:56:27.313108300Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "datasets_dump_path = pathlib.Path(\"./datasets_dump.bin\")\n",
    "\n",
    "if not datasets_dump_path.exists():\n",
    "    test_dataset = LargeMovieReviewDataset(test_data_path, vocab, max_len=max_length)\n",
    "    train_dataset = LargeMovieReviewDataset(train_data_path, vocab, max_len=max_length)\n",
    "    \n",
    "    with open(datasets_dump_path, \"wb\") as f:\n",
    "        torch.save(\n",
    "            obj=(test_dataset, train_dataset),\n",
    "            f=f\n",
    "        )\n",
    "else:\n",
    "    with open(datasets_dump_path, \"rb\") as f:\n",
    "        test_dataset, train_dataset = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:56:28.623920100Z",
     "start_time": "2024-04-15T18:56:27.315671900Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",\n 'label': tensor(0),\n 'rating': tensor(1),\n 'tokens': tensor([ 4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  1,\n         20, 21, 22, 23, 24, 25, 26, 15, 20, 27, 15,  1,  1,  1,  1, 28, 29, 30,\n         31, 32, 33, 34, 35, 36, 37, 38,  1, 19,  1, 39, 40,  1, 41, 42, 43, 44,\n         45, 46, 47,  1, 48, 17,  1, 18, 49,  1, 50, 51, 52,  1,  1, 53, 54, 55,\n         56, 57]),\n 'tokens_len': 74}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:24.848134700Z",
     "start_time": "2024-04-15T18:57:24.792770200Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим, как выглядит объект в датасете:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "for d in train_dataset:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\"\n",
    "    \n",
    "for d in test_dataset:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\""
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:33.858064Z",
     "iopub.execute_input": "2024-04-11T17:31:33.858477Z",
     "iopub.status.idle": "2024-04-11T17:31:34.788422Z",
     "shell.execute_reply.started": "2024-04-11T17:31:33.858426Z",
     "shell.execute_reply": "2024-04-11T17:31:34.787016Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:29.659055900Z",
     "start_time": "2024-04-15T18:57:29.092978500Z"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset[-2]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.808174Z",
     "iopub.execute_input": "2024-04-11T17:31:34.808645Z",
     "iopub.status.idle": "2024-04-11T17:31:34.828293Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.808610Z",
     "shell.execute_reply": "2024-04-11T17:31:34.826392Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:30.263105900Z",
     "start_time": "2024-04-15T18:57:30.181483400Z"
    }
   },
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"This movie, with all its complexity and subtlety, makes for one of the most thought-provoking short films I have ever seen. The topics it addresses are ugly, cynical, and at times, even macabre, but the film remains beautiful in its language, artful with its camera angles, and gorgeous in its style, skillfully recreating the short story of the same name written by a master of short stories, Tobias Wolff.<br /><br />Not wishing to spoil anything of the movie, I won't go into any details, other than to say that this movie is magnificent in and of itself. It takes pride in what it does, and does it well. It shows the most important memories of life, all of which can be topped by the single most elusive feeling: unexpected bliss. This movie, of its own volition, has created in me the same feelings the main character (Tom Noonan) felt when words transformed his very existence, and that is one impressive feat.\",\n 'label': tensor(1),\n 'rating': tensor(9),\n 'tokens': tensor([   6, 3680, 3314,   97,   79, 4861,  603,   61,  419,  111,    1,    1,\n          188, 1967,   74,  198,    1,   98,  760, 1733, 2546,    1,  128, 1951,\n         2747, 1365,    1,    1,  603,  301, 1457, 1533, 2836,  603,  819,    1,\n            1,  361, 1227,  665,    6,  252,   80, 2067,  542,    6, 4399, 1115,\n         3588,   42,   37, 1880, 3975, 1012,    1,  153,    1,  258, 4448,    1,\n            6,    1, 2124,  975,  232,   20,  950,    1,  578, 2419,    1, 2329,\n           79, 2513,    1]),\n 'tokens_len': 75}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь нам нужно создать `DataLoader` для наших данных. `DataLoader` умеет из коробки объединять список объектов из датасета в один батч, даже когда датасет возвращает словарь тензоров. Однако, это работает только в случае когда все эти тензоры имеют один и тот же размер во всех батчах. В нашем случае, это не так, так как разные тексты могут иметь разную длину.\n",
    "\n",
    "Чтобы обойти эту проблему у `DataLoader` есть параметр `collate_fn`, который позволяет задать функцию для объединения списка объектов в один батч."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы объединить несколько тензоров разной длины в один можно использовать функцию `torch.nn.utils.rnn.pad_sequence`\n",
    "\n",
    "Обратите внимание на её аргументы:\n",
    "1. `batch_first` определяет по какой оси \"складывать\" тензоры. Предпочтительнее использовать `batch_first=False` так как это может упростить выполнение задания в дальнейшем \n",
    "2. `padding_value` — число, которое будет использоваться в качестве паддинга, чтобы сделать все тензоры одинаковой длины"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.nn.utils.rnn.pad_sequence([\n",
    "    torch.tensor([1, 2, 3]),\n",
    "    torch.tensor([4, 5]),\n",
    "    torch.tensor([6, 7, 8, 9])\n",
    "], batch_first=False, padding_value=-1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.829776Z",
     "iopub.execute_input": "2024-04-11T17:31:34.830242Z",
     "iopub.status.idle": "2024-04-11T17:31:34.871668Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.830197Z",
     "shell.execute_reply": "2024-04-11T17:31:34.869604Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:31.817537400Z",
     "start_time": "2024-04-15T18:57:31.761618600Z"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  4,  6],\n        [ 2,  5,  7],\n        [ 3, -1,  8],\n        [-1, -1,  9]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch, padding_value, batch_first=False):\n",
    "    \"\"\"\n",
    "    :param List[Dict] batch: List of objects from dataset\n",
    "    :param int padding_value: Value that will be used to pad tokens\n",
    "    :param bool batch_first: If True resulting tensor with tokens must have shape [B, T] otherwise [T, B]\n",
    "    :return dict: Dictionary with all data collated\n",
    "        {\n",
    "            'ratings' torch.Tensor(dtype=torch.long): rating of the text for each object in batch\n",
    "            'labels' torch.Tensor(dtype=torch.long): sentiment of the text for each object in batch\n",
    "            \n",
    "            'texts' List[str]: All texts in one list\n",
    "            'tokens' torch.Tensor(dtype=torch.long): tensor of tokens ids padded with @padding_value\n",
    "            'tokens_lens' torch.Tensor(dtype=torch.long): number of tokens for each object in batch\n",
    "        }\n",
    "    \"\"\"\n",
    "    ratings = torch.LongTensor(size=(len(batch), ))\n",
    "    labels = torch.LongTensor(size=(len(batch), ))\n",
    "    texts = []\n",
    "    tokens = []\n",
    "    tokens_lens = torch.LongTensor(size=(len(batch), ))\n",
    "    \n",
    "    for i, batch_dir in enumerate(batch):\n",
    "        ratings[i] = batch_dir['rating']\n",
    "        labels[i] = batch_dir['label']\n",
    "        texts.append(batch_dir['text'])\n",
    "        tokens.append(batch_dir['tokens'])\n",
    "        tokens_lens[i] = batch_dir['tokens_len']\n",
    "    \n",
    "    tokens = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=batch_first, padding_value=padding_value)\n",
    "    \n",
    "    res = {\n",
    "            'texts': texts,\n",
    "            'labels': labels,\n",
    "            'ratings': ratings,\n",
    "            'tokens': tokens,\n",
    "            'tokens_lens': tokens_lens,\n",
    "        }\n",
    "    \n",
    "    return res"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.873549Z",
     "iopub.execute_input": "2024-04-11T17:31:34.873934Z",
     "iopub.status.idle": "2024-04-11T17:31:34.895676Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.873901Z",
     "shell.execute_reply": "2024-04-11T17:31:34.891939Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:32.538728900Z",
     "start_time": "2024-04-15T18:57:32.481744400Z"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создайте даталоадеры с использованием `collate_fn`.\n",
    "\n",
    "**tips**:\n",
    "1. Передать в `collate_fn` правильное значение паддинга можно, например, с помощью `functools.partial`\n",
    "2. Если вы работаете в Google Colab, то, возможно, вам будет необходимо установить `num_workers=0` во избежание падения ноутбука."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import functools"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.905939Z",
     "iopub.execute_input": "2024-04-11T17:31:34.907603Z",
     "iopub.status.idle": "2024-04-11T17:31:34.913490Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.907545Z",
     "shell.execute_reply": "2024-04-11T17:31:34.911716Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:33.534281200Z",
     "start_time": "2024-04-15T18:57:33.493291Z"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "collate_fn_ = functools.partial(collate_fn, padding_value=vocab['<pad>'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0, collate_fn=collate_fn_)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.917306Z",
     "iopub.execute_input": "2024-04-11T17:31:34.918343Z",
     "iopub.status.idle": "2024-04-11T17:31:34.932951Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.918275Z",
     "shell.execute_reply": "2024-04-11T17:31:34.931536Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:34.049316500Z",
     "start_time": "2024-04-15T18:57:34.019318900Z"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for d in train_dataloader:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\"\n",
    "\n",
    "for d in test_dataloader:\n",
    "    # print(type(d['tokens']))\n",
    "    # break\n",
    "    assert not torch.any(torch.isnan(d['tokens'])), f\"tokens contains NaNs: \\n{d=}\""
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:34.935437Z",
     "iopub.execute_input": "2024-04-11T17:31:34.935969Z",
     "iopub.status.idle": "2024-04-11T17:31:36.315937Z",
     "shell.execute_reply.started": "2024-04-11T17:31:34.935933Z",
     "shell.execute_reply": "2024-04-11T17:31:36.314514Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:35.557648400Z",
     "start_time": "2024-04-15T18:57:34.584846Z"
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим на какой-нибудь батч:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "batch.keys(), batch['labels'], batch['ratings'], batch['tokens'], batch['tokens_lens']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.317864Z",
     "iopub.execute_input": "2024-04-11T17:31:36.318299Z",
     "iopub.status.idle": "2024-04-11T17:31:36.345228Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.318263Z",
     "shell.execute_reply": "2024-04-11T17:31:36.343273Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:36.365314Z",
     "start_time": "2024-04-15T18:57:36.282315800Z"
    }
   },
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(dict_keys(['texts', 'labels', 'ratings', 'tokens', 'tokens_lens']),\n tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n tensor([1, 3, 0, 2, 2, 1, 1, 1, 3, 3, 2, 2, 1, 0, 0, 3, 1, 3, 2, 0, 0, 0, 3, 0,\n         2, 3, 3, 2, 1, 2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 3, 0, 3, 1, 0, 0,\n         0, 1, 0, 3, 0, 0, 0, 3, 3, 1, 3, 3, 2, 0, 0, 0]),\n tensor([[   4,   58,  136,  ..., 1984,  514,    1],\n         [   1,   59,  137,  ...,  284,    1,  274],\n         [   5,   60,  138,  ...,  349,    1, 2002],\n         ...,\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0]]),\n tensor([ 74, 128, 108, 168, 137,  52,  74,  74,  72,  98,  59, 143, 134,  52,\n         104, 112,  67, 116, 189,  47,  36,  96, 200, 200, 136, 111, 105, 200,\n         200, 144,  75,  82, 184,  99, 156, 132, 131,  56, 182, 106,  67,  61,\n          86, 200, 113,  66, 200,  55, 115,  77,  56, 145, 130,  29,  64,  60,\n         200,  16, 151,  87,  64,  71,  83,  87]))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 1. Классификация текстов (4 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Сборка и обучение RNN в pytorch (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим переменные для device-agnostic кода:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dtype, device, cuda_device_id = torch.float32, None, 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{0}'.format(str(cuda_device_id) if cuda_device_id is not None else '')\n",
    "if cuda_device_id is not None and torch.cuda.is_available():\n",
    "    device = 'cuda:{0:d}'.format(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}, dtype: {dtype}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.346749Z",
     "iopub.execute_input": "2024-04-11T17:31:36.347147Z",
     "iopub.status.idle": "2024-04-11T17:31:36.356022Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.347114Z",
     "shell.execute_reply": "2024-04-11T17:31:36.354771Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:39.472501800Z",
     "start_time": "2024-04-15T18:57:39.407469Z"
    }
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0, dtype: torch.float32\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Наша нейросеть будет обрабатывать входную последовательность по словам (word level). Мы будем использовать простую и стандартную рекуррентную архитектуру для классификации:\n",
    "1. Слой представлений, превращающий id токена в вектор-эмбеддинг этого слова\n",
    "2. Слой LSTM\n",
    "3. Полносвязный слой, предсказывающий выход по последнему скрытому состоянию\n",
    "\n",
    "Ниже дан код для сборки и обучения нашей нейросети."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите класс-обёртку над LSTM для задачи классификации. \n",
    "**Не используйте циклы.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:59:16.467178Z",
     "start_time": "2021-04-01T20:59:16.441112Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.357634Z",
     "iopub.execute_input": "2024-04-11T17:31:36.358609Z",
     "iopub.status.idle": "2024-04-11T17:31:36.371926Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.358572Z",
     "shell.execute_reply": "2024-04-11T17:31:36.370373Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:41.303459Z",
     "start_time": "2024-04-15T18:57:41.241463500Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "emb_dump = None\n",
    "inp_dump = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:41.779950100Z",
     "start_time": "2024-04-15T18:57:41.762437200Z"
    }
   },
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim, hidden_dim, output_size, vocab,\n",
    "        rec_layer=torch.nn.LSTM, dropout=None, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Create a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        #    Use torch.nn.Embedding. Do not forget specify padding_idx!\n",
    "        # YOUR CODE HERE\n",
    "        self.word_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=len(vocab),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab['<pad>'],\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        if dropout is not None:\n",
    "            self.rnn = rec_layer(\n",
    "                input_size=embedding_dim, \n",
    "                hidden_size=hidden_dim, \n",
    "                device=device,\n",
    "                dropout=dropout,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = rec_layer(\n",
    "                input_size=embedding_dim, \n",
    "                hidden_size=hidden_dim, \n",
    "                device=device,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        # Create linear layer for classification\n",
    "        # YOUR CODE HERE\n",
    "        self.output = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_size),\n",
    "        )\n",
    "        # self.output = nn.Sequential(\n",
    "        #     nn.Linear(hidden_dim, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm1d(64),\n",
    "        #     nn.Linear(64, output_size),\n",
    "        # )\n",
    "    \n",
    "    def forward(self, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor(dtype=torch.long) tokens: Batch of texts represented with tokens.\n",
    "        :param torch.Tensor(dtype=torch.long) tokens_lens: Number of non-padding tokens for each object in batch.\n",
    "        :return torch.Tensor(dtype=torch.long): Vector representation for each sequence in batch\n",
    "        \"\"\"\n",
    "        # Evaluate embeddings\n",
    "        # DEBUG: store last input in globals\n",
    "        global inp_dump\n",
    "        global emb_dump\n",
    "        \n",
    "        # DEBUG: store last input in globals\n",
    "        inp_dump = tokens.detach()\n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Tokens has NaNs\"\n",
    "        \n",
    "        x = self.word_embeddings(tokens)\n",
    "        # DEBUG: store last input in globals\n",
    "        emb_dump = x.detach()\n",
    "        \n",
    "        assert not torch.any(torch.isnan(x)), f\"Embeddings has NaNs\"\n",
    "        \n",
    "        # Make forward pass through recurrent network\n",
    "        # YOUR CODE HERE\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Pass output from rnn to linear layer \n",
    "        # Note: each object in batch has its own length \n",
    "        #     so we must take rnn hidden state after the last token for each text in batch        \n",
    "        x = x[tokens_lens - 1, torch.arange(0, x.shape[1]), :]\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.373584Z",
     "iopub.execute_input": "2024-04-11T17:31:36.374996Z",
     "iopub.status.idle": "2024-04-11T17:31:36.393418Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.374920Z",
     "shell.execute_reply": "2024-04-11T17:31:36.392097Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:42.261618900Z",
     "start_time": "2024-04-15T18:57:42.205620400Z"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Исходный код LSTM](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите функции для обучения и оценки модели:\n",
    "\n",
    "**tip:**\n",
    "1. В функции `evaluate` при подсчёте метрик учитывайте, что батчи могут иметь разный размер. (в частности последний батч)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:36.394775Z",
     "iopub.execute_input": "2024-04-11T17:31:36.395929Z",
     "iopub.status.idle": "2024-04-11T17:31:37.839947Z",
     "shell.execute_reply.started": "2024-04-11T17:31:36.395887Z",
     "shell.execute_reply": "2024-04-11T17:31:37.837984Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:44.711806300Z",
     "start_time": "2024-04-15T18:57:43.870900600Z"
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def set_global_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "#     torch.use_deterministic_algorithms(True) # если нужно гарантировать 1000% воспроизводимость\n",
    "\n",
    "    # Для Dataloader\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Для каждого woerker в Daaloader\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:31:37.842763Z",
     "iopub.execute_input": "2024-04-11T17:31:37.843189Z",
     "iopub.status.idle": "2024-04-11T17:31:37.852490Z",
     "shell.execute_reply.started": "2024-04-11T17:31:37.843154Z",
     "shell.execute_reply": "2024-04-11T17:31:37.850826Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:44.721698900Z",
     "start_time": "2024-04-15T18:57:44.714182400Z"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Make optimizer step\n",
    "        \n",
    "        # labels = data['labels'].to(device)\n",
    "        ratings = data['ratings'].to(device)\n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Eval epoch {idx} has NaNs\"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    for idx, data in enumerate(dataloader):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Evaluate accuracy\n",
    "        \n",
    "        # labels = data['labels'].to(device)\n",
    "        ratings = data['ratings'].to(device)\n",
    "        tokens = data['tokens'].to(device)\n",
    "        tokens_lens = data['tokens_lens'].to(device)\n",
    "        \n",
    "        assert not torch.any(torch.isnan(tokens)), f\"Eval epoch {idx} has NaNs\"\n",
    "        \n",
    "        logits = model(tokens, tokens_lens)\n",
    "        loss = loss_fn(logits, ratings)\n",
    "        total_loss += loss\n",
    "        \n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        total_accuracy += torch.sum(pred == ratings).item()\n",
    "        \n",
    "    return total_loss / len(dataloader.dataset), total_accuracy / len(dataloader.dataset)\n",
    "    \n",
    "\n",
    "def train(\n",
    "    train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs, name,\n",
    "):\n",
    "    wandb.init(project=\"MMP_prac_rnn_deb\", name=name)\n",
    "    \n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    rng = tqdm(range(num_epochs))\n",
    "    \n",
    "    for epoch in rng:\n",
    "        train_epoch(train_loader, model, loss_fn, optimizer, device)\n",
    "        \n",
    "        train_loss, train_acc = evaluate(train_loader, model, loss_fn, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss, test_acc = evaluate(test_loader, model, loss_fn, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        wandb.log({\"eval/loss\": test_loss, \"eval/accuracy\": test_acc}, step=epoch)\n",
    "        wandb.log({\"train/loss\": train_loss, \"train/accuracy\": train_acc}, step=epoch)\n",
    "        \n",
    "        print(\n",
    "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. Accuracy (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
    "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.177562Z",
     "iopub.execute_input": "2024-04-11T17:34:50.178008Z",
     "iopub.status.idle": "2024-04-11T17:34:50.203391Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.177974Z",
     "shell.execute_reply": "2024-04-11T17:34:50.201917Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:44.983495800Z",
     "start_time": "2024-04-15T18:57:44.927274600Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.486380Z",
     "iopub.execute_input": "2024-04-11T17:34:50.487487Z",
     "iopub.status.idle": "2024-04-11T17:34:50.498593Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.487434Z",
     "shell.execute_reply": "2024-04-11T17:34:50.497242Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:45.808183Z",
     "start_time": "2024-04-15T18:57:45.735402700Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=torch.nn.LSTM, dropout=None\n",
    ").to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:50.646098Z",
     "iopub.execute_input": "2024-04-11T17:34:50.647421Z",
     "iopub.status.idle": "2024-04-11T17:34:50.661479Z",
     "shell.execute_reply.started": "2024-04-11T17:34:50.647372Z",
     "shell.execute_reply": "2024-04-11T17:34:50.659595Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:46:24.923655200Z",
     "start_time": "2024-04-14T08:46:21.001667500Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим класс для подсчёта функции потерь и оптимизатор:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:52.169426Z",
     "iopub.execute_input": "2024-04-11T17:34:52.169838Z",
     "iopub.status.idle": "2024-04-11T17:34:52.177144Z",
     "shell.execute_reply.started": "2024-04-11T17:34:52.169807Z",
     "shell.execute_reply": "2024-04-11T17:34:52.175838Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:46:25.213590600Z",
     "start_time": "2024-04-14T08:46:24.923655200Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем обучить модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"Basic RNN ratings\",\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T17:34:57.048782Z",
     "iopub.execute_input": "2024-04-11T17:34:57.049265Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T08:47:55.873096900Z",
     "start_time": "2024-04-14T08:46:25.213590600Z"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbogachevv\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240414_114627-b5aij9ql</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">Basic RNN ratings</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd97d54cef9d46d79c1c3d93df5f018b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b9a35d153da41718eed73459b47ac6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.272/0.262\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc2d260de0dc4da3be41b341d1e44160"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.027/0.028. Accuracy (Train/Test): 0.358/0.332\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60e6ae419ca344d0bfcaa8aada6bb63e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.385/0.324\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9b7c1ffe2e346cca0bf2226604d2360"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.025/0.028. Accuracy (Train/Test): 0.392/0.335\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "465f4edcd9204f03bb3064a561ac95e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.023/0.027. Accuracy (Train/Test): 0.435/0.360\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec3c0addbc5546ef93534a869f7e2b3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.022/0.027. Accuracy (Train/Test): 0.467/0.366\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7283d55f8524e30ad62133f486a397f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.516/0.374\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5dd6bfd2ed04b6a912e18be6572dc12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.019/0.026. Accuracy (Train/Test): 0.552/0.353\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87f5f774cf4e44f4b817b4ff672949e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.017/0.027. Accuracy (Train/Test): 0.585/0.355\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2561f1bcdd24009bf88a5cd7b3508da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.016/0.028. Accuracy (Train/Test): 0.613/0.337\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e63eafcaab34dedaf7e03781951f526"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.015/0.029. Accuracy (Train/Test): 0.656/0.348\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c0204bb7d1e44ed918b60484bd4e71f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.013/0.031. Accuracy (Train/Test): 0.679/0.361\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d0310a8c9aa43a3977e317b02b01f5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.012/0.032. Accuracy (Train/Test): 0.746/0.322\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30e5ecbd9500487bbb97079864a6e101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.010/0.035. Accuracy (Train/Test): 0.776/0.343\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ec5ffd18b1e416597c22ac24a78d4c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.009/0.037. Accuracy (Train/Test): 0.817/0.317\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33ef01845d1b42d89b9e0fa03ca3d7ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▅▆▇▇█▇▇▆▆▇▅▆▄</td></tr><tr><td>eval/loss</td><td>▄▂▂▂▁▁▁▁▂▂▃▄▅▆█</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▃▃▄▅▅▅▆▆▇▇█</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▅▄▄▄▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.31728</td></tr><tr><td>eval/loss</td><td>0.03729</td></tr><tr><td>train/accuracy</td><td>0.81728</td></tr><tr><td>train/loss</td><td>0.00877</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">Basic RNN ratings</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/b5aij9ql/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240414_114627-b5aij9ql\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нерегуляризованные LSTM часто быстро переобучаются (и мы это видим по точности на контроле). Чтобы с этим бороться, часто используют *L2-регуляризацию* и *дропаут*.\n",
    "Однако способов накладывать дропаут на рекуррентный слой достаточно много, и далеко не все хорошо работают. По [ссылке](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b) доступен хороший обзор дропаутов для RNN.\n",
    "\n",
    "Мы реализуем два варианта DropOut для RNN (и третий дополнительно). Заодно увидим, что для реализации различных усовершенствований рекуррентной архитектуры приходится \"вскрывать\" слой до различной \"глубины\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Гала и Гарамани. Variational Dropout (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Начнем с дропаута, описанного в [статье Гала и Гарамани](https://arxiv.org/abs/1512.05287).\n",
    "Для этого нам потребуется перейти от использования слоя `torch.nn.LSTM`, полностью скрывающего от нас рекуррентную логику, к использованию слоя `torch.nn.LSTMCell`, обрабатывающего лишь один временной шаг нашей последовательности (а всю логику вокруг придется реализовать самостоятельно). \n",
    "\n",
    "Допишите класс `RNNLayer`. При `dropout=0` ваш класс должен работать как обычный слой LSTM, а при `dropout > 0` накладывать бинарную маску на входной и скрытый вектор на каждом временном шаге, причем эта маска должна быть одинаковой во все моменты времени.\n",
    "\n",
    "Дропаут Гала и Гарамани в виде формул (m обозначает маску дропаута):\n",
    "\n",
    "$$\n",
    "h_{t-1} = h_{t-1}*m_h, \\, x_t = x_t * m_x\n",
    "$$\n",
    "\n",
    "Далее обычный шаг рекуррентной архитектуры, например, LSTM:\n",
    "\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Union, Optional, Tuple, List"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:49.970640500Z",
     "start_time": "2024-04-15T18:57:49.945644100Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def init_h0_c0(num_objects: int, hidden_size: int, some_existing_tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    return h0 and c0, use some_existing_tensor.new_zeros() to gen them\n",
    "    h0 shape: num_objects x hidden_size\n",
    "    c0 shape: num_objects x hidden_size\n",
    "    \"\"\"\n",
    "    h0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    c0 = some_existing_tensor.new_zeros((num_objects, hidden_size))\n",
    "    \n",
    "    return h0, c0"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:50.117770600Z",
     "start_time": "2024-04-15T18:57:50.088758800Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def gen_dropout_mask(input_size, hidden_size, is_training, p, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    is_training: if True, gen masks from Bernoulli\n",
    "                 if False, gen masks consisting of (1-p)\n",
    "    \n",
    "    return dropout masks of size input_size, hidden_size if p is not None\n",
    "    return one masks if p is None\n",
    "    \"\"\"\n",
    "    if p is None:\n",
    "        return some_existing_tensor.new_ones((input_size, hidden_size))\n",
    "    \n",
    "    if is_training:\n",
    "        return some_existing_tensor.new_empty((input_size, hidden_size)).bernoulli_(1 - p)\n",
    "    else:\n",
    "        return some_existing_tensor.new_full((input_size, hidden_size), fill_value=1 - p)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:50.313704600Z",
     "start_time": "2024-04-15T18:57:50.245185800Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допишите класс-обёртку над `LSTMCell` для реализации Variational Dropout. **Используйте только цикл по времени**"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:09:12.282613Z",
     "start_time": "2021-04-01T21:09:12.256019Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout: Optional[float] = None, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device if device is not None else torch.device('cpu')\n",
    "        \n",
    "        self.rnn_cell = torch.nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize h_0, c_0\n",
    "        h, c = init_h0_c0(\n",
    "            num_objects=x.shape[1],\n",
    "            hidden_size=self.hidden_size,\n",
    "            some_existing_tensor=x\n",
    "        )\n",
    "        \n",
    "        h = h.to(self.device)\n",
    "        c = c.to(self.device)\n",
    "        \n",
    "        # Gen masks for input and hidden state\n",
    "        p = self.dropout\n",
    "        \n",
    "        input_mask = gen_dropout_mask(\n",
    "            input_size=x.shape[1],\n",
    "            hidden_size=self.input_size,\n",
    "            is_training=self.training,\n",
    "            p=p,\n",
    "            some_existing_tensor=x,\n",
    "        ).to(device)\n",
    "        \n",
    "        hidden_st_mask = gen_dropout_mask(\n",
    "            input_size=x.shape[1],\n",
    "            hidden_size=self.hidden_size,\n",
    "            is_training=self.training,\n",
    "            p=p,\n",
    "            some_existing_tensor=x,\n",
    "        ).to(device)\n",
    "                \n",
    "        # Implement recurrent logic and return what nn.LSTM returns\n",
    "        # Do not forget to apply generated dropout masks!\n",
    "        \n",
    "        rs = x.new_empty((x.shape[0], x.shape[1], self.hidden_size)).to(self.device)\n",
    "        \n",
    "        for idx in range(x.shape[0]):\n",
    "            # print(f\"DEB: {x.shape=}\\t{h.shape=}\\t{hidden_st_mask.shape=}\\t{input_mask.shape=}\")\n",
    "#             inp = x[idx] * input_mask.broadcast_to((x.shape[1], x.shape[2]))\n",
    "#             h = h * hidden_st_mask.broadcast_to((x.shape[1], self.hidden_size))\n",
    "            \n",
    "            inp = x[idx] * input_mask\n",
    "            h = h * hidden_st_mask\n",
    "    \n",
    "            h, c = self.rnn_cell(inp, (h, c))\n",
    "            rs[idx] = h\n",
    "        \n",
    "        return rs, (h, c)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:52.439981300Z",
     "start_time": "2024-04-15T18:57:52.370986900Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте реализованную модель с выключенным дропаутом (слой `RNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Сильно ли оно увеличилось по сравнению с `torch.nn.LSTM` (LSTM \"из коробки\")?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:39:51.746874900Z",
     "start_time": "2024-04-12T11:39:51.731876100Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=RNNLayer, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:39:52.200061400Z",
     "start_time": "2024-04-12T11:39:52.183332Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"DEB RNN ratings DO=0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:40:05.893103700Z",
     "start_time": "2024-04-12T11:39:52.826026400Z"
    }
   },
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_143952-nq67sl7m</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace' target=\"_blank\">DEB RNN ratings DO=0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/nq67sl7m/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0bc9e0d10594b4cbae49d71b92c94a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bcb8b67433b42838dd4aa12922f0f3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте полученную модель с `dropout=0.25`, вновь замерив время обучения. Получилось ли побороть переобучение? Сильно ли дольше обучается данная модель по сравнению с предыдущей? (доп. время тратится на генерацию масок дропаута)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=RNNLayer, dropout=0.25,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T15:30:11.356544800Z",
     "start_time": "2024-04-06T15:30:11.293802500Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"RNN ratings DO=0.25\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T16:01:38.201037Z",
     "start_time": "2024-04-06T15:30:11.341544200Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Гала и Гарамани. Дубль 2 (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<начало взлома pytorch>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "При разворачивании цикла по времени средствами python обучение рекуррентной нейросети сильно замедляется. Однако для реализации дропаута Гала и Гарамани необязательно явно задавать в коде умножение нейронов на маски. Можно схитрить и обойтись использованием слоя `torch.nn.LSTM`: перед вызовом `forward` слоя `torch.nn.LSTM` подменять его веса на веса, домноженные по строкам на маски. А обучаемые веса хранить отдельно. Именно так этот дропаут реализован в библиотеке `fastai`, код из которой использован в ячейке ниже.\n",
    "\n",
    "Такой слой реализуется в виде обертки над `torch.nn.LSTM`. Допишите класс:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:56.031158700Z",
     "start_time": "2024-04-15T18:57:55.976162500Z"
    }
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FastRNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, layers_dropout=0.0, num_layers=1, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.layers_dropout = layers_dropout\n",
    "        self.module = torch.nn.LSTM(input_size, hidden_size, dropout=layers_dropout, num_layers=num_layers, device=device)\n",
    "\n",
    "        self.layer_names = []\n",
    "        self.layer_name_tuples = []\n",
    "        for layer_n in range(self.num_layers):\n",
    "            self.layer_names += [f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}']\n",
    "            self.layer_name_tuples.append((f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}'))\n",
    "        \n",
    "        for layer in self.layer_names:\n",
    "            # Get torch.nn.Parameter with weights from torch.nn.LSTM instance\n",
    "            w = getattr(self.module, layer)\n",
    "\n",
    "            # Remove it from model\n",
    "            delattr(self.module, layer)\n",
    "\n",
    "            # And create new torch.nn.Parameter with the same data but different name\n",
    "            self.register_parameter(f'{layer}_raw', torch.nn.Parameter(w.data))\n",
    "\n",
    "            # Note. In torch.nn.LSTM.forward parameter with name `layer` will be used\n",
    "            #     so we must initialize it using `layer_raw` before forward pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _setweights(self, x):\n",
    "        \"\"\"\n",
    "            Apply dropout to the raw weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        for layer in self.layer_names:\n",
    "            # Generate mask\n",
    "            \n",
    "            # Get torch.nn.Parameter with weights\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            \n",
    "            # Apply dropout mask\n",
    "            if 'ih' in layer:\n",
    "                input_mask = gen_dropout_mask(\n",
    "                    input_size=raw_w.shape[0],\n",
    "                    hidden_size=raw_w.shape[1],\n",
    "                    is_training=self.training,\n",
    "                    p=self.layers_dropout,\n",
    "                    some_existing_tensor=x,\n",
    "                )\n",
    "                \n",
    "                # print(f\"DEB: {raw_w.shape=}\\t{input_mask.shape=}\")\n",
    "                \n",
    "                masked_raw_w = raw_w * input_mask\n",
    "            elif 'hh' in layer:\n",
    "                hidden_st_mask = gen_dropout_mask(\n",
    "                    input_size=raw_w.shape[0],\n",
    "                    hidden_size=raw_w.shape[1],\n",
    "                    is_training=self.training,\n",
    "                    p=self.dropout,\n",
    "                    some_existing_tensor=x,\n",
    "                )\n",
    "                \n",
    "                # print(f\"DEB: {raw_w.shape=}\\t{hidden_st_mask.shape=}\")\n",
    "                \n",
    "                masked_raw_w = raw_w * hidden_st_mask\n",
    "            else:\n",
    "                assert False, \"WHF???\"\n",
    "            \n",
    "            # Set modified weights in its place\n",
    "            setattr(self.module, layer, masked_raw_w)\n",
    "\n",
    "    def forward(self, x, h_c: Optional[Tuple[torch.Tensor, torch.Tensor]]=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor containing the features of the input sequence.\n",
    "        :param Optional[Tuple[torch.Tensor, torch.Tensor]] h_c: initial hidden state and initial cell state\n",
    "        \"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            # To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # Set new weights of self.module and call its forward\n",
    "            # Pass h_c with x if it is not None. Otherwise pass only x\n",
    "           \n",
    "            self._setweights(x)  # set weights from layer_raw to layer\n",
    "            \n",
    "            if h_c is not None:\n",
    "                return self.module.forward(x, h_c)\n",
    "            else:\n",
    "                return self.module.forward(x)\n",
    "            \n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'):\n",
    "            self.module.reset()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T18:57:56.241869100Z",
     "start_time": "2024-04-15T18:57:56.187910300Z"
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте реализованную модель с выключенным дропаутом (слой `FastRNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Убедитесь, что модель выдаёт такое же качество, как и оригинальная реализация LSTM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=FastRNNLayer, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T14:52:56.064754200Z",
     "start_time": "2024-04-14T14:52:55.968492300Z"
    }
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"FastRNN ratings DO=0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T14:54:32.045015900Z",
     "start_time": "2024-04-14T14:52:56.212804900Z"
    }
   },
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240414_175256-461lw0d4</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace' target=\"_blank\">FastRNN ratings DO=0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "075d3034362241e4a95fbd488520e7c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87c2402ff0be47d6b42df3a96948ce61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.031/0.032. Accuracy (Train/Test): 0.234/0.232\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36b54db43ef14766af1749a842b8caa4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.031/0.031. Accuracy (Train/Test): 0.256/0.247\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33e119f0c2c142a4927b5b06061bd9e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.281/0.267\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d675ff54b5c04430b10c4c0f3abdac47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.029/0.030. Accuracy (Train/Test): 0.312/0.297\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6970e7262ea418a940f9d605ebb3284"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.026/0.027. Accuracy (Train/Test): 0.368/0.339\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d521172bd87a476286e4dacc8b256532"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.390/0.353\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a49b44a05814492951ea407d9dc8cb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.024/0.027. Accuracy (Train/Test): 0.395/0.349\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d1578433d9f4f5e91fb8dae8c7f747a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.024/0.026. Accuracy (Train/Test): 0.414/0.357\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76795eaa891b4e04b5bf441235042510"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.022/0.026. Accuracy (Train/Test): 0.436/0.371\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c91ecaa4dc9435eae32bcae098898b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.022/0.025. Accuracy (Train/Test): 0.452/0.373\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06fa2ae316d84c7f93d1d01a83534f1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.021/0.025. Accuracy (Train/Test): 0.462/0.365\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bd179bc335246599f5de399ea6d8c5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.462/0.372\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "506e5b5e3915471fa693298cff0d7149"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.487/0.379\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "820d9b418a77435aa00a29cbeee512f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.493/0.382\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c1fc00a00034315aca154d7242cd2e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.499/0.380\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6a3b01864c2476fa57f411b5cd25256"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▃▄▆▇▆▇▇█▇████</td></tr><tr><td>eval/loss</td><td>██▇▆▃▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▅▅▅▆▆▇▇▇███</td></tr><tr><td>train/loss</td><td>██▇▆▅▄▄▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.3804</td></tr><tr><td>eval/loss</td><td>0.02591</td></tr><tr><td>train/accuracy</td><td>0.49924</td></tr><tr><td>train/loss</td><td>0.01992</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">FastRNN ratings DO=0</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/461lw0d4/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240414_175256-461lw0d4\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте полученный слой (вновь подставив его в `RNNClassifier` в качестве `rec_layer`) с `dropout=0.25`. Сравните время обучения с предыдущими моделями. Проследите, чтобы качество получилось такое же, как при первой реализации этого дропаута."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=FastRNNLayer, dropout=0.25,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T14:54:32.075999100Z",
     "start_time": "2024-04-14T14:54:32.039486900Z"
    }
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"FastRNN ratings DO=0.25\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T14:56:07.753270400Z",
     "start_time": "2024-04-14T14:54:32.046997700Z"
    }
   },
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01127777777777131, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc3a95e9bcb7479e9b1d6eb6db9d9b5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240414_175432-ffqdhdo1</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace' target=\"_blank\">FastRNN ratings DO=0.25</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e61ed27a858a4c51a2fc6c7d77cb6c37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1e2656634044f01a9704ae6e8f2a402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.031/0.032. Accuracy (Train/Test): 0.233/0.229\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75b91705b6b940849ce60fb569266541"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.031/0.031. Accuracy (Train/Test): 0.253/0.246\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0d112e9a8c340f78a6b0afac535a88a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15. Loss (Train/Test): 0.030/0.031. Accuracy (Train/Test): 0.277/0.263\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ab636090fb641ef9beef2544f6a2407"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15. Loss (Train/Test): 0.028/0.030. Accuracy (Train/Test): 0.313/0.296\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b638ff464e64328bc91d6a101162e06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15. Loss (Train/Test): 0.026/0.028. Accuracy (Train/Test): 0.360/0.336\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b21b4a8edf94d5db1c0b845a94c3ee9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15. Loss (Train/Test): 0.025/0.027. Accuracy (Train/Test): 0.380/0.349\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9678f556f1db4568a43666f1fdd85835"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15. Loss (Train/Test): 0.024/0.027. Accuracy (Train/Test): 0.391/0.348\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f90638c355d4f4491678d9e5201d30f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15. Loss (Train/Test): 0.024/0.026. Accuracy (Train/Test): 0.409/0.358\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d35355cdf4e44e08cbe36300cf46c22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15. Loss (Train/Test): 0.023/0.026. Accuracy (Train/Test): 0.416/0.356\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d9867dbac5e46479b511c12cfdea795"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15. Loss (Train/Test): 0.023/0.026. Accuracy (Train/Test): 0.431/0.360\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb8fb1ce285f48869b95b9ae41c6eaa8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15. Loss (Train/Test): 0.022/0.026. Accuracy (Train/Test): 0.441/0.364\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51d10331f42241c9b195ae59227d291a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15. Loss (Train/Test): 0.023/0.027. Accuracy (Train/Test): 0.425/0.343\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "505753adba0c481c98b89d5211761813"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.461/0.366\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24604d7a070348d7b114d4b6554470d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15. Loss (Train/Test): 0.021/0.026. Accuracy (Train/Test): 0.473/0.373\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4061ab14fff48eaa9f82f4534ddd1c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Loss (Train/Test): 0.020/0.026. Accuracy (Train/Test): 0.484/0.370\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d41fdbdd91848aaacab558c52fd4b8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▂▃▄▆▇▇▇▇▇█▇███</td></tr><tr><td>eval/loss</td><td>██▇▆▃▂▂▁▁▁▁▃▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▅▅▅▆▆▇▇▆▇██</td></tr><tr><td>train/loss</td><td>██▇▆▅▄▄▃▃▂▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.36968</td></tr><tr><td>eval/loss</td><td>0.02558</td></tr><tr><td>train/accuracy</td><td>0.48428</td></tr><tr><td>train/loss</td><td>0.02048</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">FastRNN ratings DO=0.25</strong> at: <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/ffqdhdo1/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240414_175432-ffqdhdo1\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "</конец взлома pytorch>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация дропаута по статье Семениуты и др. (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Перейдем к реализации дропаута для LSTM по статье [Semeniuta et al](http://www.aclweb.org/anthology/C16-1165). \n",
    "\n",
    "Этот метод применения дропаута не менее популярен, чем предыдущий. Его особенность состоит в том, что он придуман специально для гейтовых архитектур. В контексте LSTM этот дропаут накладывается только на информационный поток ($m_h$ — маска дропаута):\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot g \\odot {\\bf m_h} \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$\n",
    "На входы $x_t$ маска накладывается как в предыдущем дропауте. Впрочем, на входы маску можно наложить вообще до вызова рекуррентного слоя.\n",
    "\n",
    "Согласно статье, маска дропаута может быть как одинаковая, так и разная для всех моментов времени. Мы сделаем одинаковую для всех моментов времени.\n",
    "\n",
    "Для реализации этого дропаута можно: \n",
    "1. самостоятельно реализовать LSTM (интерфейса LSTMCell не хватит) \n",
    "2. снова воспользоваться трюком с установкой весов (но тут мы опираемся на свойство $tanh(0)=0$, к тому же, трюк в данном случае выглядит менее тривиально, чем с дропаутом Гала). \n",
    "\n",
    "Предлагается реализовать дропаут по сценарию 1. Допишите класс:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class HandmadeLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device('cpu') if device is None else device\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_weights = torch.nn.Linear(input_size, 4 * hidden_size).to(self.device)\n",
    "        self.hidden_weights = torch.nn.Linear(hidden_size, 4 * hidden_size).to(self.device)\n",
    "        self.activations = nn.ModuleList([\n",
    "            nn.ReLU(), # i\n",
    "            nn.ReLU(), # o\n",
    "            nn.ReLU(), # f\n",
    "            nn.Tanh(), # g\n",
    "            nn.Tanh(), # h\n",
    "        ]).to(self.device)\n",
    "        \n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        \"\"\"\n",
    "        Initialization as in Pytorch. \n",
    "        Do not forget to call this method!\n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert not torch.any(torch.isnan(x)), f\"x has nan: {x=}\"\n",
    "        # Use functions init_h0_c0 and gen_dropout_masks defined above\n",
    "        seq_len = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        input_dim = x.shape[2]\n",
    "        \n",
    "        h, c = init_h0_c0(\n",
    "            num_objects=batch_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            some_existing_tensor=x,\n",
    "        )\n",
    "        \n",
    "        # input_mask, hidden_st_mask = gen_dropout_mask(\n",
    "        #     input_size=input_dim,\n",
    "        #     hidden_size=hidden_dim,\n",
    "        #     is_training=self.training,\n",
    "        #     p=self.dropout,\n",
    "        #     some_existing_tensor=x,\n",
    "        # )\n",
    "        \n",
    "        # print(f\"DEB: {torch.all(input_mask == 1.0)}\\t{torch.all(hidden_st_mask == 1.0)}\")\n",
    "        \n",
    "        # Implement recurrent logic to mimic torch.nn.LSTM\n",
    "        # Do not forget to apply dropout mask\n",
    "        rs = x.new_empty((seq_len, batch_size, self.hidden_size))\n",
    "        for idx in range(seq_len):\n",
    "            # x ~ [L, B, F], h ~ [H]\n",
    "            # print(f\"DEB: {x.shape=}\\t{h.shape=}\\t{c.shape=}\")\n",
    "            \n",
    "            # DEB: disable masks\n",
    "            # inp = x[idx, :, :] * input_mask # x --> [B, F]\n",
    "            inp = x[idx, :, :]\n",
    "            assert not torch.any(torch.isnan(inp)), f\"inp has nan: idx={idx}\\n{inp=}\"\n",
    "            \n",
    "            inp4 = self.input_weights(inp)  # x4 ~ [B, 4H]\n",
    "            h4 = self.hidden_weights(h)  # h4 ~ [4H]\n",
    "            assert not torch.any(torch.isnan(inp4)), f\"inp4 has nan: idx={idx}\\n{inp4=}\"\n",
    "            assert not torch.any(torch.isnan(h4)), f\"h4 has nan: idx={idx}\\n{h4=}\"\n",
    "            \n",
    "            y = inp4 + h4.broadcast_to((batch_size, 4 * self.hidden_size))\n",
    "            assert not torch.any(torch.isnan(y)), f\"y has nan: idx={idx}\\n{y=}\"\n",
    "            \n",
    "            i = self.activations[0](y[:, 0:self.hidden_size])\n",
    "            o = self.activations[1](y[:, 1*self.hidden_size:2*self.hidden_size])\n",
    "            f = self.activations[2](y[:, 2*self.hidden_size:3*self.hidden_size])\n",
    "            g = self.activations[3](y[:, 3*self.hidden_size:4*self.hidden_size])\n",
    "            \n",
    "            assert not torch.any(torch.isnan(i)), f\"i has nan: idx={idx}\\n{i=}\"\n",
    "            assert not torch.any(torch.isnan(o)), f\"o has nan: idx={idx}\\n{o=}\"\n",
    "            assert not torch.any(torch.isnan(f)), f\"f has nan: idx={idx}\\n{f=}\"\n",
    "            assert not torch.any(torch.isnan(g)), f\"g has nan: idx={idx}\\n{g=}\"\n",
    "            \n",
    "            # DEB: disable masks\n",
    "            # c = f * c + i * g * hidden_st_mask\n",
    "            c = f * c + i * g\n",
    "            h = o * self.activations[4](c)\n",
    "            assert not torch.any(torch.isnan(c)), f\"c has nan: idx={idx}\\n{c=}\"\n",
    "            assert not torch.any(torch.isnan(h)), f\"h has nan: idx={idx}\\n{h=}\"\n",
    "            \n",
    "            rs[idx, :, :] = h\n",
    "        \n",
    "        assert not torch.any(torch.isnan(rs)), f\"rs has nan: {rs=}\"\n",
    "        return rs, (h, c)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:47.175776300Z",
     "start_time": "2024-04-12T11:56:47.121999800Z"
    }
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте вашу реализацию без дропаута (проконтролируйте качество и сравните время обучения с временем обучения `torch.nn.LSTM` и `RNNLayer`), а также с `dropout=0.25`. Сравните качество модели с таким дропаутом с качеством модели с дропаутом Гала и Гарамани."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:48.427670500Z",
     "start_time": "2024-04-12T11:56:48.365671500Z"
    }
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "g = set_global_seed(42)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=collate_fn_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=collate_fn_, \n",
    "                              shuffle=True, drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()\n",
    "\n",
    "model = RNNClassifier(\n",
    "    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10, vocab=vocab,\n",
    "    rec_layer=HandmadeLSTM, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:56:48.935714200Z",
     "start_time": "2024-04-12T11:56:48.890122200Z"
    }
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure = train(\n",
    "    train_dataloader, test_dataloader, model, loss_fn, optimizer, device, num_epochs,\n",
    "    name=\"HandmadeLSTM ratings DO=0.0\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:09:13.369299700Z",
     "start_time": "2024-04-12T11:56:49.600384700Z"
    }
   },
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01145555555555499, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6a25206998d45aba4d7eec256bf2312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Vladimir\\PycharmProjects\\MMP_prac_spring_24\\task_3\\wandb\\run-20240412_145649-57fgbyzk</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace' target=\"_blank\">HandmadeLSTM ratings DO=0.0</a></strong> to <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace' target=\"_blank\">https://wandb.ai/bogachevv/MMP_prac_rnn_deb/runs/57fgbyzk/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7127aa3a73084c9d85403fb4f76f17cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bebf6b51d52945bd93d60276aebb160e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15. Loss (Train/Test): 0.030/0.032. Accuracy (Train/Test): 0.264/0.233\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a955bc0791124d4884a1592a89fd63d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15. Loss (Train/Test): 0.029/0.031. Accuracy (Train/Test): 0.293/0.242\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/390 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac264afed37a46399e18cb26076b9785"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Embeddings has NaNs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses_pure, train_accuracies_pure, test_losses_pure, test_accuracies_pure \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHandmadeLSTM ratings DO=0.0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[36], line 66\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs, name)\u001B[0m\n\u001B[0;32m     63\u001B[0m rng \u001B[38;5;241m=\u001B[39m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs))\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m rng:\n\u001B[1;32m---> 66\u001B[0m     \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m evaluate(train_loader, model, loss_fn, device)\n\u001B[0;32m     69\u001B[0m     train_accuracies\u001B[38;5;241m.\u001B[39mappend(train_acc)\n",
      "Cell \u001B[1;32mIn[36], line 17\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(dataloader, model, loss_fn, optimizer, device)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(torch\u001B[38;5;241m.\u001B[39misnan(tokens)), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEval epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 17\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens_lens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits, ratings)\n\u001B[0;32m     19\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[33], line 71\u001B[0m, in \u001B[0;36mRNNClassifier.forward\u001B[1;34m(self, tokens, tokens_lens)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# DEBUG: store last input in globals\u001B[39;00m\n\u001B[0;32m     69\u001B[0m emb_dump \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(torch\u001B[38;5;241m.\u001B[39misnan(x)), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbeddings has NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# Make forward pass through recurrent network\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# YOUR CODE HERE\u001B[39;00m\n\u001B[0;32m     75\u001B[0m x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(x)\n",
      "\u001B[1;31mAssertionError\u001B[0m: Embeddings has NaNs"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([   0,    1,    4,  ..., 5001, 5004, 5016])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dump = inp_dump.cpu()\n",
    "\n",
    "torch.unique(inp_dump)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:12:56.040351100Z",
     "start_time": "2024-04-12T12:12:55.983535100Z"
    }
   },
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:15:24.836069800Z",
     "start_time": "2024-04-12T12:15:24.780492400Z"
    }
   },
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Token: 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(vocab)):\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39many(\n\u001B[0;32m      3\u001B[0m         torch\u001B[38;5;241m.\u001B[39misnan(\n\u001B[0;32m      4\u001B[0m             model\u001B[38;5;241m.\u001B[39mword_embeddings(\n\u001B[0;32m      5\u001B[0m                 torch\u001B[38;5;241m.\u001B[39mLongTensor([i])\n\u001B[0;32m      6\u001B[0m             )       \n\u001B[0;32m      7\u001B[0m         )\n\u001B[0;32m      8\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToken: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Token: 1"
     ]
    }
   ],
   "source": [
    "for i in range(len(vocab)):\n",
    "    assert not torch.any(\n",
    "        torch.isnan(\n",
    "            model.word_embeddings(\n",
    "                torch.LongTensor([i])\n",
    "            )       \n",
    "        )\n",
    "    ), f\"Token: {i}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:24.317561300Z",
     "start_time": "2024-04-12T12:16:24.250852100Z"
    }
   },
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        [ 1.1044, -1.1028,  0.5543,  ..., -0.6463,  0.0749,  0.1939],\n        ...,\n        [ 1.0837, -0.6411,  1.0597,  ...,  1.3895, -0.3897,  0.6304],\n        [ 0.4510, -0.0567, -1.1788,  ..., -0.0786,  2.0267,  0.6129],\n        [-0.6576, -0.7521,  0.2827,  ..., -0.0446,  1.5368, -0.1816]],\n       requires_grad=True)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embeddings.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:45.419413500Z",
     "start_time": "2024-04-12T12:16:45.392862300Z"
    }
   },
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.387698Z",
     "start_time": "2024-03-30T22:22:19.019606Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Сравнение всех предложенных моделей (1 балл)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T23:33:28.831346Z",
     "start_time": "2021-04-01T23:33:28.810453Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Используя замеры времени заполните табличку с временем работы четырёх реализованных моделей в следующей ячейке:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T23:48:05.361634Z",
     "start_time": "2021-04-01T23:48:05.333901Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| torch.nn.LSTM | RNNLayer | FastRNNLayer | HandmadeLSTM |\n",
    "|---------------|----------|--------------|--------------|\n",
    "| 2m 35s        | 14m 16s  | 2m 41s       | 31m 44s      |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.566217Z",
     "start_time": "2024-03-30T22:43:31.389958Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Крайне желательно рисовать графики в векторном формате. \n",
    "\n",
    "Если по каким-то причинам, отрисовка не будет работать, закомментируйте следующую ячейку."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib_inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:31.573342Z",
     "start_time": "2024-03-30T22:43:31.567690Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нарисуйте два графика — функция потерь и качество на обучающей и тестовой выборке для всех 7 моделей обученных выше."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "...\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_title('CrossEntropy Loss')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_title('Accuracy')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T22:43:32.290598Z",
     "start_time": "2024-03-30T22:43:31.575031Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сделайте итоговые выводы о качестве работы моделей с разными реализациями DropOut:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Бонус. Zoneout (0.5 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Это еще одна модификация идеи дропаута применительно к рекуррентным нейросетям. В Zoneout на каждом временном шаге с вероятностью $p$ компонента скрытого состояния обновляется, а с вероятностью $1-p$ берется с предыдущего шага. \n",
    "В Виде формул ($m^t_h$ - бинарная маска):\n",
    " \n",
    "(сначала обычный рекуррентный переход, например LSTM)\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t)\n",
    "$$\n",
    "Затем Zoneout:\n",
    "$$\n",
    "h_t = h_t * m_h^t + h_{t-1}*(1-m_h^t)\n",
    "$$\n",
    "В этом методе маска уже должна быть разная во все моменты времени (иначе метод упрощается до дропаута Гала и Гарамани). На входы $x_t$ вновь можно накладывать маску до начала работы рекуррентного слоя.  \n",
    "\n",
    "Если у вас осталось время, вы можете реализовать этот метод. Выберите основу из трех рассмотренных случаев самостоятельно.\n",
    "\n",
    "**Полный балл ставится только при наличии качественного и количественного сравнения с предыдущими моделями.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `Часть 2. Language Modeling с помощью LSTM (5 баллов)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:05:00.702763Z",
     "start_time": "2021-03-31T16:05:00.674835Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Во второй части мы попробуем обучить модель для генерации отзывов по их началу."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Концептуально модель будет выглядеть следующим образом:\n",
    "    \n",
    "![image info](https://blog.feedly.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-12.08.35-PM.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В процессе обучения будем тренировать сеть предсказывать вероятность следующего символа при условии всех предыдущих. Эту вероятность можно моделировать с помощью скрытого состояния $h^{(t)}$ пропуская его через линейный слой с выходной размерностью равной размерности словаря:\n",
    "$$\n",
    "p(x^{t}|x^{t-1}, ..., x^{1}) = SoftMax(Linear(h^{(t)}))\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обратите внимание, что для вычисления $p(x^{t}|x^{t-1}, ..., x^{1})$ для всех моментов времени достаточно сделать один проход по RNN, а затем применить линейное преобразование ко всем скрытым состояниям."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве функции потерь необходимо использовать `CrossEntropy`."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T00:37:56.100520Z",
     "start_time": "2021-04-02T00:37:56.072747Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим другой важный момент. Для того, чтобы решить данную задачу, модель должна уметь определять момент начала генерации предложения и оповещать о завершении генерации — конце предложения. Для этого добавим в словарь вспомогательные токены `<sos>`, `<eos>`. Добавив `<sos>` в начало каждого предложения и `<eos>` в конец.\n",
    "\n",
    "Модель сможет начинать генерацию как только ей будет передан токен `<sos>` и заканчивать генерацию, как только на очередном месте самым вероятным токеном оказывается `<eos>`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для решения этой задачи мы воспользуемся уже реализованной LSTM с дропаутом `FastRNNLayer` и классом `RNNClassifier`, то есть архитектура сети принципиально не поменяется. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация модели и цикла обучения (2 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Не используйте циклы в `RNNLM`, `LMCrossEntropyLoss`, `LMAccuracy`**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNLM(RNNClassifier):\n",
    "    def __init__(\n",
    "        self, embedding_dim, hidden_dim, vocab, dropout=0.5, layers_dropout=0.5, num_layers=1\n",
    "    ):\n",
    "        super().__init__(\n",
    "            embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(vocab), vocab=vocab,\n",
    "            rec_layer=FastRNNLayer, dropout=dropout, layers_dropout=layers_dropout, num_layers=num_layers\n",
    "        )\n",
    "    \n",
    "        # super().__init__(\n",
    "        #     embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(vocab), vocab=vocab,\n",
    "        #     rec_layer=nn.LSTM, dropout=None, num_layers=num_layers\n",
    "        # )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, len(vocab))\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor(dtype=torch.long) tokens: \n",
    "            Batch of texts represented with tokens. Shape: [T, B]\n",
    "        :param torch.Tensor(dtype=torch.long) tokens_lens: \n",
    "            Number of non-padding tokens for each object in batch. Shape: [B]\n",
    "        :return torch.Tensor: \n",
    "            Distribution of next token for each time step. Shape: [T, B, V], V — size of vocabulary\n",
    "        \"\"\"\n",
    "        # Make embeddings for all tokens\n",
    "        x = self.word_embeddings(tokens)\n",
    "        \n",
    "        # Forward pass embeddings through network\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Take all hidden states from the last layer of LSTM for each step and perform linear transformation\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:44.623407700Z",
     "start_time": "2024-04-15T19:59:44.549402400Z"
    }
   },
   "execution_count": 182,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуем функцию потерь для данной задачи. \n",
    "\n",
    "Моменты на которые нужно обратить внимание:\n",
    "1. Распределение вероятности следующего токена для последнего токена в последовательности не участвует в подсчёте функции потерь.\n",
    "2. Необходимо учитывать, что в одном батче могут быть тексты разной длины."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для решения второй проблемы можно воспользоваться функцией `torch.nn.utils.rnn.pack_padded_sequence`. \n",
    "\n",
    "Принимая на вход батч тензоров и длину каждого тензора без учёта паддинга эта функция позволяет получить все элементы в тензорах, которые не относятся к паддингу в виде плоского массива:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "padded_tensors = torch.tensor([\n",
    "    [[1, 11, 111], [2, 22, 222], [3, 33, 333]],\n",
    "    [[4, 44, 444], [5, 55, 555], [6, 66, 666]],\n",
    "    [[7, 77, 777], [0, 0, 0], [8, 88, 888]],\n",
    "    [[9, 99, 999], [0, 0, 0], [0, 0, 0]]\n",
    "])\n",
    "tensors_lens = torch.tensor([4, 2, 3])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:45.486132300Z",
     "start_time": "2024-04-15T19:59:45.432619700Z"
    }
   },
   "execution_count": 183,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обратите внимание, что `torch.nn.utils.rnn.pack_padded_sequence` автоматически переупорядочивает тензоры в батче по убыванию их длины."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.nn.utils.rnn.pack_padded_sequence(padded_tensors, tensors_lens, batch_first=False, enforce_sorted=False)[0].shape"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:47.014888900Z",
     "start_time": "2024-04-15T19:59:46.945319Z"
    }
   },
   "execution_count": 184,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([9, 3])"
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class LMCrossEntropyLoss(torch.nn.CrossEntropyLoss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, outputs, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n",
    "        :param torch.Tensor tokens: Batch of tokens. Shape: [T, B]\n",
    "        :param torch.Tensor tokens_lens: Length of each sequence in batch\n",
    "        :return torch.Tensor: CrossEntropyLoss between corresponding logits and tokens\n",
    "        \"\"\"\n",
    "        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n",
    "        # Do not forget specify enforce_sorted=False and correct value of batch_first \n",
    "        \n",
    "        # print(f\"DEB: {outputs.shape=}\\t{tokens.shape=}\")\n",
    "        \n",
    "        # packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs[:-1], tokens_lens.cpu()-1, batch_first=False, enforce_sorted=False)[0]\n",
    "        # packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens[1:], tokens_lens.cpu()-1, batch_first=False, enforce_sorted=False)[0]\n",
    "        \n",
    "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "        \n",
    "        # print(f\"DEB: {packed_outputs.shape=}\\t{packed_tokens.shape=}\")\n",
    "        \n",
    "        # Use super().forward(..., ...) to compute CrossEntropyLoss\n",
    "        return super().forward(\n",
    "            input=packed_outputs,\n",
    "            target=packed_tokens\n",
    "        )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:47.271639900Z",
     "start_time": "2024-04-15T19:59:47.217107400Z"
    }
   },
   "execution_count": 185,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для оценки качества нам также необходимо вычислять долю правильно предсказанных токенов. Реализуйте класс для вычисления точности."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class LMAccuracy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, outputs, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n",
    "        :param torch.Tensor tokens: Batch of tokens. Shape: [T, B]\n",
    "        :param torch.Tensor tokens_lens: Length of each sequence in batch\n",
    "        :return torch.Tensor: Accuracy for given logits and tokens\n",
    "        \"\"\"\n",
    "        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n",
    "        # Do not forget specify enforce_sorted=False and correct value of batch_first \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "        # packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens.cpu(), batch_first=False, enforce_sorted=False)[0]\n",
    "    \n",
    "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens, tokens_lens, batch_first=False, enforce_sorted=False)[0]\n",
    "    \n",
    "        preds = torch.argmax(packed_outputs, dim=1)\n",
    "    \n",
    "        return torch.sum(\n",
    "            preds == packed_tokens\n",
    "        )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:47.877403200Z",
     "start_time": "2024-04-15T19:59:47.823396100Z"
    }
   },
   "execution_count": 186,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Модифицируйте функции `train_epoch`, `evaluate`, `train` для обучения LM.\n",
    "\n",
    "**При вычислении точности, обратите внимание на то, что мы не предсказываем первый токен в каждой последовательности и токены, относящиеся к паддингу.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch_lm(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Make optimizer step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tokens = data['tokens'][:-1, :].to(device)\n",
    "        targets = data['tokens'][1:, :].to(device)\n",
    "        tokens_lens = data['tokens_lens'].cpu()\n",
    "        \n",
    "        logits = model(tokens, tokens_lens - 1)\n",
    "        loss = loss_fn(logits, targets, tokens_lens - 1)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_lm(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    accuracy_fn = LMAccuracy()  # DEB: return 0.0\n",
    "    \n",
    "    for idx, data in enumerate(dataloader):\n",
    "        # 1. Take data from batch\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Evaluate loss\n",
    "        # 4. Evaluate accuracy\n",
    "        \n",
    "        tokens = data['tokens'][:-1, :].to(device)\n",
    "        targets = data['tokens'][1:, :].to(device)\n",
    "        tokens_lens = data['tokens_lens'].cpu()\n",
    "        \n",
    "        logits = model(tokens, tokens_lens - 1)\n",
    "        loss = loss_fn(logits, targets, tokens_lens - 1).item()\n",
    "        acc = accuracy_fn(logits, targets, tokens_lens - 1).item()\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += torch.sum(tokens_lens)\n",
    "        total_accuracy += acc\n",
    "        \n",
    "            \n",
    "    return total_loss / total_tokens, total_accuracy / total_tokens\n",
    "\n",
    "def train_lm(\n",
    "    train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs\n",
    "):\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_epoch_lm(train_loader, model, loss_fn, optimizer, device)\n",
    "        \n",
    "        train_loss, train_acc = evaluate_lm(train_loader, model, loss_fn, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss, test_acc = evaluate_lm(test_loader, model, loss_fn, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(\n",
    "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. Accuracy (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
    "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:49.878122900Z",
     "start_time": "2024-04-15T19:59:49.858609200Z"
    }
   },
   "execution_count": 187,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь у нас всё готово для обучения модели."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим словарь с `<sos>`, `<eos>` токенами.\n",
    "\n",
    "Обратите внимание, что в отличие от классификации текстов нам необходимо значительно увеличить размер словаря, чтобы доля `<unk>` токенов была не велика.\n",
    "\n",
    "Так же, так как задача генерации значительно сложнее задачи классификации текстов будем обучать модель только на префиксах рецензий длины $20$. Это позволяет значительно ускорить обучение."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T01:06:12.736180Z",
     "start_time": "2021-04-02T01:06:12.708814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for special in specials:\n",
    "    counter[special] = 0\n",
    "# min_freq=8 is approximately equivalent to max_size=30000. \n",
    "#   You can lower min_freq in order to make model vocabulary more diverse \n",
    "lm_vocab = torchtext.vocab.vocab(counter, specials=specials, special_first=True, min_freq=8)\n",
    "lm_vocab.set_default_index(vocab['<unk>'])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:11:47.157093800Z",
     "start_time": "2024-04-15T19:11:47.103579200Z"
    }
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lm_datasets_dump_path = pathlib.Path(\"./lm_datasets_dump.bin\")\n",
    "\n",
    "if not lm_datasets_dump_path.exists():\n",
    "    lm_test_dataset = LargeMovieReviewDataset(test_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)\n",
    "    lm_train_dataset = LargeMovieReviewDataset(train_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)\n",
    "    \n",
    "    with open(lm_datasets_dump_path, \"wb\") as f:\n",
    "        torch.save(\n",
    "            obj=(lm_test_dataset, lm_train_dataset),\n",
    "            f=f\n",
    "        )\n",
    "else:\n",
    "    with open(lm_datasets_dump_path, \"rb\") as f:\n",
    "        lm_test_dataset, lm_train_dataset = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:11:49.553279900Z",
     "start_time": "2024-04-15T19:11:47.734173800Z"
    }
   },
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",\n 'label': tensor(0),\n 'rating': tensor(1),\n 'tokens': tensor([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n         21, 22, 23,  3]),\n 'tokens_len': 22}"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_test_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:11:50.062055900Z",
     "start_time": "2024-04-15T19:11:49.973366400Z"
    }
   },
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим даталоадеры для тестовой и обучающей выборок:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "lm_test_dataloader = DataLoader(\n",
    "    lm_test_dataset, batch_size=196, shuffle=False, num_workers=0, \n",
    "    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n",
    ")\n",
    "lm_train_dataloader = DataLoader(\n",
    "    lm_train_dataset, batch_size=196, shuffle=True, num_workers=0, \n",
    "    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:02.835636800Z",
     "start_time": "2024-04-15T19:59:02.785122900Z"
    }
   },
   "execution_count": 174,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Убедитесь, что все предложения имеют в начале `<sos>` токен, а в конце — `<eos>` токен."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch = next(iter(lm_train_dataloader))\n",
    "batch['tokens'], batch['tokens_lens']"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:03.185392100Z",
     "start_time": "2024-04-15T19:59:03.117857700Z"
    }
   },
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[    2,     2,     2,  ...,     2,     2,     2],\n         [ 3236,  1768,     7,  ...,    50, 10243,   431],\n         [ 1385,   369,  2059,  ...,    78,    35,  1096],\n         ...,\n         [ 7065,     1,  3278,  ...,  3694,   325,  1886],\n         [ 4045,  3826,    94,  ...,   495,    24,  1223],\n         [    3,     3,     3,  ...,     3,     3,     3]]),\n tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]))"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создадим модель, функцию потерь и оптимизатор: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:04.222309800Z",
     "start_time": "2024-04-15T19:59:04.195303500Z"
    }
   },
   "execution_count": 176
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2486"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:04.740501300Z",
     "start_time": "2024-04-15T19:59:04.549302200Z"
    }
   },
   "execution_count": 177
  },
  {
   "cell_type": "code",
   "source": [
    "lm_model = RNNLM(\n",
    "    embedding_dim=512, hidden_dim=512, vocab=lm_vocab, dropout=0.6, layers_dropout=0.6, num_layers=2\n",
    ").to(device=device)\n",
    "\n",
    "# lm_model = RNNLM(\n",
    "#     embedding_dim=512, hidden_dim=512, vocab=lm_vocab, num_layers=2\n",
    "# ).to(device=device)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:10.423207600Z",
     "start_time": "2024-04-15T19:59:10.233707Z"
    }
   },
   "execution_count": 178,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_loss_fn = LMCrossEntropyLoss(reduction='sum')\n",
    "lm_optimizer = torch.optim.Adam(lm_model.parameters(), lr=0.005, weight_decay=1.2e-6)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:10.813146400Z",
     "start_time": "2024-04-15T19:59:10.761411900Z"
    }
   },
   "execution_count": 179,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучим модель:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# lm_model = torch.compile(lm_model)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:11.530186Z",
     "start_time": "2024-04-15T19:59:11.483673100Z"
    }
   },
   "execution_count": 180,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_train_losses, lm_train_accuracies, lm_test_losses, lm_test_accuracies = train_lm(\n",
    "    lm_train_dataloader, lm_test_dataloader, lm_model, lm_loss_fn, lm_optimizer, device, 10\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:59:18.932039600Z",
     "start_time": "2024-04-15T19:59:11.782008900Z"
    }
   },
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0021407f39846c582098eb94a772ad9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "670bf8803cb943b28750848905f94cc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n",
      "DEB: x.shape=torch.Size([21, 196, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[181], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m lm_train_losses, lm_train_accuracies, lm_test_losses, lm_test_accuracies \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_lm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlm_train_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_test_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_loss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\n\u001B[0;32m      3\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[173], line 60\u001B[0m, in \u001B[0;36mtrain_lm\u001B[1;34m(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs)\u001B[0m\n\u001B[0;32m     58\u001B[0m train_accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs)):\n\u001B[1;32m---> 60\u001B[0m     \u001B[43mtrain_epoch_lm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m evaluate_lm(train_loader, model, loss_fn, device)\n\u001B[0;32m     63\u001B[0m     train_accuracies\u001B[38;5;241m.\u001B[39mappend(train_acc)\n",
      "Cell \u001B[1;32mIn[173], line 19\u001B[0m, in \u001B[0;36mtrain_epoch_lm\u001B[1;34m(dataloader, model, loss_fn, optimizer, device)\u001B[0m\n\u001B[0;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits, targets, tokens_lens \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     17\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 19\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:356\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprofile_hook_step\u001B[39m(func: Callable[_P, R]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Callable[_P, R]:\n\u001B[1;32m--> 356\u001B[0m     \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    357\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m R:\n\u001B[0;32m    358\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m_ \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    359\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m cast(Optimizer, \u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lm_model_dump_path = pathlib.Path(\"./lm_model_dump.bin\")\n",
    "\n",
    "with open(lm_model_dump_path, \"wb\") as f:\n",
    "    torch.save(\n",
    "        obj=lm_model,\n",
    "        f=f\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:34:50.487930Z",
     "start_time": "2024-04-15T19:34:50.217132500Z"
    }
   },
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(lm_model_dump_path, \"rb\") as f:\n",
    "    lm_model = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:01:58.778993300Z",
     "start_time": "2024-04-15T20:01:58.612215500Z"
    }
   },
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pred_0 = lm_model(batch['tokens'].to(device), batch['tokens_lens'].to(device))[:, 1, :].cpu()\n",
    "pred_0 = torch.argmax(pred_0, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:03:01.496366100Z",
     "start_time": "2024-04-15T20:03:01.206170400Z"
    }
   },
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([  7,   7,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 169,   1,\n           1,   1,   3,   3,   3,   3,   3,   3]),\n tensor([    2,  1768,   369,  1205,  7630,   502,  1363, 12547,   524,  2417,\n          5326,    76,  1109,  9018,     1,  1168,   657,   657,     1,     1,\n          3826,     3]))"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_0, batch['tokens'][:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:03:01.500366300Z",
     "start_time": "2024-04-15T20:03:01.486032500Z"
    }
   },
   "execution_count": 193
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Реализация декодера (1 балл)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь, реализуем последнюю деталь — декодирование с использованием обученной модели.\n",
    "Есть несколько вариантов. Рассмотрим два самых простых:\n",
    "1. **Жадное декодирование.** На каждом шаге мы выбираем токен с максимальной вероятностью и используем его для обновления скрытого состояния RNN.\n",
    "2. **Top-k sampling.** На очередном шаге рассматриваются $k$ токенов с самыми большими вероятностями. Остальные токены игнорируются. Из выбранных токенов семплируется следующий токен пропорционально их вероятностям.\n",
    "\n",
    "Прочитать подробнее про разные варианты декодирования можно по ссылкам:\n",
    "1. [От huggingface](https://huggingface.co/blog/how-to-generate)\n",
    "2. [На towardsdatascience](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Существенным в процессе декодирования является критерий останова. Как только очередной самый вероятный символ оказался `<eos>`, то данная последовательность считается сгенерированной. Однако, может так оказаться, что `<eos>` никогда не будет выбран, тогда необходимо прекратить генерацию, как только длина последовательности перейдёт порог `max_generated_len`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def decode(model, start_tokens, start_tokens_lens, max_generated_len=20, top_k=None):\n",
    "    \"\"\"\n",
    "    :param RNNLM model: Model\n",
    "    :param torch.Tensor start_tokens: Batch of seed tokens. Shape: [T, B]\n",
    "    :param torch.Tensor start_tokens_lens: Length of each sequence in batch. Shape: [B]\n",
    "    :param int max_generated_len: Maximum lenght of generated samples\n",
    "    :param Optional[int] top_k: Number of tokens with the largest probability to sample from\n",
    "    :return Tuple[torch.Tensor, torch.Tensor]. \n",
    "        Newly predicted tokens and length of generated part. Shape [T*, B], [B]\n",
    "    \"\"\"\n",
    "    # Get embedding for start_tokens\n",
    "    embedding = model.word_embeddings(start_tokens)\n",
    "    \n",
    "    # Pass embedding through rnn and collect hidden states and cell states for each time moment\n",
    "    all_h, all_c = [], []\n",
    "    h = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    c = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    for time_step in range(start_tokens.shape[0]):\n",
    "        _, (h, c) = model.rnn(embedding[time_step][None, :, :], (h, c))\n",
    "        all_h.append(h)\n",
    "        all_c.append(c)\n",
    "    \n",
    "    all_h = torch.stack(all_h, dim=1)\n",
    "    all_c = torch.stack(all_c, dim=1)\n",
    "    \n",
    "    # Take final hidden state and cell state for each start sequence in batch\n",
    "    # We will use them as h_0, c_0 for generation new tokens\n",
    "    h = all_h[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n",
    "    c = all_c[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n",
    "    \n",
    "    # List of predicted tokens for each time step\n",
    "    predicted_tokens = []\n",
    "    # Length of generated part for each object in the batch\n",
    "    decoded_lens = torch.zeros_like(start_tokens_lens, dtype=torch.long)\n",
    "    # Boolean mask where we store if the sequence has already generated\n",
    "    # i.e. `<eos>` was selected on any step\n",
    "    is_finished_decoding = torch.zeros_like(start_tokens_lens, dtype=torch.bool)\n",
    "    \n",
    "    # Stop when all sequences in the batch are finished\n",
    "    while not torch.all(is_finished_decoding) and torch.max(decoded_lens) < max_generated_len:\n",
    "        # Evaluate next token distribution using hidden state h.\n",
    "        # Note. Over first dimension h has hidden states for each layer of LSTM.\n",
    "        #     We must use hidden state from the last layer\n",
    "        # logits, (h, c) = model.rnn(h, (h, c))\n",
    "        # logits = model.linear(logits)\n",
    "        \n",
    "        logits = model.linear(h[-1, :, :])\n",
    "        \n",
    "        if top_k is not None:\n",
    "            # Top-k sampling. Use only top-k most probable logits to sample next token\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            # Mask non top-k logits\n",
    "            logits[indices_to_remove] = -1e10\n",
    "            # Sample next_token. \n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(\n",
    "                input=logits,\n",
    "                num_samples=1,\n",
    "            )[:, 0]\n",
    "        else:\n",
    "            # Select most probable token\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        predicted_tokens.append(next_token)\n",
    "        \n",
    "        decoded_lens += (~is_finished_decoding)\n",
    "        is_finished_decoding |= (next_token == torch.tensor(model.vocab.lookup_indices(['<eos>'])[0]))\n",
    "\n",
    "        # Compute embedding for next token\n",
    "        embedding = model.word_embeddings(next_token)\n",
    "        \n",
    "        # Update hidden and cell states\n",
    "        _, (h, c) = model.rnn(embedding[None, :, :], (h, c))\n",
    "    \n",
    "    return torch.stack(predicted_tokens), decoded_lens"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:35:40.990672Z",
     "start_time": "2024-04-15T19:35:40.927872Z"
    }
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем сгенерировать продолжения для нескольких префиксов:"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T01:38:06.232189Z",
     "start_time": "2021-04-02T01:38:06.205413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "start_tokens = torch.tensor([\n",
    "    lm_model.vocab.lookup_indices(['<sos>', '<pad>', '<pad>', '<pad>']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'my', 'favorite', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'best', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'worst', 'movie']),\n",
    "]).T\n",
    "\n",
    "start_tokens_lens = torch.tensor([1, 4, 4, 4])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:35:41.829200600Z",
     "start_time": "2024-04-15T19:35:41.769171100Z"
    }
   },
   "execution_count": 103,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lm_model = lm_model.cpu()\n",
    "lm_model.eval()\n",
    "# decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=20)\n",
    "# decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=5)\n",
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=None)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:36:17.928816300Z",
     "start_time": "2024-04-15T19:36:17.781783Z"
    }
   },
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 4]), torch.Size([4]))"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens.shape, decoded_lens.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:36:18.144888100Z",
     "start_time": "2024-04-15T19:36:18.016830900Z"
    }
   },
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "source": [
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:36:18.263901400Z",
     "start_time": "2024-04-15T19:36:18.217888500Z"
    }
   },
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1406, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> favorite movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 60, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> best movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1203, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> worst movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуйте выполнить семплирование для разных $k$. Сравните результаты top-k семплирования с жадным декодированием. Опишите ваши наблюдения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=15)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:37:43.382025500Z",
     "start_time": "2024-04-15T19:37:43.166386100Z"
    }
   },
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 7, 525, 1891, 131, 7, 136, 169, 93, 241, 28]\n",
      "<sos> movie ever heard seen movie made like one movies time\n",
      "tokens=[2, 1, 1406, 7, 525, 131, 7, 169, 169, 371, 93, 60, 3017, 93]\n",
      "<sos> <unk> favorite movie ever seen movie like like love one best flicks one\n",
      "tokens=[2, 1, 60, 7, 1, 136, 1, 115, 131, 159, 159, 174, 28, 28]\n",
      "<sos> <unk> best movie <unk> made <unk> film seen first first minutes time time\n",
      "tokens=[2, 1, 1203, 7, 525, 130, 131, 131, 88, 7, 1, 7, 689, 689]\n",
      "<sos> <unk> worst movie ever ive seen seen times movie <unk> movie bad bad\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 93, 241, 525, 169, 7, 131, 88, 704, 115, 131]\n",
      "<sos> one movies ever like movie seen times say film seen\n",
      "tokens=[2, 1, 1406, 7, 1023, 435, 72, 1023, 159, 7, 65, 211, 1, 438]\n",
      "<sos> <unk> favorite movie great director films great first movie could get <unk> make\n",
      "tokens=[2, 1, 60, 7, 159, 28, 99, 1, 1, 7, 136, 3]\n",
      "<sos> <unk> best movie first time watch <unk> <unk> movie made <eos>\n",
      "tokens=[2, 1, 1203, 7, 131, 1, 1, 463, 1, 93, 72, 60, 525, 525]\n",
      "<sos> <unk> worst movie seen <unk> <unk> horror <unk> one films best ever ever\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=25)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:37:50.078633700Z",
     "start_time": "2024-04-15T19:37:49.884945400Z"
    }
   },
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 115, 239, 159, 261, 7, 3340, 1, 261, 7, 119]\n",
      "<sos> film thought first saw movie theater <unk> saw movie still\n",
      "tokens=[2, 1, 1406, 7, 241, 882, 169, 7, 230, 895, 709, 169, 241, 704]\n",
      "<sos> <unk> favorite movie movies terrible like movie even worse think like movies say\n",
      "tokens=[2, 1, 60, 7, 525, 525, 131, 129, 65, 689, 159, 91, 99, 7]\n",
      "<sos> <unk> best movie ever ever seen every could bad first dont watch movie\n",
      "tokens=[2, 1, 1203, 7, 131, 882, 1, 7, 169, 7, 689, 204, 689, 147]\n",
      "<sos> <unk> worst movie seen terrible <unk> movie like movie bad pretty bad script\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=50)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:37:56.926254100Z",
     "start_time": "2024-04-15T19:37:56.722079600Z"
    }
   },
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[2, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1406, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> favorite movie <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 60, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> best movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tokens=[2, 1, 1203, 7, 525, 131, 7, 136, 1, 1, 1, 1, 1, 1]\n",
      "<sos> <unk> worst movie ever seen movie made <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=10, top_k=None)\n",
    "for text_idx in range(start_tokens.shape[1]):\n",
    "    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n",
    "    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist()\n",
    "    tokens = tokens + decoded_text_tokens.tolist()\n",
    "    print(f\"{tokens=}\")\n",
    "    words = np.array(lm_model.vocab.get_itos())[np.array(tokens)]\n",
    "    print(' '.join(words))\n",
    "    \n",
    "    # text = ' '.join(words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # print(f\"{text}\")\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{text}</b></div>'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:38:28.258702700Z",
     "start_time": "2024-04-15T19:38:28.096160Z"
    }
   },
   "execution_count": 116
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Beam Search (2 балла)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим более продвинутый алгоритм для декодирования. Реализуйте алгоритм Beam Search."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Несколько замечаний по имплементации:\n",
    "\n",
    "1. При больших размерах `beam_size` число гипотез ($B \\times \\text{beam\\_size}$) на очередном шаге может быть слишком большим. Поэтому может потребоваться разбить все гипотезы на отдельные батчи и делать forward-pass в несколько итераций. Используйте [`torch.split`](https://pytorch.org/docs/stable/generated/torch.split.html)\n",
    "2. Для выбора лучших гипотез используйте [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html). Обратите внимание на индексы, которые возвращает эта функция (может пригодиться метод [`torch.remainder`](https://pytorch.org/docs/stable/generated/torch.remainder.html))\n",
    "3. Можно отслеживать, какие элементы в батче (или какие гипотезы) закончили генерацию. Делая forward-pass только для незавершённых гипотез, можно ускорить декодинг, однако, это усложнит реализацию"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_tensor(t):\n",
    "    print(f\"{t.shape=}\")\n",
    "    print(\"tensor=(\")\n",
    "    for i in range(t.shape[0]):\n",
    "        print(f\"\\t{t[i]}\")\n",
    "    print(f\")\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:32:57.913438700Z",
     "start_time": "2024-04-15T20:32:57.867925Z"
    }
   },
   "execution_count": 213
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def decode_beam_search(model, start_tokens, start_tokens_lens, max_generated_len=20, beam_size=5):\n",
    "    \"\"\"\n",
    "    :param RNNLM model: Model\n",
    "    :param torch.Tensor start_tokens: Batch of seed tokens. Shape: [T, B]\n",
    "    :param torch.Tensor start_tokens_lens: Length of each sequence in batch. Shape: [B]\n",
    "    :param int max_generated_len: Maximum length of generated samples\n",
    "    :param int beam_size: Size of beam\n",
    "    :return Tuple[torch.Tensor, torch.Tensor, torch.Tensor]. \n",
    "        Newly predicted tokens, lengths of generated parts and log probabilities for each hypotheses \n",
    "        Shape [T*, B, beam_size], [T*, beam_size], [T*, beam_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    # L — number of RNN layers in the model, H — hidden size, BS — beam size\n",
    "    #\n",
    "    # 1. Make forward pass of start_tokens through the model. \n",
    "    #      Obtain the last cell and hidden state for each element in the batch \n",
    "    #          (i.e. tensors of shape [L, B, H])\n",
    "    #      Use those states as the initialization for each hypotheses in the beam \n",
    "    #          (i.e. tensors of shape [L, B * BS, H])\n",
    "    #      Initialize probabilities for each hypotheses in the beam with 1.0\n",
    "    #          (i.e. tensor of shape [B * BS])\n",
    "    #      Initialize vector that show whether hypothesis is finished\n",
    "    #          (i.e. tensor of shape [B * BS])\n",
    "    # 2. While all sequences do not end with <eos> and their length less than max_generated_len\n",
    "    #      1. Get probabilities for the next token for each hypothesis \n",
    "    #          (i.e. tensor of shape [B * BS, V])\n",
    "    #      2. Use those probabilities to compute probability for each extension of each hypothesis\n",
    "    #          (i.e. tensor of shape [B * BS, V])\n",
    "    #      3. For each element in the batch select new BS best hypotheses\n",
    "    #          Note, that some of the hypotheses on the previous step have been finished\n",
    "    #            so their probability should not change. So you have to select BS best hypotheses\n",
    "    #            among all extension of unfinished hypotheses and finished hypotheses\n",
    "    #          As a result you will have a new token for best extensions of unfinished hypotheses\n",
    "    #          For simplisity you can use <EOS> token if you select finished hypothesis in the beam\n",
    "    #            i.e. tensor of shape [B * BS] of indices for selected hypotheses and\n",
    "    #                 tensor of shape [B * BS] of extension tokens for each hypothesis\n",
    "    #      4. Update probabilities for each hypotheses and is_finished state for each hypothesis\n",
    "    #          Concat new tokens to the existing prefixes\n",
    "    #      5. Update hidden and cell state to correspond to the selected hypothesis\n",
    "\n",
    "    batch_size = start_tokens.shape[1]\n",
    "\n",
    "    eos_idx = model.vocab.get_stoi()['<eos>']\n",
    "    \n",
    "    # Get embeddings for the start tokens\n",
    "    embedding = model.word_embeddings(start_tokens)\n",
    "    \n",
    "    # Make forward pass through the RNN and \n",
    "    #   obtain the last cell and hidden state for each element in the batch\n",
    "    all_h, all_c = [], []\n",
    "    h = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    c = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n",
    "    for time_step in range(start_tokens.shape[0]):\n",
    "        _, (h, c) = model.rnn(embedding[time_step][None, :, :], (h, c))\n",
    "        all_h.append(h)\n",
    "        all_c.append(c)\n",
    "    \n",
    "    all_h = torch.stack(all_h, dim=1)\n",
    "    all_c = torch.stack(all_c, dim=1)\n",
    "\n",
    "    start_h = all_h[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])] # [L, B, H]\n",
    "    start_c = all_c[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])] # [L, B, H]\n",
    "\n",
    "    # Use those states as the initialization for each hypotheses in the beam\n",
    "    h = torch.cat([start_h] * beam_size, dim=1) # [L, B * BS, H]\n",
    "    c = torch.cat([start_c] * beam_size, dim=1) # [L, B * BS, H]\n",
    "    \n",
    "    # Select initial tokens for each hypotheses in the beam\n",
    "    #   Compute log probabilities and select top-beam_size tokens for each element\n",
    "    #   Use them to initialize beam search state\n",
    "    \n",
    "    \n",
    "    start_log_probas = F.log_softmax(model.linear(start_h[-1, :, :]), dim=-1) # [B, V]\n",
    "    log_probas, new_tokens = torch.topk(\n",
    "        input=start_log_probas,\n",
    "        k=beam_size,\n",
    "    ) # ([B, BS], [B, BS])\n",
    "    log_probas = log_probas.view((-1, )) # [B * BS]\n",
    "    new_tokens = new_tokens.view((-1, )) # [B * BS]\n",
    "    print(f\"DEB: {log_probas.shape=}\\t{new_tokens.shape=}\")\n",
    "    hypotesis = torch.clone(new_tokens.reshape((1, -1))) # [1, B * BS]\n",
    "    print(f\"DEB: \", end=\"\")\n",
    "    print_tensor(hypotesis)\n",
    "    \n",
    "    is_finished = h.new_zeros((batch_size * beam_size, ), dtype=torch.bool) # [B * BS]\n",
    "    decoded_lens = h.new_zeros((batch_size * beam_size, ), dtype=torch.int64) # [B * BS]\n",
    "\n",
    "    while not torch.all(is_finished) and hypotesis.shape[0] < max_generated_len:\n",
    "        # Update hidden and cell state to correspond to the selected hypothesis\n",
    "        embedding = model.word_embeddings(new_tokens)\n",
    "        _, (h, c) = model.rnn(embedding[None, :, :], (h, c))\n",
    "        \n",
    "        # Get probabilities for the next token for each hypothesis\n",
    "        next_token_log_probas = F.log_softmax(model.linear(h[-1, :, :]), dim=-1)  # [B * BS, V]\n",
    "        \n",
    "        # Use those probabilities to compute probability for each extension of each hypothesis\n",
    "        print(f\"DEB: {next_token_log_probas.shape=}\\t{log_probas.shape=}\")\n",
    "        # extension_log_probas = log_probas[:, None] + next_token_log_probas * (~is_finished)[:, None]  # [B * BS, V]\n",
    "        extension_log_probas = log_probas[:, None] + next_token_log_probas  # [B * BS, V]\n",
    "\n",
    "        # For each element in the batch select new BS best hypotheses\n",
    "        #   You can use loop over different beams\n",
    "        for batch_idx in range(batch_size):\n",
    "            indices_to_process = torch.topk(\n",
    "                input=extension_log_probas[batch_idx * beam_size:(batch_idx + 1) * beam_size].view((-1, )), \n",
    "                k=beam_size\n",
    "            ).indices\n",
    "            beam_idx = indices_to_process // extension_log_probas.shape[1]\n",
    "            token_idx = indices_to_process % extension_log_probas.shape[1]\n",
    "            \n",
    "            print(f\"Beam idx: {beam_idx}\\tToken idx: {token_idx}\\tRaw indices: {indices_to_process=}\")\n",
    "            \n",
    "            new_tokens[batch_idx * beam_size:(batch_idx + 1) * beam_size] = token_idx\n",
    "            \n",
    "            idx = batch_idx * beam_size + beam_idx\n",
    "            log_probas[idx] = extension_log_probas[idx, token_idx]\n",
    "            is_finished[idx] = is_finished[idx] | (token_idx == eos_idx)\n",
    "            \n",
    "        decoded_lens += (~is_finished)\n",
    "\n",
    "        # Update probabilities for each hypotheses and is_finished state and decoded_lens for each hypothesis\n",
    "\n",
    "        # Concat new tokens to the existing prefixes\n",
    "        hypotesis = torch.cat([torch.clone(hypotesis), torch.clone(new_tokens.reshape((1, -1)))], dim=0)\n",
    "        print(f\"DEB: \", end=\"\")\n",
    "        print_tensor(hypotesis)     \n",
    "        \n",
    "    return (\n",
    "        hypotesis.view(-1, start_tokens.shape[1], beam_size), \n",
    "        decoded_lens.view(start_tokens.shape[1], beam_size),\n",
    "        log_probas.view(start_tokens.shape[1], beam_size)\n",
    "    )"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:15.910480800Z",
     "start_time": "2024-04-15T20:35:15.826633400Z"
    }
   },
   "execution_count": 220,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start_tokens = torch.tensor([\n",
    "    lm_model.vocab.lookup_indices(['<sos>', '<pad>', '<pad>', '<pad>']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'my', 'favorite', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'best', 'movie']),\n",
    "    lm_model.vocab.lookup_indices(['<sos>', 'the', 'worst', 'movie']),\n",
    "]).T\n",
    "\n",
    "start_tokens_lens = torch.tensor([1, 4, 4, 4])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:16.106016400Z",
     "start_time": "2024-04-15T20:35:16.047503300Z"
    }
   },
   "execution_count": 221,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# lm_model.to(device).eval()\n",
    "# start_tokens = start_tokens.to(device)\n",
    "# start_tokens_lens = start_tokens_lens.to(device)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:16.263400Z",
     "start_time": "2024-04-15T20:35:16.217393100Z"
    }
   },
   "execution_count": 222,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lm_model.cpu().eval()\n",
    "start_tokens = start_tokens.cpu()\n",
    "start_tokens_lens = start_tokens_lens.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:16.605533500Z",
     "start_time": "2024-04-15T20:35:16.547254600Z"
    }
   },
   "execution_count": 223
  },
  {
   "cell_type": "code",
   "source": [
    "beam_size = 100\n",
    "# beam_size = 5\n",
    "decoded_tokens, decoded_lens, log_probas = decode_beam_search(\n",
    "    lm_model, start_tokens, start_tokens_lens, max_generated_len=10, beam_size=beam_size\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:52.686390Z",
     "start_time": "2024-04-15T20:35:50.149023100Z"
    }
   },
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEB: log_probas.shape=torch.Size([400])\tnew_tokens.shape=torch.Size([400])\n",
      "DEB: t.shape=torch.Size([1, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([ 2,  3, 18,  4,  4,  8,  0,  2, 18,  0,  3,  5,  0,  9,  4,  2,  1,  0,\n",
      "         1, 43, 30,  0, 16,  8,  0,  1,  0,  0,  3,  4,  0, 12,  0, 41,  2,  0,\n",
      "         0,  0,  7,  2,  1,  4,  2,  1, 32,  6,  3, 28,  2,  0,  7,  4,  0,  0,\n",
      "         0, 16,  3,  3,  7,  2, 39,  1,  4, 24,  4,  2,  0, 20,  1,  4,  3,  0,\n",
      "         3, 30,  6, 79, 36, 23,  0,  0, 19,  3,  3, 10, 40,  6,  1,  1,  2, 26,\n",
      "         0,  0, 17,  0,  7,  6,  2,  0,  0,  1])\tToken idx: tensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29])\tRaw indices: indices_to_process=tensor([  66734,  100066,  600485,  134615,  133472,  266831,       1,   67231,\n",
      "         600879,      93,  100174,  166766,      29,  301049,  133653,   66967,\n",
      "          33489,       7,   33878, 1434704, 1000721,     159,  533655,  266939,\n",
      "            689,   33354,     169,     136,  100060,  133419,     261,  400243,\n",
      "            426, 1367998,   66880,     131,    1023,     130,  233996,   66836,\n",
      "          33824,  133484,   66713,   45425, 1067303,  200643,  101781,  933891,\n",
      "          66837,     525,  234854,  133527,      91,    2059,     115,  533763,\n",
      "         100218,  100584,  233986,   66707, 1301292,   33484,  134818,  800479,\n",
      "         133413,   66799,    1496,  667067,   33446,  133505,  100189,      50,\n",
      "         100152, 1001115,  200807, 2635412, 1201939,  767644,     455,     471,\n",
      "         633714,  100087,  100300,  334055, 1334127,  200119,   34042,   33483,\n",
      "          68428,  867703,     305,     118,  567008,     882,  233602,  200287,\n",
      "          66821,     239,     510,   33382])\n",
      "Beam idx: tensor([ 5,  1,  3,  0,  2,  0,  5,  0, 31,  3,  1, 15,  4,  3, 12,  7,  7,  6,\n",
      "        18,  3,  0, 30,  2,  3,  4,  0, 11,  8, 12,  1, 58, 11, 23, 11,  6,  2,\n",
      "        10, 44, 10, 40,  2,  2, 27,  4,  7,  0, 30,  7, 19, 13,  3, 14, 51,  8,\n",
      "        13,  0,  7,  3, 75,  9, 79, 22, 10,  3, 39,  6,  1, 31,  0,  7,  0, 51,\n",
      "         4,  0, 86,  6,  2,  1,  7,  8,  4, 11, 27,  3,  0,  1,  2,  3, 32, 11,\n",
      "        43,  0, 11, 71, 23,  2, 26,  5,  0, 15])\tToken idx: tensor([  131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544])\tRaw indices: indices_to_process=tensor([ 166896,   33484,  100584,       1,   67231,       7,  167290,     115,\n",
      "        1034468,  101262,   33489,  502027,  133419,  100300,  400243,  233499,\n",
      "         233996,  200643,  600879,  100119,      93, 1000721,   66842,  100189,\n",
      "         133527,     159,  367572,  266831,  400351,   33878, 1934999,  367408,\n",
      "         767126,  366890,  200254,   66837,  333537, 1467675,  333531, 1334127,\n",
      "          66836,   66707,  901403,  133413,  233732,      29, 1001115,  233601,\n",
      "         634232,  433590,  100131,  466943, 1701031,  266939,  433596,     369,\n",
      "         233602,  103519, 2502000,  300178, 2635412,  733767,  333645,  105337,\n",
      "        1301292,  212190,   33354, 1034073,     426,  233478,     261, 1701528,\n",
      "         133505,    1023, 2868873,  200249,   67395,   33522,  233645,  266993,\n",
      "         133571,  367013,  901056,  100190,     371,   33382,   67177,  101465,\n",
      "        1067303,  366884, 1434186,     169,  366973, 2368304,  767234,   66799,\n",
      "         867185,  166901,      60,  500839])\n",
      "Beam idx: tensor([ 0,  2,  2,  0,  1,  0,  0, 19,  0, 15,  0,  0,  0,  7,  0,  0,  0,  6,\n",
      "        18,  0,  0,  4,  4,  0,  0,  0,  0,  3,  0,  0,  4, 12,  5,  3,  5, 26,\n",
      "        13,  0,  0,  0,  0,  3, 30,  0, 12, 24,  0,  1,  0,  0,  4,  0,  1,  0,\n",
      "         3,  0, 11,  0, 33,  0,  6,  9,  8,  0,  0,  0,  2,  0,  4,  0, 15,  7,\n",
      "         2,  0,  0,  5,  0,  4, 23,  0,  0,  3,  3, 19,  1,  9,  0,  4,  7,  0,\n",
      "         4, 25,  4,  0, 27, 11, 13, 10, 26,  0])\tToken idx: tensor([ 131,  131,  525,    7,    1,  136,  261,  525,  579,  525, 2059,   93,\n",
      "          29,  525,  169, 1891,  704,  525, 1732,    1,  115, 1203,   60,  159,\n",
      "         872,   95,   99,  525,  510,   91,  241,    7,    1,    7,    7,  131,\n",
      "          28,  305,  709,  130,  118,    1,  525,  426,  115,    7,  211,    7,\n",
      "          78,  230,    7, 1231,   93,  689,  130,  455,  525,  239,  143,  141,\n",
      "         136,    7,    7,  525,  769, 1023,  130,  502,   72,   26,  131,  136,\n",
      "         136,  371, 1469,  115,   50,  115,  525,  124, 1213,  241,   93,  130,\n",
      "         115,    1,  438, 1406,  131,   65,    1,   28,   93,   94,    7,  689,\n",
      "         261,  689,  525, 1123])\tRaw indices: indices_to_process=tensor([    131,   66837,   67231,       7,   33354,     136,     261,  634232,\n",
      "            579,  500820,    2059,      93,      29,  233996,     169,    1891,\n",
      "            704,  200643,  602086,       1,     115,  134615,  133472,     159,\n",
      "            872,      95,      99,  100584,     510,      91,  133653,  400243,\n",
      "         166766,  100066,  166772,  867309,  433617,     305,     709,     130,\n",
      "            118,  100060, 1001115,     426,  400351,  800479,     211,   33360,\n",
      "             78,     230,  133419,    1231,   33446,     689,  100189,     455,\n",
      "         367408,     239, 1100792,     141,  200254,  300184,  266831,     525,\n",
      "            769,    1023,   66836,     502,  133484,      26,  500426,  233607,\n",
      "          66842,     371,    1469,  166880,      50,  133527,  767644,     124,\n",
      "           1213,  100300,  100152,  633837,   33468,  300178,     438,  134818,\n",
      "         233602,      65,  133413,  833853,  133505,      94,  900538,  367572,\n",
      "         433850,  334219,  867703,    1123])\n",
      "Beam idx: tensor([ 0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  2,  0,  0,  0,  0,  6,  0,  2,  0,  0,  0,  0,  0,  0, 12,\n",
      "         4,  0, 11,  0,  0,  0,  0,  1,  3,  0, 33,  0,  4,  0,  0,  2,  1,  0,\n",
      "         6,  0,  0,  3,  0,  0,  6,  7,  0,  0,  0,  4,  0,  0,  0,  0,  3,  5,\n",
      "         2,  0,  0,  0,  0,  0,  7,  6,  0,  7,  0,  0, 86,  0, 20,  0,  2,  0,\n",
      "         0,  3,  0,  0,  0,  0,  2,  0,  0,  0])\tToken idx: tensor([ 131,  131,  525,    7,  136,  261,  579, 2059,   93,   29,  169, 1891,\n",
      "         704,    1,  115,  159,  872,   95,   99,  510,   91,    1,  305,  709,\n",
      "         130,  118,  525,  426,    7,  211,   78,  230, 1231,  689,  455,    7,\n",
      "           1,  239,  525,  141,  525,  769, 1023,  136,  525,  502, 1732,   26,\n",
      "           7,  371, 1469,  115,  130,   50,   60,  124, 1213,    7,  438,   65,\n",
      "        1203,  689,   94, 1123, 4740,  115,  190,  235,  158,   40,    1,  525,\n",
      "          93,   28, 1203,  447,  439,  518,  525,  241,  206,    7,   87, 3236,\n",
      "         887,  428,    7,  719, 1722,  877, 1804,  130, 1893,  204, 1604,   60,\n",
      "         241, 1385, 2416, 1751])\tRaw indices: indices_to_process=tensor([    131,   33484,   33878,       7,     136,     261,     579,    2059,\n",
      "             93,      29,     169,    1891,     704,       1,     115,     159,\n",
      "            872,      95,      99,     510,      91,   66707,     305,     709,\n",
      "            130,     118,  200643,     426,   66713,     211,      78,     230,\n",
      "           1231,     689,     455,  400243,  133413,     239,  367408,     141,\n",
      "            525,     769,    1023,   33489,  100584,     502, 1102381,      26,\n",
      "         133419,     371,    1469,   66821,   33483,      50,  200178,     124,\n",
      "           1213,  100066,     438,      65,  201321,  234160,      94,    1123,\n",
      "           4740,  133527,     190,     235,     158,      40,  100060,  167290,\n",
      "          66799,      28,    1203,     447,     439,     518,  233996,  200359,\n",
      "            206,  233478,      87,    3236, 2869245,     428,  667067,     719,\n",
      "          68428,     877,    1804,  100189,    1893,     204,    1604,      60,\n",
      "          66947,    1385,    2416,    1751])\n",
      "DEB: t.shape=torch.Size([2, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([37, 18, 70, 88, 14, 38, 87, 46, 15, 11, 33, 75, 77, 49, 87, 22, 41, 73,\n",
      "        60, 45, 83, 21, 70, 52, 57, 31, 25, 15, 13, 13, 14, 35, 39, 53, 98, 37,\n",
      "        47, 21, 62, 21, 89, 43, 14, 42, 82, 56, 33, 14, 11, 38, 51, 48, 69, 56,\n",
      "        64, 67, 39, 15, 27, 22, 15, 63, 65, 72, 76, 68, 68, 59,  7, 35, 68, 52,\n",
      "        49, 51, 65, 69, 85, 75, 15, 35, 62, 46, 35, 86, 54, 25, 69, 62, 29, 61,\n",
      "        77, 32, 74, 19, 51, 65, 56, 11, 45, 35])\tToken idx: tensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241])\tRaw indices: indices_to_process=tensor([1234192,  600485, 2334841, 2936796,  467467, 1267545, 2901842, 1535970,\n",
      "         500302,  366884, 1100780, 2501606, 2568312, 1634428, 2902236,  734291,\n",
      "        1367998, 2434900, 2001311, 1501016, 2768430,  700441, 2335235, 1735228,\n",
      "        1901252, 1034468,  833826,  500410,  433596,  433590,  467073, 1167362,\n",
      "        1300898, 1767716, 3268725, 1234586, 1568116,  700420, 2067893,  700674,\n",
      "        2968548, 1434704,  467072, 1401351, 2735471, 1868029, 1100785,  467078,\n",
      "         366890, 1267550, 1701528, 1600951, 2301598, 1867775, 2134593, 2235176,\n",
      "        1301292,  500296,  900532,  733902,  502017, 2101764, 2168005, 2401476,\n",
      "        2534835, 2268245, 2268064, 1967828,  233602, 1167356, 2269207, 1735060,\n",
      "        1634433, 1701134, 2169148, 2302560, 2835006, 2501611,  500454, 1167443,\n",
      "        2068127, 1534782, 1167470, 2869047, 1801587,  833832, 2301417, 2068001,\n",
      "         967238, 2034534, 2568317, 1067303, 2468811,  633838, 1701139, 2168186,\n",
      "        1867796,  367124, 1501021, 1167596])\n",
      "Beam idx: tensor([16, 47, 17, 29, 67, 46, 47, 67, 81, 82, 18, 48, 25, 20, 50, 60, 35, 76,\n",
      "        68, 63, 21, 30, 70, 81, 16, 56, 17, 58, 41, 18, 34, 72, 29, 38, 64, 70,\n",
      "        87, 87, 20, 49, 40, 46, 95, 25, 21, 68, 33, 72, 54, 80, 82, 42, 59, 56,\n",
      "        62, 35, 45, 35, 72, 66, 76, 31, 17, 34, 20, 83, 20, 80, 61, 84, 25, 17,\n",
      "        50, 80, 24, 52, 23, 73, 94, 95,  6, 77, 63, 38, 97, 89, 57, 17, 25, 35,\n",
      "        95, 50, 31, 73, 63, 16, 90, 17, 34, 63])\tToken idx: tensor([ 131,  131,  131,  131,  131,  131,  525,  525,  131,  131,  131,  131,\n",
      "          28,   60,  525,  131,    7,    7,    7,  525,    7,  131,    7,  525,\n",
      "         136,    7,  136,  131,    1,  525,    7, 1203,  136,    1,  131,  115,\n",
      "           7,  241, 1203,    1,  131,  136, 1203,  261,    1,  115,  689,   60,\n",
      "         525,  261,  136,    1,    1,  115,  525,  241,  169,    1,  241,    1,\n",
      "         115,  131,  169,    1,  241,    7,    1,   28,    1,    7,    7,   95,\n",
      "         136,    7,    1,    7,  131,    7,  525,  525,  131,    1,    7,    7,\n",
      "           7,    1,    1,  704,  174,  115,  241,  131,  525,    1,  130,  525,\n",
      "         525,   99,  115,  131])\tRaw indices: indices_to_process=tensor([ 533779, 1567722,  567132,  967368, 2234782, 1534369, 1568116, 2235176,\n",
      "        2701724, 2735077,  600485, 1601075,  833853,  667120, 1668175, 2001311,\n",
      "        1167362, 2534835, 2268011, 2101764,  700420, 1000721, 2334717, 2702118,\n",
      "         533784, 1867775,  567137, 1934605, 1367474,  600879, 1134009, 2402619,\n",
      "         967373, 1267415, 2134723, 2334825, 2901718, 2901952,  668263, 1634298,\n",
      "        1334251, 1534374, 3169738,  834086,  700414, 2268119, 1101338, 2401476,\n",
      "        1801587, 2668501, 2735082, 1400827, 1967828, 1867883, 2068411, 1167596,\n",
      "        1501054, 1167356, 2401657, 2201299, 2534943, 1034074,  567170, 1134003,\n",
      "         667301, 2768306,  667061, 2668268, 2034534, 2801659,  833832,  567096,\n",
      "        1667786, 2668247,  800473, 1734363,  767250, 2434776, 3135707, 3169060,\n",
      "         200249, 2568182, 2101246, 1267421, 3235248, 2968418, 1901122,  567705,\n",
      "         833999, 1167470, 3168776, 1667781, 1034468, 2434770, 2101369,  534173,\n",
      "        3002295,  567100, 1134117, 2101370])\n",
      "Beam idx: tensor([54, 39, 83, 66, 39, 42, 17, 21, 63, 16, 66, 28, 21,  2, 16, 54, 14, 29,\n",
      "        98, 78, 14, 22, 81, 16, 60, 30, 47, 17, 40, 41, 48, 56, 43, 32, 48, 21,\n",
      "        21,  7, 14, 40, 88, 21, 35, 83, 42, 14, 96, 34, 96, 19, 50, 52, 59, 28,\n",
      "        51, 80, 31, 53, 57, 82, 22, 16, 42, 87, 63, 31, 88, 64, 65, 35, 94, 72,\n",
      "        82, 29, 52, 90, 71, 70, 81, 60, 29, 21, 78, 92, 52, 13, 99, 36, 17, 98,\n",
      "        73, 24, 63, 85, 32, 38, 17, 27, 28, 49])\tToken idx: tensor([ 131,  131,  131,  131,  525,  131,  131,  525,  131,    7,  525,  131,\n",
      "           7,  131,  115,  525,    7,  872,  131,  131,    1,  525,  525,    1,\n",
      "           7,  525,  525,  136,  515,    1,    7,  131,    7,    1,  115,  131,\n",
      "         130,  131,  115, 1383,    7,  241,    1,  525,  136,  241,  115,  525,\n",
      "           7,  525,  525,   60,    7, 1891,    7,    7,  525,    7,    7,   60,\n",
      "           1,   93,  525,    7,  136,  689,  115,    7,    7,    7,  525,    7,\n",
      "        1203,   95,  241,    1,    7,    1,  131,  115,  211,  115,  136,   60,\n",
      "        1203,  131,    7,    7,    1,  136,    1,    7,  525,    1,    7,  872,\n",
      "         169,  131,  579,  639])\tRaw indices: indices_to_process=tensor([1801193, 1300898, 2768430, 2201429, 1301292, 1400957,  567132,  700938,\n",
      "        2101370,  533655, 2201823,  934015,  700420,   66837,  533763, 1801587,\n",
      "         466949,  968109, 3268725, 2601665,  466943,  734291, 2702118,  533649,\n",
      "        2001187, 1001115, 1568116,  567137, 1334635, 1367474, 1600951, 1867899,\n",
      "        1434186, 1067297, 1601059,  700544,  700543,  233602,  467057, 1335503,\n",
      "        2935071,  700654, 1167356, 2768824, 1400962,  467183, 3202003, 1134527,\n",
      "        3201895,  634232, 1668175, 1734416, 1967834,  935775, 1701010, 2668247,\n",
      "        1034468, 1767716, 1901128, 2735006,  733767,  533741, 1401351, 2901718,\n",
      "        2101375, 1034632, 2935179, 2134599, 2167952, 1167362, 3135707, 2401423,\n",
      "        2736149,  967332, 1734597, 3001771, 2368070, 2334711, 2701724, 2001295,\n",
      "         967448,  700528, 2601670, 3068536, 1735559,  433720, 3301954, 1200715,\n",
      "         567002, 3268730, 2434770,  800479, 2101764, 2835006, 1067303, 1268286,\n",
      "         567170,  900662,  934463, 1634936])\n",
      "Beam idx: tensor([24, 26, 19, 15,  8,  8, 38, 52, 35, 27, 13, 18, 91, 80, 17, 16, 15,  8,\n",
      "         8, 74, 60, 88, 78, 40, 10, 12, 37, 71,  9, 44, 17, 91, 10, 27, 16, 25,\n",
      "        19, 21, 17,  9,  9, 54,  8, 42, 18,  9, 67, 14, 23, 38,  8, 51, 17, 95,\n",
      "         9,  8, 99, 36, 26, 30, 79, 24, 76, 26, 18, 95,  9, 13,  8, 25, 10, 30,\n",
      "        15,  8, 15, 16,  8, 17, 34, 28, 14, 12,  7, 61, 23, 62, 19, 12, 68, 15,\n",
      "         9,  8, 16, 60,  8, 64, 32, 74, 61, 10])\tToken idx: tensor([ 131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\tRaw indices: indices_to_process=tensor([ 800603,  867309,  633838,  500323,  268027,  266884, 1267545, 1734487,\n",
      "        1167880,  900538,  433590,  600361, 3035254, 2668755,  567008,  533655,\n",
      "         500302,  267065,  267349, 2468647, 2001187, 2936796, 2601665, 1334251,\n",
      "         333531,  400243, 1234068, 2368194,  300346, 1467663,  567116, 3035648,\n",
      "         333537,  900646,  533763,  835208,  634232,  700414,  567242,  300184,\n",
      "         300178, 1801587,  266954, 1400833,  600469,  300866, 2234658,  466943,\n",
      "         767120, 1267550,  268230, 1701528,  567002, 3169060,  300603,  266917,\n",
      "        3302472, 1200709,  868050, 1000618, 2635412,  800997, 2534835,  867314,\n",
      "         600355, 3168776,  300206,  433596,  266825,  834340,  333645, 1000597,\n",
      "         500556,  266896,  500820,  533649,  272102,  567073, 1134171,  934573,\n",
      "         467467,  400477,  233478, 2034540,  767126, 2067914,  633843,  400351,\n",
      "        2268011,  500410,  300270,  270072,  533909, 2001295,  266831, 2134599,\n",
      "        1067303, 2468363, 2035222,  334219])\n",
      "DEB: t.shape=torch.Size([3, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([44, 55, 50, 84, 41, 70, 34, 55, 44, 60, 87,  4, 34, 78, 87, 71, 58, 91,\n",
      "        42, 50, 34, 12, 99, 95, 93,  5, 84, 94, 66, 70, 43, 88,  4, 92, 14, 80,\n",
      "        99, 92, 78, 24, 41,  3, 18, 78, 94, 93,  9, 18, 37,  8, 99, 20, 66, 94,\n",
      "        81, 91,  9, 80, 80, 55, 80, 50, 90, 91, 22, 30, 50, 20, 97, 50, 12, 66,\n",
      "        90, 34, 16, 42, 88, 81,  8, 44, 24, 97, 93, 99, 37, 44, 30, 58, 78,  5,\n",
      "        99, 95, 18, 34,  3, 71, 75, 94, 95, 50])\tToken idx: tensor([ 131,  131,  131,  131,  131,  525,    7,  136,  136, 1732,  525,  131,\n",
      "           1,   28,  131,    7,    1,  525,  131,  136,  115,    7,  525,  525,\n",
      "           7,    1,  136,    7,    1,  131,  131, 1732,  525,  689,  131,  131,\n",
      "         169,    7,    7,    7,  525,  525,  131,  261,    1,    1,    1,    7,\n",
      "         131,  131,  131,    7,    7,  115,    1,  136,  241,  136,  169,  525,\n",
      "         525,   95,    7,  131,  131,    7,  169,  115,  169,   99,  115,  115,\n",
      "           1,   93,  131,  525,    1,    7,  136,  169,  115,  131,  115,  689,\n",
      "         525,  525,  115,    7,  115,    7,  136,  130,    1,  159,    7,    1,\n",
      "         131,  241,    1,  525])\tRaw indices: indices_to_process=tensor([1467663, 1834546, 1667781, 2801783, 1367604, 2335235, 1134009, 1834551,\n",
      "        1467668, 2002912, 2902236,  133543, 1134003, 2601562, 2901842, 2368070,\n",
      "        1934475, 3035648, 1400957, 1667786, 1134117,  400243, 3302472, 3169060,\n",
      "        3101836,  166766, 2801788, 3135189, 2201299, 2334841, 1434310, 2936796,\n",
      "         133937, 3069165,  467073, 2668371, 3302116, 3068483, 2601541,  800479,\n",
      "        1367998,  100584,  600485, 2601795, 3135183, 3101830,  300178,  600361,\n",
      "        1234192,  266955, 3302078,  667067, 2201305, 3135297, 2701594, 3035259,\n",
      "         300418, 2668376, 2668409, 1834940, 2668765, 1667745, 3001777, 3035254,\n",
      "         733897, 1000597, 1667819,  667175, 3235410, 1667749,  400351, 2201413,\n",
      "        3001771, 1134095,  533779, 1401351, 2935065, 2701600,  266960, 1467701,\n",
      "         800587, 3235372, 3101944, 3302636, 1234586, 1468057, 1000705, 1934481,\n",
      "        2601649,  166772, 3302083, 3168665,  600355, 1134161,  100066, 2368064,\n",
      "        2501606, 3135423, 3168536, 1668175])\n",
      "Beam idx: tensor([53, 47, 96, 78, 48, 98, 28, 19, 92, 29, 67, 67, 91, 37, 99,  7, 85, 74,\n",
      "        96, 55, 78, 58, 23, 14, 36, 55, 47, 37, 93, 55,  6, 30, 91, 92, 99, 55,\n",
      "        94, 69, 36, 36, 37, 31, 37, 91, 69, 19, 53, 79, 54, 37, 65, 18, 85, 91,\n",
      "        53, 36, 37, 65, 98, 37, 36, 18,  9, 69, 43, 36, 47, 99, 92, 28, 95, 64,\n",
      "        53, 99, 28, 44, 65, 55, 92, 88, 44, 36, 58, 69, 36, 55, 36, 14, 37, 47,\n",
      "        49, 96, 78, 13, 51,  9, 55, 69, 86, 28])\tToken idx: tensor([12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426])\tRaw indices: indices_to_process=tensor([1779781, 1568116, 3202019, 2601665, 1601075, 3269119,  933885,  633838,\n",
      "        3068607,  967368, 2235176, 2234782, 3035130, 1234586, 3301954,  233602,\n",
      "        2835006, 2468123, 3202024, 1834940, 2601670, 1934605,  767250,  467073,\n",
      "        1200844, 1834584, 1567722, 1234192, 3101830, 1834551,  200249, 1000721,\n",
      "        3035124, 3068612, 3301948, 1834546, 3135313, 2301493, 1201233, 1201397,\n",
      "        1234197, 1034468, 1234230, 3035364, 2302046,  634232, 1782108, 2635018,\n",
      "        1801193, 1234191, 2168470,  600879, 2835012, 3035238, 1767845, 1200737,\n",
      "        1234154, 2167946, 3268595, 1234062, 1200709,  600485,  300184, 2301882,\n",
      "        1434186, 1200839, 1567721, 3302062, 3069165,  933977, 3168666, 2134723,\n",
      "        1768180, 3302188,  933999, 1467539, 2168081, 1835104, 3068645, 2935065,\n",
      "        1467647, 1201179, 1934999, 2301526, 1200801, 1834416, 1200877,  467078,\n",
      "        1234090, 1567832, 1634304, 3202413, 2601703,  434114, 1701004,  300418,\n",
      "        1834444, 2301386, 2868359,  934310])\n",
      "Beam idx: tensor([62, 84, 20, 37, 62, 74, 15, 74, 61,  7, 26, 39, 56, 39, 66, 97, 77, 54,\n",
      "        61, 84, 66, 67, 47, 84, 54, 93, 75, 37, 18, 45,  4, 37, 84, 45, 44, 44,\n",
      "        84, 45, 83, 79, 55, 68, 55, 95, 68, 95, 18, 18, 30, 18, 61, 89, 74, 45,\n",
      "        21, 20, 55, 67, 20,  6, 46, 10, 50, 62, 61, 95, 11, 44, 12, 36,  0, 91,\n",
      "        22,  5, 97, 58, 67, 76, 44, 58, 84, 79, 25, 61, 67, 97, 61, 37, 46, 46,\n",
      "        68, 37, 37, 91, 33, 74,  8, 37, 45, 69])\tToken idx: tensor([ 131,  525,    1,    7,  136,  131,  131,  525,   60,  131,  131,  131,\n",
      "         131,  525,  525,    7,    1,  525, 1203,  130,  131,  525,  131,    7,\n",
      "         131,    1,    1,    1,    7,  136,  131,  115,  131,  525,    1,  115,\n",
      "         115,  131,  525,  525,  136,  525,  525,    1,  136,    7,    1, 1722,\n",
      "         525,  115,  525,    7,  136,  169,  131,  115,  169,  136,   93,    7,\n",
      "           1,  131,  131,  525,    1,  689,  241,    7,  525,  131,    7,  136,\n",
      "         131,    7,    1,    1,    1,  136,   93,  136,  241,  136,  131, 1406,\n",
      "         689,  689,  241,  241,  136,  689,  131, 1722,   28,  525,    1,  130,\n",
      "           7,  159,    1,    1])\tRaw indices: indices_to_process=tensor([2068017, 2802177,  667061, 1234068, 2068022, 2468253,  500426, 2468647,\n",
      "        2034593,  233602,  867309, 1300898, 1867899, 1301292, 2201823, 3235248,\n",
      "        2568182, 1801587, 2035736, 2801782, 2201429, 2235176, 1567722, 2801659,\n",
      "        1801193, 3101830, 2501476, 1234062,  600361, 1501021,  133543, 1234176,\n",
      "        2801783, 1501410, 1467533, 1467647, 2801767, 1501016, 2768824, 2635412,\n",
      "        1834551, 2268529, 1834940, 3168536, 2268140, 3168542,  600355,  602076,\n",
      "        1001115,  600469, 2035058, 2968424, 2468258, 1501054,  700544,  667175,\n",
      "        1834584, 2234787,  667153,  200125, 1534239,  333661, 1667781, 2068411,\n",
      "        2034534, 3169224,  367124, 1467539,  400761, 1200839,       7, 3035259,\n",
      "         733897,  166772, 3235242, 1934475, 2234652, 2534964, 1467625, 1934610,\n",
      "        2801893, 2635023,  833956, 2035939, 2235340, 3235930, 2034774, 1234302,\n",
      "        1534374, 1534927, 2268135, 1235783, 1234089, 3035648, 1100650, 2468252,\n",
      "         266831, 1234220, 1500886, 2301358])\n",
      "Beam idx: tensor([31, 41, 53, 56, 22, 29,  4, 94, 50, 31, 55, 45, 22, 91, 92, 41, 72, 49,\n",
      "        98, 92, 55, 65, 53, 48, 66, 29, 57, 63,  4, 45, 55,  1, 82, 72, 46, 47,\n",
      "         5, 29, 58, 19,  1, 81, 87,  0, 39, 70, 22, 75, 51, 55, 66, 50, 42, 90,\n",
      "         4, 83, 65, 95, 90, 11, 49, 46, 97, 65, 72, 80, 58, 50, 70, 31, 74, 66,\n",
      "        39, 60, 56, 38, 72, 55, 31, 22, 31, 43, 98, 47, 45, 46, 63, 45,  1, 43,\n",
      "        66, 58, 27, 90, 93, 52, 39, 81, 81, 39])\tToken idx: tensor([  131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\tRaw indices: indices_to_process=tensor([1034074, 1367604, 1767840, 1867899,  733773,  967244,  133419, 3135707,\n",
      "        1667657, 1034079, 1834656, 1500892,  733767, 3035648, 3068483, 1367609,\n",
      "        2401531, 1634304, 3268601, 3068591, 1835618, 2168470, 1767845, 1600945,\n",
      "        2201467,  967238, 1901122, 2101246,  133653, 1501574, 1834475,   33360,\n",
      "        2735471, 2401423, 1534763, 1567592,  166772,  967352, 1934481,  633838,\n",
      "          33441, 2702118, 2902236,       7, 1301292, 2346782,  733881, 2501476,\n",
      "        1701134, 1834940, 2201987, 1667765, 1400957, 3001830,  133527, 2768824,\n",
      "        2168081, 3169060, 3002011,  367408, 1634298, 1534374, 3235766, 2168076,\n",
      "        2401657, 2668371, 1934475, 1667651, 2334711, 1034112, 2468253, 2201724,\n",
      "        1300903, 2001311, 1867904, 1267545, 2401488, 1834487, 1034632,  733859,\n",
      "        1034468, 1434704, 3268709, 1567598, 1501000, 1534369, 2101240, 1500886,\n",
      "          33468, 1434180, 2201324, 1934589,  900538, 3002973, 3102518, 1734487,\n",
      "        1300936, 2701729, 2701724, 1300898])\n",
      "DEB: t.shape=torch.Size([4, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      "\ttensor([  131,   131,   131,   131,   131,   525,     7,   136,   136,  1732,\n",
      "          525,   131,     1,    28,   131,     7,     1,   525,   131,   136,\n",
      "          115,     7,   525,   525,     7,     1,   136,     7,     1,   131,\n",
      "          131,  1732,   525,   689,   131,   131,   169,     7,     7,     7,\n",
      "          525,   525,   131,   261,     1,     1,     1,     7,   131,   131,\n",
      "          131,     7,     7,   115,     1,   136,   241,   136,   169,   525,\n",
      "          525,    95,     7,   131,   131,     7,   169,   115,   169,    99,\n",
      "          115,   115,     1,    93,   131,   525,     1,     7,   136,   169,\n",
      "          115,   131,   115,   689,   525,   525,   115,     7,   115,     7,\n",
      "          136,   130,     1,   159,     7,     1,   131,   241,     1,   525,\n",
      "        12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426,\n",
      "          131,   525,     1,     7,   136,   131,   131,   525,    60,   131,\n",
      "          131,   131,   131,   525,   525,     7,     1,   525,  1203,   130,\n",
      "          131,   525,   131,     7,   131,     1,     1,     1,     7,   136,\n",
      "          131,   115,   131,   525,     1,   115,   115,   131,   525,   525,\n",
      "          136,   525,   525,     1,   136,     7,     1,  1722,   525,   115,\n",
      "          525,     7,   136,   169,   131,   115,   169,   136,    93,     7,\n",
      "            1,   131,   131,   525,     1,   689,   241,     7,   525,   131,\n",
      "            7,   136,   131,     7,     1,     1,     1,   136,    93,   136,\n",
      "          241,   136,   131,  1406,   689,   689,   241,   241,   136,   689,\n",
      "          131,  1722,    28,   525,     1,   130,     7,   159,     1,     1,\n",
      "          131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([23, 96, 41, 70, 84, 82, 96, 60, 83, 28, 96, 10, 91, 22, 83, 36, 62, 87,\n",
      "         0, 83, 96, 26, 85, 56,  2,  2, 73, 75, 23, 65, 57, 17, 79, 73, 10, 96,\n",
      "        59, 96, 73,  2, 98,  1, 26,  6, 74, 79, 13, 74, 76, 87,  0, 57, 54,  1,\n",
      "        33, 99, 40, 68, 96, 96,  2,  2,  1,  0, 86, 40, 17, 36,  0, 39, 49, 38,\n",
      "        36, 85, 64, 15, 19, 13, 40, 63, 72, 29, 73, 73, 70, 73, 11, 96, 43, 91,\n",
      "        25, 79, 96, 23, 38, 40, 61, 96, 26, 11])\tToken idx: tensor([  131,   115,   131,   525,   131, 12072,     7,   131,   689,     1,\n",
      "            1,   131,   131,   131,     7,     7,   525,   525,     7,    90,\n",
      "         1722,     1,   131,   131,    88,     7,   241,   131,   136,   525,\n",
      "            1,   131,     1,  1203,   136,    93,   131,   159,    60,   115,\n",
      "            1,    88,     7,     1,   241,     7,     7,     7,     1,   131,\n",
      "          115,     7,     1,     1,   689,   131,     1,   241,    88,    28,\n",
      "         1722,     1,     7,     1,   525,   131,   136,     1,    88,   525,\n",
      "            7,   136,   115,   136,     1,   131,     7,     1,     7,     7,\n",
      "            1,     1,     7,    72,   131,     1,     7,   261,     7,   525,\n",
      "            1,   115,    78,   169,   131,   115,     7,    87,   115,     1])\tRaw indices: indices_to_process=tensor([ 767250, 3202003, 1367604, 2335235, 2801783, 2747018, 3201895, 2001311,\n",
      "        2768988,  933885, 3201889,  333661, 3035254,  733897, 2768306, 1200715,\n",
      "        2068411, 2902236,       7, 2768389, 3203610,  867179, 2835136, 1867899,\n",
      "          66794,   66713, 2435010, 2501606,  767255, 2168470, 1901122,  567132,\n",
      "        2634888, 2435972,  333666, 3201981, 1967958, 3202047, 2434829,   66821,\n",
      "        3268595,   33441,  867185,  200119, 2468363, 2634894,  433596, 2468129,\n",
      "        2534829, 2901842,     115, 1901128, 1801063,   33354, 1101338, 3302078,\n",
      "        1334121, 2268245, 3201976, 3201916,   68428,   66707,   33360,       1,\n",
      "        2868883, 1334251,  567137, 1200709,      88, 1301292, 1634304, 1267550,\n",
      "        1200823, 2835141, 2134593,  500426,  633714,  433590, 1334127, 2101246,\n",
      "        2401417,  967238, 2434776, 2434841, 2334841, 2434770,  366890, 3202149,\n",
      "        1434186, 3035648,  833826, 2635002, 3201966,  767288, 1267545, 1334235,\n",
      "        2034540, 3201975,  867293,  366884])\n",
      "Beam idx: tensor([82, 66,  5, 38, 19, 71,  8, 45, 63, 59, 53,  4, 11, 26, 81, 56, 40, 87,\n",
      "        41, 39, 38, 27, 50, 66, 21,  0, 93, 82, 68, 76,  3, 15,  3,  2, 67, 60,\n",
      "         0,  8, 22, 42, 12, 42,  4,  1, 48, 32, 15, 57, 10,  3, 71, 87, 21, 39,\n",
      "        95, 39,  0, 13, 12, 75,  5, 11, 39, 49, 12, 56, 11,  4, 27, 26, 45,  2,\n",
      "        75, 71, 68, 67, 33, 61, 12,  8, 40, 94, 70, 94,  3, 84,  2, 75, 10, 41,\n",
      "        27, 61, 90, 87, 70, 26, 12, 31, 76, 21])\tToken idx: tensor([  131,   131,   131,   131,   131,     7,     7,   131,   131,     1,\n",
      "        12072,     7,     7,     7,     1,   241,     7,     7,   131,   689,\n",
      "          525,     7,   131,   525,     7,     7,   131,   136,     7,     7,\n",
      "            1,     7,     7,     7,   525,     1,   115,   115,     1,   241,\n",
      "          136,     7,   115,   131,     7,     1,    88,     1,   131,   241,\n",
      "          115,   241,    88,     1,   131,     7,     1,   131,   689,   525,\n",
      "          136,    88,    90,   131,     1,    72,   115,     1,    88,   115,\n",
      "          136,     1,   689,     1,     1,   131,     1,     7,    29,     1,\n",
      "          115,   131,     7,   525,   115,    60,    88,   136,     7,   136,\n",
      "            1,     1,   525,   115,  1722,     1,   525,   241,   115,     1])\tRaw indices: indices_to_process=tensor([2735077, 2201429,  166896, 1267545,  633838, 2368070,  266831, 1501016,\n",
      "        2101370, 1967828, 1779781,  133419,  366890,  867185, 2701594, 1868009,\n",
      "        1334127, 2901718, 1367604, 1301456, 1267939,  900538, 1667781, 2201823,\n",
      "         700420,       7, 3101960, 2735082, 2268011, 2534835,  100060,  500302,\n",
      "         100066,   66713, 2235176, 2001181,     115,  266939,  733767, 1401067,\n",
      "         400372, 1400833,  133527,   33484, 1600951, 1067297,  500383, 1901122,\n",
      "         333661,  100300, 2368178, 2901952,  700501, 1300768, 3168666, 1300774,\n",
      "              1,  433720,  400925, 2502000,  166901,  366971, 1300857, 1634428,\n",
      "         400237, 1867840,  366998,  133413,  900619,  867293, 1501021,   66707,\n",
      "        2502164, 2368064, 2268005, 2234782, 1100650, 2034540,  400265,  266825,\n",
      "        1334235, 3135313, 2334717, 3135707,  100174, 2801712,   66794, 2501611,\n",
      "         333537, 1367609,  900532, 2034534, 3002295, 2901826, 2336432,  867179,\n",
      "         400761, 1034184, 2534943,  700414])\n",
      "Beam idx: tensor([ 7, 86, 41, 86, 39, 86, 19, 19, 42, 63, 48, 21, 86, 51, 14, 50, 38, 66,\n",
      "         2, 86, 59, 87, 47, 66, 15, 17, 86, 23, 13, 95, 41, 72, 72, 86, 86, 86,\n",
      "        90, 91,  9, 23, 14,  3,  2, 86, 60,  1,  1,  9, 68, 23, 48, 15, 65, 86,\n",
      "        83, 65, 42,  1, 86, 65, 75, 53, 93, 43, 72, 85, 86, 36, 16, 95, 86, 63,\n",
      "        86, 18, 64, 86, 86, 88, 99,  3, 78, 88,  1, 70, 70, 38, 34, 72, 86, 86,\n",
      "        33, 77, 82, 83, 15, 86,  2, 49, 86, 29])\tToken idx: tensor([ 131,  525,  131,  131,  131,  136,  525,  131,  131,  131,  131,  131,\n",
      "         169,  525,  131,  131,  131,  525,    1,  689,  525,  525, 1732,  131,\n",
      "         131,  131,  130,  525,  131,  131,  136,  241,    7,   29,   93,    1,\n",
      "           1, 1732,    1,    1,  136,  689,    7,  230,    1,    1,  131,    7,\n",
      "         131,  136,  136,  136,    7,  426,  241,  689,  136,    7,   28,   90,\n",
      "           1,    7,  131,    1,  115,    7,  211,  131,    1,  525,  455,  136,\n",
      "          99,    7,    1,  471,   95,    1,    1,    1,  241,    7,  136,  131,\n",
      "         136,  136,    1,   72,   91,  510,  131,    1,  241,  525,  525,   76,\n",
      "         115,  131,  502,    1])\tRaw indices: indices_to_process=tensor([ 233602, 2868883, 1367604, 2868489, 1300898, 2868494,  634232,  633838,\n",
      "        1400957, 2101370, 1601075,  700544, 2868527, 1701528,  467073, 1667781,\n",
      "        1267545, 2201823,   66707, 2869047, 1968352, 2902236, 1569323, 2201429,\n",
      "         500426,  567132, 2868488,  767644,  433720, 3168666, 1367609, 2401657,\n",
      "        2401423, 2868387, 2868451, 2868359, 3001771, 3036855,  300178,  767120,\n",
      "         467078,  100748,   66713, 2868588, 2001181,   33354,   33484,  300184,\n",
      "        2268135,  767255, 1601080,  500431, 2167952, 2868784, 2768540, 2168634,\n",
      "        1400962,   33360, 2868386, 2168035, 2501476, 1767716, 3101960, 1434180,\n",
      "        2401531, 2835012, 2868569, 1200839,  533649, 3169060, 2868813, 2101375,\n",
      "        2868457,  600361, 2134593, 2868829, 2868453, 2935065, 3301948,  100060,\n",
      "        2601775, 2935071,   33489, 2334841, 2334846, 1267550, 1134003, 2401488,\n",
      "        2868449, 2868868, 1100780, 2568182, 2735187, 2768824,  500820, 2868434,\n",
      "          66821, 1634428, 2868860,  967238])\n",
      "Beam idx: tensor([59, 44, 20, 59, 41, 91, 69, 20, 89, 77, 85, 69, 42, 21,  2, 77, 85, 33,\n",
      "        86, 20, 73, 73,  2, 77, 34, 20, 69, 77, 33, 35,  3, 84, 80, 85, 62,  2,\n",
      "        73, 59, 69, 59, 96, 44, 20,  2, 78, 59,  6,  3, 32, 33, 59, 59, 73,  4,\n",
      "        17, 33, 84, 59, 59, 84, 19,  8, 57, 76, 99, 13, 96, 55, 81,  6, 79, 33,\n",
      "        73, 59, 96, 33, 79, 84, 73, 84, 69, 69, 77, 86, 77, 54, 37, 96, 91, 78,\n",
      "        59, 59, 59, 23, 85, 20, 59, 77, 85,  6])\tToken idx: tensor([ 131,  131,  525,  136,  131,  525,    7,    7,    1,  131,    7,  241,\n",
      "         131,  131,    7,  136,    1,  689,    1,  130,    1,    7,   88,  525,\n",
      "         131,  131,    1,  169,  136,    1,    7,  136,  131,  115,  131,    1,\n",
      "         115,   95,  115,   99,    1,  136,  241,  115,    7,  169,    1,    1,\n",
      "         131,  525,  525,  211,   93,  525,  525,  471,  471, 2059,  704,    1,\n",
      "         131,  131,  131,  525,    7,  131,  169,  131,  131,  525,  241,   29,\n",
      "        1722,  872,    7,    1, 1203,  525,   28,  689,   72,   93,  211,    7,\n",
      "           1,  525,    1,   93,  131,    1,  579,  230,   29,    1,   28,  689,\n",
      "        1891,   95,   93,   93])\tRaw indices: indices_to_process=tensor([1967958, 1467663,  667585, 1967963, 1367604, 3035648, 2301364,  667067,\n",
      "        2968418, 2568312, 2835012, 2301598, 1400957,  700544,   66713, 2568317,\n",
      "        2835006, 1101338, 2868359,  667190, 2434770, 2434776,   66794, 2568706,\n",
      "        1134133,  667191, 2301358, 2568350, 1100785, 1167356,  100066, 2801788,\n",
      "        2668371, 2835120, 2068017,   66707, 2434884, 1967922, 2301472, 1967926,\n",
      "        3201889, 1467668,  667301,   66821, 2601541, 1967996,  200119,  100060,\n",
      "        1067427, 1101174, 1968352, 1968038, 2434862,  133937,  567526, 1101120,\n",
      "        2802123, 1969886, 1968531, 2801653,  633838,  266955, 1901252, 2535353,\n",
      "        3301954,  433720, 3202057, 1834546, 2701724,  200643, 2635128, 1100678,\n",
      "        2436491, 1968699, 3201895, 1100650, 2636090, 2802177, 2434797, 2802341,\n",
      "        2301429, 2301450, 2568392, 2868365, 2568182, 1801587, 1234062, 3201981,\n",
      "        3035254, 2601535, 1968406, 1968057, 1967856,  767120, 2835033,  667749,\n",
      "        1969718, 2568276, 2835098,  200211])\n",
      "DEB: t.shape=torch.Size([5, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      "\ttensor([  131,   131,   131,   131,   131,   525,     7,   136,   136,  1732,\n",
      "          525,   131,     1,    28,   131,     7,     1,   525,   131,   136,\n",
      "          115,     7,   525,   525,     7,     1,   136,     7,     1,   131,\n",
      "          131,  1732,   525,   689,   131,   131,   169,     7,     7,     7,\n",
      "          525,   525,   131,   261,     1,     1,     1,     7,   131,   131,\n",
      "          131,     7,     7,   115,     1,   136,   241,   136,   169,   525,\n",
      "          525,    95,     7,   131,   131,     7,   169,   115,   169,    99,\n",
      "          115,   115,     1,    93,   131,   525,     1,     7,   136,   169,\n",
      "          115,   131,   115,   689,   525,   525,   115,     7,   115,     7,\n",
      "          136,   130,     1,   159,     7,     1,   131,   241,     1,   525,\n",
      "        12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426,\n",
      "          131,   525,     1,     7,   136,   131,   131,   525,    60,   131,\n",
      "          131,   131,   131,   525,   525,     7,     1,   525,  1203,   130,\n",
      "          131,   525,   131,     7,   131,     1,     1,     1,     7,   136,\n",
      "          131,   115,   131,   525,     1,   115,   115,   131,   525,   525,\n",
      "          136,   525,   525,     1,   136,     7,     1,  1722,   525,   115,\n",
      "          525,     7,   136,   169,   131,   115,   169,   136,    93,     7,\n",
      "            1,   131,   131,   525,     1,   689,   241,     7,   525,   131,\n",
      "            7,   136,   131,     7,     1,     1,     1,   136,    93,   136,\n",
      "          241,   136,   131,  1406,   689,   689,   241,   241,   136,   689,\n",
      "          131,  1722,    28,   525,     1,   130,     7,   159,     1,     1,\n",
      "          131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\n",
      "\ttensor([  131,   115,   131,   525,   131, 12072,     7,   131,   689,     1,\n",
      "            1,   131,   131,   131,     7,     7,   525,   525,     7,    90,\n",
      "         1722,     1,   131,   131,    88,     7,   241,   131,   136,   525,\n",
      "            1,   131,     1,  1203,   136,    93,   131,   159,    60,   115,\n",
      "            1,    88,     7,     1,   241,     7,     7,     7,     1,   131,\n",
      "          115,     7,     1,     1,   689,   131,     1,   241,    88,    28,\n",
      "         1722,     1,     7,     1,   525,   131,   136,     1,    88,   525,\n",
      "            7,   136,   115,   136,     1,   131,     7,     1,     7,     7,\n",
      "            1,     1,     7,    72,   131,     1,     7,   261,     7,   525,\n",
      "            1,   115,    78,   169,   131,   115,     7,    87,   115,     1,\n",
      "          131,   131,   131,   131,   131,     7,     7,   131,   131,     1,\n",
      "        12072,     7,     7,     7,     1,   241,     7,     7,   131,   689,\n",
      "          525,     7,   131,   525,     7,     7,   131,   136,     7,     7,\n",
      "            1,     7,     7,     7,   525,     1,   115,   115,     1,   241,\n",
      "          136,     7,   115,   131,     7,     1,    88,     1,   131,   241,\n",
      "          115,   241,    88,     1,   131,     7,     1,   131,   689,   525,\n",
      "          136,    88,    90,   131,     1,    72,   115,     1,    88,   115,\n",
      "          136,     1,   689,     1,     1,   131,     1,     7,    29,     1,\n",
      "          115,   131,     7,   525,   115,    60,    88,   136,     7,   136,\n",
      "            1,     1,   525,   115,  1722,     1,   525,   241,   115,     1,\n",
      "          131,   525,   131,   131,   131,   136,   525,   131,   131,   131,\n",
      "          131,   131,   169,   525,   131,   131,   131,   525,     1,   689,\n",
      "          525,   525,  1732,   131,   131,   131,   130,   525,   131,   131,\n",
      "          136,   241,     7,    29,    93,     1,     1,  1732,     1,     1,\n",
      "          136,   689,     7,   230,     1,     1,   131,     7,   131,   136,\n",
      "          136,   136,     7,   426,   241,   689,   136,     7,    28,    90,\n",
      "            1,     7,   131,     1,   115,     7,   211,   131,     1,   525,\n",
      "          455,   136,    99,     7,     1,   471,    95,     1,     1,     1,\n",
      "          241,     7,   136,   131,   136,   136,     1,    72,    91,   510,\n",
      "          131,     1,   241,   525,   525,    76,   115,   131,   502,     1,\n",
      "          131,   131,   525,   136,   131,   525,     7,     7,     1,   131,\n",
      "            7,   241,   131,   131,     7,   136,     1,   689,     1,   130,\n",
      "            1,     7,    88,   525,   131,   131,     1,   169,   136,     1,\n",
      "            7,   136,   131,   115,   131,     1,   115,    95,   115,    99,\n",
      "            1,   136,   241,   115,     7,   169,     1,     1,   131,   525,\n",
      "          525,   211,    93,   525,   525,   471,   471,  2059,   704,     1,\n",
      "          131,   131,   131,   525,     7,   131,   169,   131,   131,   525,\n",
      "          241,    29,  1722,   872,     7,     1,  1203,   525,    28,   689,\n",
      "           72,    93,   211,     7,     1,   525,     1,    93,   131,     1,\n",
      "          579,   230,    29,     1,    28,   689,  1891,    95,    93,    93])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([89, 60, 20, 89, 41, 31, 69,  7, 35, 48, 67, 16, 21, 53, 92, 31, 47, 62,\n",
      "        47, 47,  4,  3, 32, 77, 17, 27, 48, 31, 46,  4, 35,  7, 69, 51, 70, 31,\n",
      "         9, 12,  7, 52, 31, 51, 35, 45, 28, 37, 32, 81, 31, 35, 55, 44, 92, 51,\n",
      "         8, 41, 12, 30, 47, 92,  4, 82, 31,  4, 31, 47, 89,  4, 31, 90, 93, 47,\n",
      "        27, 35, 31, 47, 34, 31, 91, 16, 84, 35, 82, 47, 21, 20, 48,  4, 53, 12,\n",
      "        66, 53, 83, 53, 51, 69, 31, 51, 31, 47])\tToken idx: tensor([  131,  1732,  1732,   136,   131,     7,   131,     7,   241,     1,\n",
      "            1,   131,     1,     1,     7,     1,   689,   525,   525,   136,\n",
      "            7,   131,     1,     1,   131,     1,     7,   689,     1,   115,\n",
      "         1203,   115,   136,   136,   525,    93,     1,     7,     1,     1,\n",
      "          159,   525,    72,     1,     1,    28,     7,     1,    88,    60,\n",
      "            7,   131,     1,   689,     7,   136,     1,     1,   131,   115,\n",
      "          241, 12072,  1722,     1,   115,    29,   169,    88,    28,     1,\n",
      "            7,   169,   689,    88,    90,   882,     7,    29,   131,   136,\n",
      "            7,     7,   136,  3856,    93,   544,    93,    72,   689,   115,\n",
      "            1,    93,   689,   136,   131,     1,   426,   471,   205,   471])\tRaw indices: indices_to_process=tensor([2968548, 2002912,  668792, 2968553, 1367604, 1033950, 2301488,  233478,\n",
      "        1167596, 1600945, 2234652,  533779,  700414, 1767710, 3068483, 1033944,\n",
      "        1568280, 2068411, 1568116, 1567727,  133419,  100190, 1067297, 2568182,\n",
      "         567132,  900532, 1600951, 1034632, 1534239,  133527, 1168558,  233586,\n",
      "        2301493, 1701139, 2335235, 1034036,  300178,  400243,  233472, 1734357,\n",
      "        1034102, 1701528, 1167427, 1500886,  933885, 1234089, 1067303, 2701594,\n",
      "        1034031, 1167415, 1834422, 1467663, 3068477, 1701692,  266831, 1367609,\n",
      "         400237, 1000591, 1567722, 3068591,  133653, 2747018, 1035665,  133413,\n",
      "        1034058, 1567620, 2968586,  133500, 1033971, 3001771, 3101836, 1567760,\n",
      "         901220, 1167443, 1034033, 1568473, 1134009, 1033972, 3035254,  533784,\n",
      "        2801659, 1167362, 2735082, 1571447,  700506,  667604, 1601037,  133484,\n",
      "        1768398,  400351, 2201299, 1767802, 2768988, 1767845, 1701134, 2301358,\n",
      "        1034369, 1701474, 1034148, 1568062])\n",
      "Beam idx: tensor([23, 80, 20, 62, 83, 96, 72, 74, 35, 77, 20, 98, 51,  7, 72, 34, 79, 72,\n",
      "        46, 92, 97, 30, 72, 46, 73,  7, 29, 53,  6, 83, 66, 29,  7, 85,  7, 54,\n",
      "        23,  6, 16, 49, 59, 89, 66, 19, 34, 62, 85, 17, 34, 83, 24, 62, 82, 52,\n",
      "        62, 91, 99, 81, 43, 72, 97, 72, 17, 20, 16,  6, 46, 46,  7, 72, 49, 46,\n",
      "        51,  9, 35, 62, 29, 31, 46, 35, 18, 86, 57, 85, 51, 72, 25, 69, 99, 97,\n",
      "        54, 63, 29, 38, 62, 72, 80, 82, 46, 77])\tToken idx: tensor([  131, 12072,   131,   689,   131,   131,     7,     1,     1,   689,\n",
      "          136, 12072,   131,     7,     1,   131,     1,   115,     7,   131,\n",
      "          169,     1,   241,     1,     1,    88,   136, 12072,   136,   136,\n",
      "          131,   131,   115,     1,     1,     7,   136,   131,   136,   131,\n",
      "          131,     1,   525,   131,   136,   426,     7,   136,     1,   169,\n",
      "            1,     1,   131,     1,   882,     1,     1,     1,     7,    72,\n",
      "          689,   689,     1,   525,     1,     1,   689,   115,  1722,   369,\n",
      "          525,    93,   136,     1,     7,    90,   689,   525,   169,   115,\n",
      "            7,     1,     1,   241,   525,    93,     1,   525,     7,   525,\n",
      "            1,     7,     1,   525,   205,    90, 14399,   136,    95,     1])\tRaw indices: indices_to_process=tensor([ 767250, 2680312,  667191, 2068575, 2768430, 3202019, 2401423, 2468123,\n",
      "        1167356, 2568870,  667196, 3280666, 1701134,  233478, 2401417, 1134133,\n",
      "        2634888, 2401531, 1534245, 3068607, 3235410, 1000591, 2401657, 1534239,\n",
      "        2434770,  233559,  967373, 1779781,  200254, 2768435, 2201429,  967368,\n",
      "         233586, 2835006,  233472, 1801069,  767255,  200249,  533784, 1634428,\n",
      "        1967958, 2968418, 2201823,  633838, 1134138, 2068312, 2835012,  567137,\n",
      "        1134003, 2768468,  800473, 2067887, 2735077, 1734357, 2068768, 3035124,\n",
      "        3301948, 2701594, 1434186, 2401488, 3235930, 2402105,  567002,  667585,\n",
      "         533649,  200119, 1534927, 1534353,  235193, 2401785, 1634822, 1534331,\n",
      "        1701139,  300178, 1167362, 2067976,  967926, 1034468, 1534407, 1167470,\n",
      "         600361, 2868359, 1901122, 2835246, 1701528, 2401509,  833826, 2301882,\n",
      "        3301954, 3235766, 1801063, 2101246,  967238, 1267939, 2068091, 2401506,\n",
      "        2682639, 2735082, 1534333, 2568182])\n",
      "Beam idx: tensor([26, 21, 94, 27, 17, 13, 92, 94, 96, 35, 80, 24, 40, 56, 80, 19, 92, 26,\n",
      "        54, 40,  4, 71, 31, 92, 87, 93, 27, 10,  0, 98,  0, 31, 55,  4, 24, 52,\n",
      "        80, 11, 28, 97, 57, 31, 84, 11, 74, 39,  7,  5, 56, 80, 62,  8,  8, 46,\n",
      "        54,  4,  7, 25, 81, 40, 71, 24, 10, 31, 19, 89, 24,  1, 11, 32, 44, 79,\n",
      "        96, 20,  0, 31,  7, 81, 40, 12, 76, 96, 98, 52, 96, 76, 56, 69, 19, 25,\n",
      "        37, 97, 80, 60, 19, 57, 28, 94, 81, 54])\tToken idx: tensor([  131,   131,   131,   131,   131,   131,   131,   136, 12072,     1,\n",
      "          525,     7,     7,     7,   131,   525,   525,   525,   525,     1,\n",
      "            7,     7,   525,   136,   525,   131,   136,     7,    88,     1,\n",
      "            7,   136,   689,     1,    88,   525,   169,    88,     7,     7,\n",
      "          689,   131,     7,     7,     1,   131,     7,     7,     1,   136,\n",
      "            7,   115,     7,     1,   131,   115,   131,     1,   525,   115,\n",
      "            1,     1,    88,   689,     7,   131,   115,   131,   241,     1,\n",
      "            1,     1,   136,   131,   115,   169,     1,   136,    93,     7,\n",
      "            1,     1,     7,   136,   471,     7,   115,   131,   131,     7,\n",
      "            7,    28,   130,     1,   241,     1,     1,   525,   689,   130])\tRaw indices: indices_to_process=tensor([ 867309,  700544, 3135313,  900662,  567132,  433720, 3068607, 3135318,\n",
      "        3213960, 1167356, 2668765,  800479, 1334127, 1867775, 2668371,  634232,\n",
      "        3069001,  867703, 1801587, 1334121,  133419, 2368070, 1034468, 3068612,\n",
      "        2902236, 3101960,  900667,  333537,      88, 3268595,       7, 1034079,\n",
      "        1835104,  133413,  800560, 1734881, 2668409,  366971,  933891, 3235248,\n",
      "        1901810, 1034074, 2801659,  366890, 2468123, 1300898,  233478,  166772,\n",
      "        1867769, 2668376, 2067893,  266939,  266831, 1534239, 1801193,  133527,\n",
      "         233602,  833826, 2702118, 1334235, 2368064,  800473,  333618, 1034632,\n",
      "         633714, 2968548,  800587,   33484,  367124, 1067297, 1467533, 2634888,\n",
      "        3202024,  667191,     115, 1034112,  233472, 2701729, 1334213,  400243,\n",
      "        2534829, 3201889, 3268601, 1734492, 3202359, 2534835, 1867883, 2301488,\n",
      "         633838,  833832, 1234068, 3235269, 2668370, 2001181,  633948, 1901122,\n",
      "         933885, 3135707, 2702282, 1801192])\n",
      "Beam idx: tensor([53, 72, 42,  5, 49, 40, 30, 71, 24, 19, 26, 67, 68, 67, 12, 61, 80, 71,\n",
      "        25, 88, 36, 71, 98, 48, 75, 71, 64, 24, 24, 40,  0,  9,  1, 12, 12, 47,\n",
      "         0, 48, 76, 50, 68, 10, 89, 88, 53, 87, 15, 87, 25,  5, 14, 36, 11, 29,\n",
      "        88, 42, 18, 52, 71, 68, 48,  7, 21, 39, 24,  5, 16, 97, 54,  7, 60, 88,\n",
      "        74, 77, 64, 12,  0, 94, 93, 40, 95, 29, 71, 71, 88, 14, 88, 88,  9, 10,\n",
      "        44, 71, 94, 99,  1, 38, 11, 40,  7, 95])\tToken idx: tensor([ 131, 1732,  131,  131,  131,    1,  525,  689,    7,  131,    1,    1,\n",
      "           1,    7,    7,    7,  131,  426,    1,    1,  136,    7,  241,    7,\n",
      "           1,  169,  525,    1,  115,    7,   88,    7,   88,    1,  115,    1,\n",
      "           7,    1,  525,  131,    7,  525,    1, 1722,  136, 1203,    7,   60,\n",
      "           7,  136,  136,  131,  525,    1,   88,  525,    1,  241,   90,  115,\n",
      "         115,  131,  131,    7,  241,  525,    1,    7,  131,  136,    7,    7,\n",
      "         525,  131,  136,  241,  115,  689,  525,  115,    7,    7,   29,    1,\n",
      "          93,  525,  159,  115,    1,  136,  131,  205,    1,  241,    7,  136,\n",
      "         131,   93,  689,  689])\tRaw indices: indices_to_process=tensor([1767840, 2403148, 1400957,  166896, 1634428, 1334121, 1001115, 2368752,\n",
      "         800479,  633838,  867179, 2234652, 2268005, 2234658,  400243, 2034540,\n",
      "        2668371, 2368489,  833826, 2935065, 1200844, 2368070, 3268835, 1600951,\n",
      "        2501476, 2368232, 2135117,  800473,  800587, 1334127,      88,  300184,\n",
      "          33441,  400237,  400351, 1567592,       7, 1600945, 2535353, 1667781,\n",
      "        2268011,  334055, 2968418, 2936786, 1767845, 2902914,  500302, 2901771,\n",
      "         833832,  166901,  467078, 1200839,  367408,  967238, 2935152, 1401351,\n",
      "         600355, 1734597, 2368153, 2268119, 1601059,  233602,  700544, 1300774,\n",
      "         800713,  167290,  533649, 3235248, 1801193,  233607, 2001187, 2935071,\n",
      "        2468647, 2568312, 2134728,  400477,     115, 3135871, 3102354, 1334235,\n",
      "        3168542,  967244, 2368092, 2368064, 2935157,  467467, 2935223, 2935179,\n",
      "         300178,  333666, 1467663, 2368268, 3135183, 3302188,   33360, 1267550,\n",
      "         367014, 1334213,  234160, 3169224])\n",
      "DEB: t.shape=torch.Size([6, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      "\ttensor([  131,   131,   131,   131,   131,   525,     7,   136,   136,  1732,\n",
      "          525,   131,     1,    28,   131,     7,     1,   525,   131,   136,\n",
      "          115,     7,   525,   525,     7,     1,   136,     7,     1,   131,\n",
      "          131,  1732,   525,   689,   131,   131,   169,     7,     7,     7,\n",
      "          525,   525,   131,   261,     1,     1,     1,     7,   131,   131,\n",
      "          131,     7,     7,   115,     1,   136,   241,   136,   169,   525,\n",
      "          525,    95,     7,   131,   131,     7,   169,   115,   169,    99,\n",
      "          115,   115,     1,    93,   131,   525,     1,     7,   136,   169,\n",
      "          115,   131,   115,   689,   525,   525,   115,     7,   115,     7,\n",
      "          136,   130,     1,   159,     7,     1,   131,   241,     1,   525,\n",
      "        12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426,\n",
      "          131,   525,     1,     7,   136,   131,   131,   525,    60,   131,\n",
      "          131,   131,   131,   525,   525,     7,     1,   525,  1203,   130,\n",
      "          131,   525,   131,     7,   131,     1,     1,     1,     7,   136,\n",
      "          131,   115,   131,   525,     1,   115,   115,   131,   525,   525,\n",
      "          136,   525,   525,     1,   136,     7,     1,  1722,   525,   115,\n",
      "          525,     7,   136,   169,   131,   115,   169,   136,    93,     7,\n",
      "            1,   131,   131,   525,     1,   689,   241,     7,   525,   131,\n",
      "            7,   136,   131,     7,     1,     1,     1,   136,    93,   136,\n",
      "          241,   136,   131,  1406,   689,   689,   241,   241,   136,   689,\n",
      "          131,  1722,    28,   525,     1,   130,     7,   159,     1,     1,\n",
      "          131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\n",
      "\ttensor([  131,   115,   131,   525,   131, 12072,     7,   131,   689,     1,\n",
      "            1,   131,   131,   131,     7,     7,   525,   525,     7,    90,\n",
      "         1722,     1,   131,   131,    88,     7,   241,   131,   136,   525,\n",
      "            1,   131,     1,  1203,   136,    93,   131,   159,    60,   115,\n",
      "            1,    88,     7,     1,   241,     7,     7,     7,     1,   131,\n",
      "          115,     7,     1,     1,   689,   131,     1,   241,    88,    28,\n",
      "         1722,     1,     7,     1,   525,   131,   136,     1,    88,   525,\n",
      "            7,   136,   115,   136,     1,   131,     7,     1,     7,     7,\n",
      "            1,     1,     7,    72,   131,     1,     7,   261,     7,   525,\n",
      "            1,   115,    78,   169,   131,   115,     7,    87,   115,     1,\n",
      "          131,   131,   131,   131,   131,     7,     7,   131,   131,     1,\n",
      "        12072,     7,     7,     7,     1,   241,     7,     7,   131,   689,\n",
      "          525,     7,   131,   525,     7,     7,   131,   136,     7,     7,\n",
      "            1,     7,     7,     7,   525,     1,   115,   115,     1,   241,\n",
      "          136,     7,   115,   131,     7,     1,    88,     1,   131,   241,\n",
      "          115,   241,    88,     1,   131,     7,     1,   131,   689,   525,\n",
      "          136,    88,    90,   131,     1,    72,   115,     1,    88,   115,\n",
      "          136,     1,   689,     1,     1,   131,     1,     7,    29,     1,\n",
      "          115,   131,     7,   525,   115,    60,    88,   136,     7,   136,\n",
      "            1,     1,   525,   115,  1722,     1,   525,   241,   115,     1,\n",
      "          131,   525,   131,   131,   131,   136,   525,   131,   131,   131,\n",
      "          131,   131,   169,   525,   131,   131,   131,   525,     1,   689,\n",
      "          525,   525,  1732,   131,   131,   131,   130,   525,   131,   131,\n",
      "          136,   241,     7,    29,    93,     1,     1,  1732,     1,     1,\n",
      "          136,   689,     7,   230,     1,     1,   131,     7,   131,   136,\n",
      "          136,   136,     7,   426,   241,   689,   136,     7,    28,    90,\n",
      "            1,     7,   131,     1,   115,     7,   211,   131,     1,   525,\n",
      "          455,   136,    99,     7,     1,   471,    95,     1,     1,     1,\n",
      "          241,     7,   136,   131,   136,   136,     1,    72,    91,   510,\n",
      "          131,     1,   241,   525,   525,    76,   115,   131,   502,     1,\n",
      "          131,   131,   525,   136,   131,   525,     7,     7,     1,   131,\n",
      "            7,   241,   131,   131,     7,   136,     1,   689,     1,   130,\n",
      "            1,     7,    88,   525,   131,   131,     1,   169,   136,     1,\n",
      "            7,   136,   131,   115,   131,     1,   115,    95,   115,    99,\n",
      "            1,   136,   241,   115,     7,   169,     1,     1,   131,   525,\n",
      "          525,   211,    93,   525,   525,   471,   471,  2059,   704,     1,\n",
      "          131,   131,   131,   525,     7,   131,   169,   131,   131,   525,\n",
      "          241,    29,  1722,   872,     7,     1,  1203,   525,    28,   689,\n",
      "           72,    93,   211,     7,     1,   525,     1,    93,   131,     1,\n",
      "          579,   230,    29,     1,    28,   689,  1891,    95,    93,    93])\n",
      "\ttensor([  131,  1732,  1732,   136,   131,     7,   131,     7,   241,     1,\n",
      "            1,   131,     1,     1,     7,     1,   689,   525,   525,   136,\n",
      "            7,   131,     1,     1,   131,     1,     7,   689,     1,   115,\n",
      "         1203,   115,   136,   136,   525,    93,     1,     7,     1,     1,\n",
      "          159,   525,    72,     1,     1,    28,     7,     1,    88,    60,\n",
      "            7,   131,     1,   689,     7,   136,     1,     1,   131,   115,\n",
      "          241, 12072,  1722,     1,   115,    29,   169,    88,    28,     1,\n",
      "            7,   169,   689,    88,    90,   882,     7,    29,   131,   136,\n",
      "            7,     7,   136,  3856,    93,   544,    93,    72,   689,   115,\n",
      "            1,    93,   689,   136,   131,     1,   426,   471,   205,   471,\n",
      "          131, 12072,   131,   689,   131,   131,     7,     1,     1,   689,\n",
      "          136, 12072,   131,     7,     1,   131,     1,   115,     7,   131,\n",
      "          169,     1,   241,     1,     1,    88,   136, 12072,   136,   136,\n",
      "          131,   131,   115,     1,     1,     7,   136,   131,   136,   131,\n",
      "          131,     1,   525,   131,   136,   426,     7,   136,     1,   169,\n",
      "            1,     1,   131,     1,   882,     1,     1,     1,     7,    72,\n",
      "          689,   689,     1,   525,     1,     1,   689,   115,  1722,   369,\n",
      "          525,    93,   136,     1,     7,    90,   689,   525,   169,   115,\n",
      "            7,     1,     1,   241,   525,    93,     1,   525,     7,   525,\n",
      "            1,     7,     1,   525,   205,    90, 14399,   136,    95,     1,\n",
      "          131,   131,   131,   131,   131,   131,   131,   136, 12072,     1,\n",
      "          525,     7,     7,     7,   131,   525,   525,   525,   525,     1,\n",
      "            7,     7,   525,   136,   525,   131,   136,     7,    88,     1,\n",
      "            7,   136,   689,     1,    88,   525,   169,    88,     7,     7,\n",
      "          689,   131,     7,     7,     1,   131,     7,     7,     1,   136,\n",
      "            7,   115,     7,     1,   131,   115,   131,     1,   525,   115,\n",
      "            1,     1,    88,   689,     7,   131,   115,   131,   241,     1,\n",
      "            1,     1,   136,   131,   115,   169,     1,   136,    93,     7,\n",
      "            1,     1,     7,   136,   471,     7,   115,   131,   131,     7,\n",
      "            7,    28,   130,     1,   241,     1,     1,   525,   689,   130,\n",
      "          131,  1732,   131,   131,   131,     1,   525,   689,     7,   131,\n",
      "            1,     1,     1,     7,     7,     7,   131,   426,     1,     1,\n",
      "          136,     7,   241,     7,     1,   169,   525,     1,   115,     7,\n",
      "           88,     7,    88,     1,   115,     1,     7,     1,   525,   131,\n",
      "            7,   525,     1,  1722,   136,  1203,     7,    60,     7,   136,\n",
      "          136,   131,   525,     1,    88,   525,     1,   241,    90,   115,\n",
      "          115,   131,   131,     7,   241,   525,     1,     7,   131,   136,\n",
      "            7,     7,   525,   131,   136,   241,   115,   689,   525,   115,\n",
      "            7,     7,    29,     1,    93,   525,   159,   115,     1,   136,\n",
      "          131,   205,     1,   241,     7,   136,   131,    93,   689,   689])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([18, 42, 17, 60, 14, 60, 62, 75, 24, 42, 10, 58, 95, 56, 88, 14, 18, 22,\n",
      "        41, 58, 94, 94, 14, 42, 33, 80, 86, 14,  0, 94, 80, 78, 58, 63, 70, 97,\n",
      "        60, 49, 78, 71, 65, 71, 24,  0, 88, 99, 33, 43, 75, 49, 57, 14, 50, 14,\n",
      "        14, 22, 74, 85, 14, 36, 42, 13,  5, 80, 80, 11, 80, 94, 60, 86, 25, 86,\n",
      "        24, 54,  2,  5, 75, 11, 14, 24, 80, 71, 14, 14, 54, 59, 14, 65,  6, 58,\n",
      "        60,  9,  5, 98, 94, 87, 49, 36, 80,  0])\tToken idx: tensor([ 131,  525,  131,  131,  136, 1732, 1732,    7,    7,  131,    1,    7,\n",
      "           1,    1,  689,  689,  136,    1,  131,    1,   88,    7,  525,  136,\n",
      "           7,  136,  525,  131,   88,    1,    1,    7,  115,    1,  525,    1,\n",
      "         136,    7,    1,    1,  689,    7,    1,    7,    1,    1,  241,    1,\n",
      "          90,  241,    1,  169,  136,    1,   29,    7,  689,    1,  882,    1,\n",
      "         130,    1,  136,  471,  525,   88,  689,  115, 2059, 1203,    1,   60,\n",
      "         115,  689,    7,  131,  689,    7,  230,   88,  169,  115,  426,  471,\n",
      "           1,  136,   93,  169,    1,  241,  169,    1,    1,    1, 1722,  525,\n",
      "         115,    7,   29,  115])\tRaw indices: indices_to_process=tensor([ 600485, 1401351,  567132, 2001311,  467078, 2002912, 2069618, 2501482,\n",
      "         800479, 1400957,  333531, 1934481, 3168536, 1867769, 2935753,  467631,\n",
      "         600490,  733767, 1367604, 1934475, 3135270, 3135189,  467467, 1400962,\n",
      "        1100656, 2668376, 2868883,  467073,      88, 3135183, 2668241, 2601541,\n",
      "        1934589, 2101240, 2335235, 3235242, 2001316, 1634304, 2601535, 2368064,\n",
      "        2168634, 2368070,  800473,       7, 2935065, 3301948, 1100890, 1434180,\n",
      "        2501565, 1634538, 1901122,  467111, 1667786,  466943,  466971,  733773,\n",
      "        2468811, 2835006,  467824, 1200709, 1400956,  433590,  166901, 2668711,\n",
      "        2668765,  366971, 2668929, 3135297, 2003239, 2869561,  833826, 2868418,\n",
      "         800587, 1801751,   66713,  166896, 2502164,  366890,  467172,  800560,\n",
      "        2668409, 2368178,  467368,  467413, 1801063, 1967963,  467035, 2168114,\n",
      "         200119, 1934715, 2001349,  300178,  166766, 3268595, 3136904, 2902236,\n",
      "        1634412, 1200715, 2668269,     115])\n",
      "Beam idx: tensor([93, 42, 68, 64, 14, 84, 65, 47, 28, 47, 58, 93, 67, 88, 78,  2, 42, 56,\n",
      "        78, 40,  4, 48, 50, 55, 44, 47, 71,  0, 44,  1, 60,  4, 13,  3, 37, 87,\n",
      "        84, 58, 44, 47,  0,  0, 36, 13, 60, 90, 26,  2, 61, 42,  8, 48, 89, 58,\n",
      "        58, 27, 75, 76, 13, 88, 68, 15, 55, 60, 88, 15, 33, 78, 37, 88, 37,  4,\n",
      "        21, 37,  8, 70, 36, 60, 95, 36, 45,  0, 41, 59, 73, 53, 10, 63, 45, 55,\n",
      "        40, 65, 32, 98, 88,  3, 71, 37, 15, 47])\tToken idx: tensor([  131,   131,  1732,     1,     1,   131,     1,     1,     1,     7,\n",
      "          689,   136, 12072,   689,     1,    88,   136,     1,     7,     7,\n",
      "           88,     1,     1,     1,     1,   241,   241,     7,     7,     7,\n",
      "            7,     7,   525,     7,     1,   131,   136,     1,   689,   115,\n",
      "          115,    88,     1,   136,     1,     1,     7,     7,     7,   525,\n",
      "            1,     7,   131,   525,   136,     7,   689,     7,   131,   169,\n",
      "          544,    88,   689,   115,   136,     7,     1,   169,     7,    29,\n",
      "           88,   115,     1,  1722,     7,   131,   689,   689,   689,     7,\n",
      "            7,     1,     1,   131,     1,     1,     7,   131,     1,   169,\n",
      "          115,    93,   136,     1,  3856,   241,    60,   115,     1,    72])\tRaw indices: indices_to_process=tensor([3101960, 1400957, 2269736, 2134593,  466943, 2801783, 2167946, 1567592,\n",
      "         933885, 1567598, 1935163, 3101965, 2246723, 2935753, 2601535,   66794,\n",
      "        1400962, 1867769, 2601541, 1334127,  133500, 1600945, 1667651, 1834416,\n",
      "        1467533, 1567832, 2368304,       7, 1467539,   33360, 2001187,  133419,\n",
      "         434114,  100066, 1234062, 2901842, 2801788, 1934475, 1468221, 1567706,\n",
      "            115,      88, 1200709,  433725, 2001181, 3001771,  867185,   66713,\n",
      "        2034540, 1401351,  266825, 1600951, 2968548, 1934999, 1934610,  900538,\n",
      "        2502164, 2534835,  433720, 2935233, 2268548,  500383, 1835104, 2001295,\n",
      "        2935200,  500302, 1100650, 2601703, 1234068, 2935093, 1234149,  133527,\n",
      "         700414, 1235783,  266831, 2334841, 1201397, 2001869, 3169224, 1200715,\n",
      "        1500892,       1, 1367474, 1967958, 2434770, 1767710,  333537, 2101370,\n",
      "        1500886, 1834584, 1334235, 2168038, 1067432, 3268595, 2938920,  100300,\n",
      "        2368123, 1234176,  500296, 1567663])\n",
      "Beam idx: tensor([22, 21, 58, 17, 21, 92, 99,  6, 73, 18, 61, 45, 67, 58, 13, 73, 22,  6,\n",
      "        51, 67,  6, 30, 67, 35,  6, 75, 73, 41, 30, 73, 50,  9, 15, 68, 72, 50,\n",
      "        99, 78, 13, 58, 59, 45, 51,  6, 90, 47, 59, 21, 78, 78, 73, 88, 42, 73,\n",
      "        73, 41, 48, 59, 29, 83, 30, 14, 26, 73, 77, 30, 73, 30, 53, 73, 23, 73,\n",
      "         6,  3, 18, 73, 73, 68, 67, 14, 30, 67, 51, 73, 30,  6,  6, 42, 65,  2,\n",
      "         6, 92, 17, 14, 23, 35, 22, 51, 51, 61])\tToken idx: tensor([ 131,  131,  131,  131,  136,  131,  131,    7,    1,  131,    1,    1,\n",
      "           7,  136,  131,    7,  136,   88,  525,   88,    1,  689,    1,  131,\n",
      "         115,    1,  689,    7,    1,   93,  136,    1,  131,  131,    7,  131,\n",
      "         525,  241,  136,  525,  689,    7,  136, 1722,    1,  136,  525,  525,\n",
      "          60, 1203,  159,    7,  136, 1722,   28,    1,    1,  136,    1,    7,\n",
      "         169,    7,  131,   88,    1,   93,   29,  136,    1,   90,    7, 1346,\n",
      "         159,   88,  525,  426,  369,  136,   90,    1,  525,  689,  689,  205,\n",
      "          29,   28, 1346,  131,  689,    7,   93,  525,  136,  115,    1,  136,\n",
      "        1891,  130,    1,   93])\tRaw indices: indices_to_process=tensor([ 733897,  700544, 1934605,  567132,  700549, 3068607, 3302078,  200125,\n",
      "        2434770,  600485, 2034534, 1500886, 2234658, 1934610,  433720, 2434776,\n",
      "         733902,  200206, 1701528, 2234739,  200119, 1001279, 2234652, 1167486,\n",
      "         200233, 2501476, 2435458, 1367480, 1000591, 2434862, 1667786,  300178,\n",
      "         500426, 2268135, 2401423, 1667781, 3302472, 2601775,  433725, 1934999,\n",
      "        1968516, 1500892, 1701139,  201840, 3001771, 1567727, 1968352,  700938,\n",
      "        2601594, 2602737, 2434928, 2935071, 1400962, 2436491, 2434797, 1367474,\n",
      "        1600945, 1967963,  967238, 2768306, 1000759,  466949,  867309, 2434857,\n",
      "        2568182, 1000683, 2434798, 1000726, 1767710, 2434859,  767126, 2436115,\n",
      "         200277,  100147,  600879, 2435195, 2435138, 2268140, 2234741,  466943,\n",
      "        1001115, 2235340, 1701692, 2434974, 1000619,  200146,  201464, 1400957,\n",
      "        2168634,   66713,  200211, 3069001,  567137,  467057,  767120, 1167491,\n",
      "         735657, 1701133, 1701004, 2034626])\n",
      "Beam idx: tensor([41, 43, 45, 45, 92, 55, 35, 66, 56, 85, 72, 41, 76, 82, 19,  6, 82, 27,\n",
      "        70, 43, 72, 65, 51, 92, 26, 58, 28, 82, 62, 28, 83, 90, 63, 28,  3, 63,\n",
      "        78, 28, 90, 45, 70, 51,  2,  4, 38, 45, 81, 98, 82, 51, 37, 93, 46, 31,\n",
      "        30, 92, 83, 82, 28, 28, 96, 46, 90, 92, 92, 17, 22, 17, 52, 65, 61, 66,\n",
      "        92, 80, 58, 28, 62,  6, 22, 45, 65, 79, 72, 70, 27, 28, 84, 20, 56, 45,\n",
      "        62, 55, 92, 73, 92, 46, 82, 17, 31, 70])\tToken idx: tensor([ 131, 1732,    7,  241,    1,  131,    1,    1,    1,  131,  131,  136,\n",
      "         525,  689,  131,  131,  169,    1,  525,  544, 1732,  131,    1,    7,\n",
      "         131,  689,  689,  426,    7,  136,    1,    1,  136,    1,    7,  131,\n",
      "         131,  169,    7,  115,  136,    7,    7,    7,  131,  525,  525,  241,\n",
      "           1,  115,    1,  525,  136,  689,  525,  169,  689,   29,  525,   93,\n",
      "           7,    1,  115,   93,  115,    7,  169,   90,  131,  136,    7,    7,\n",
      "         689,  131,  426,   29,   88,  136,  131,   72,    1,  689,  136,    1,\n",
      "           7,  426,   60,    7,    7,  130,    1,  136,  426,    7,   29,  525,\n",
      "          26,  689,  136,  131])\tRaw indices: indices_to_process=tensor([1367604, 1435911, 1500892, 1501126, 3068477, 1834546, 1167356, 2201299,\n",
      "        1867769, 2835136, 2401547, 1367609, 2535353, 2735635,  633838,  200249,\n",
      "        2735115,  900532, 2335235, 1434723, 2403148, 2168076, 1701004, 3068483,\n",
      "         867309, 1935163,  934573, 2735372, 2067893,  934020, 2768300, 3001771,\n",
      "        2101375,  933885,  100066, 2101370, 2601665,  934053, 3001777, 1501000,\n",
      "        2334846, 1701010,   66713,  133419, 1267545, 1501410, 2702118, 3268835,\n",
      "        2734947, 1701118, 1234062, 3102354, 1534374, 1034632, 1001115, 3068645,\n",
      "        2768988, 2734975,  934409,  933977, 3201895, 1534239, 3001885, 3068569,\n",
      "        3068591,  567008,  733935,  567091, 1734487, 2168081, 2034540, 2201305,\n",
      "        3069165, 2668371, 1934900,  933913, 2067974,  200254,  733897, 1500957,\n",
      "        2167946, 2635576, 2401552, 2334711,  900538,  934310, 2801712,  667067,\n",
      "        1867775, 1501015, 2067887, 1834551, 3068902, 2434776, 3068505, 1534763,\n",
      "        2734972,  567690, 1034079, 2334841])\n",
      "DEB: t.shape=torch.Size([7, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      "\ttensor([  131,   131,   131,   131,   131,   525,     7,   136,   136,  1732,\n",
      "          525,   131,     1,    28,   131,     7,     1,   525,   131,   136,\n",
      "          115,     7,   525,   525,     7,     1,   136,     7,     1,   131,\n",
      "          131,  1732,   525,   689,   131,   131,   169,     7,     7,     7,\n",
      "          525,   525,   131,   261,     1,     1,     1,     7,   131,   131,\n",
      "          131,     7,     7,   115,     1,   136,   241,   136,   169,   525,\n",
      "          525,    95,     7,   131,   131,     7,   169,   115,   169,    99,\n",
      "          115,   115,     1,    93,   131,   525,     1,     7,   136,   169,\n",
      "          115,   131,   115,   689,   525,   525,   115,     7,   115,     7,\n",
      "          136,   130,     1,   159,     7,     1,   131,   241,     1,   525,\n",
      "        12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426,\n",
      "          131,   525,     1,     7,   136,   131,   131,   525,    60,   131,\n",
      "          131,   131,   131,   525,   525,     7,     1,   525,  1203,   130,\n",
      "          131,   525,   131,     7,   131,     1,     1,     1,     7,   136,\n",
      "          131,   115,   131,   525,     1,   115,   115,   131,   525,   525,\n",
      "          136,   525,   525,     1,   136,     7,     1,  1722,   525,   115,\n",
      "          525,     7,   136,   169,   131,   115,   169,   136,    93,     7,\n",
      "            1,   131,   131,   525,     1,   689,   241,     7,   525,   131,\n",
      "            7,   136,   131,     7,     1,     1,     1,   136,    93,   136,\n",
      "          241,   136,   131,  1406,   689,   689,   241,   241,   136,   689,\n",
      "          131,  1722,    28,   525,     1,   130,     7,   159,     1,     1,\n",
      "          131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\n",
      "\ttensor([  131,   115,   131,   525,   131, 12072,     7,   131,   689,     1,\n",
      "            1,   131,   131,   131,     7,     7,   525,   525,     7,    90,\n",
      "         1722,     1,   131,   131,    88,     7,   241,   131,   136,   525,\n",
      "            1,   131,     1,  1203,   136,    93,   131,   159,    60,   115,\n",
      "            1,    88,     7,     1,   241,     7,     7,     7,     1,   131,\n",
      "          115,     7,     1,     1,   689,   131,     1,   241,    88,    28,\n",
      "         1722,     1,     7,     1,   525,   131,   136,     1,    88,   525,\n",
      "            7,   136,   115,   136,     1,   131,     7,     1,     7,     7,\n",
      "            1,     1,     7,    72,   131,     1,     7,   261,     7,   525,\n",
      "            1,   115,    78,   169,   131,   115,     7,    87,   115,     1,\n",
      "          131,   131,   131,   131,   131,     7,     7,   131,   131,     1,\n",
      "        12072,     7,     7,     7,     1,   241,     7,     7,   131,   689,\n",
      "          525,     7,   131,   525,     7,     7,   131,   136,     7,     7,\n",
      "            1,     7,     7,     7,   525,     1,   115,   115,     1,   241,\n",
      "          136,     7,   115,   131,     7,     1,    88,     1,   131,   241,\n",
      "          115,   241,    88,     1,   131,     7,     1,   131,   689,   525,\n",
      "          136,    88,    90,   131,     1,    72,   115,     1,    88,   115,\n",
      "          136,     1,   689,     1,     1,   131,     1,     7,    29,     1,\n",
      "          115,   131,     7,   525,   115,    60,    88,   136,     7,   136,\n",
      "            1,     1,   525,   115,  1722,     1,   525,   241,   115,     1,\n",
      "          131,   525,   131,   131,   131,   136,   525,   131,   131,   131,\n",
      "          131,   131,   169,   525,   131,   131,   131,   525,     1,   689,\n",
      "          525,   525,  1732,   131,   131,   131,   130,   525,   131,   131,\n",
      "          136,   241,     7,    29,    93,     1,     1,  1732,     1,     1,\n",
      "          136,   689,     7,   230,     1,     1,   131,     7,   131,   136,\n",
      "          136,   136,     7,   426,   241,   689,   136,     7,    28,    90,\n",
      "            1,     7,   131,     1,   115,     7,   211,   131,     1,   525,\n",
      "          455,   136,    99,     7,     1,   471,    95,     1,     1,     1,\n",
      "          241,     7,   136,   131,   136,   136,     1,    72,    91,   510,\n",
      "          131,     1,   241,   525,   525,    76,   115,   131,   502,     1,\n",
      "          131,   131,   525,   136,   131,   525,     7,     7,     1,   131,\n",
      "            7,   241,   131,   131,     7,   136,     1,   689,     1,   130,\n",
      "            1,     7,    88,   525,   131,   131,     1,   169,   136,     1,\n",
      "            7,   136,   131,   115,   131,     1,   115,    95,   115,    99,\n",
      "            1,   136,   241,   115,     7,   169,     1,     1,   131,   525,\n",
      "          525,   211,    93,   525,   525,   471,   471,  2059,   704,     1,\n",
      "          131,   131,   131,   525,     7,   131,   169,   131,   131,   525,\n",
      "          241,    29,  1722,   872,     7,     1,  1203,   525,    28,   689,\n",
      "           72,    93,   211,     7,     1,   525,     1,    93,   131,     1,\n",
      "          579,   230,    29,     1,    28,   689,  1891,    95,    93,    93])\n",
      "\ttensor([  131,  1732,  1732,   136,   131,     7,   131,     7,   241,     1,\n",
      "            1,   131,     1,     1,     7,     1,   689,   525,   525,   136,\n",
      "            7,   131,     1,     1,   131,     1,     7,   689,     1,   115,\n",
      "         1203,   115,   136,   136,   525,    93,     1,     7,     1,     1,\n",
      "          159,   525,    72,     1,     1,    28,     7,     1,    88,    60,\n",
      "            7,   131,     1,   689,     7,   136,     1,     1,   131,   115,\n",
      "          241, 12072,  1722,     1,   115,    29,   169,    88,    28,     1,\n",
      "            7,   169,   689,    88,    90,   882,     7,    29,   131,   136,\n",
      "            7,     7,   136,  3856,    93,   544,    93,    72,   689,   115,\n",
      "            1,    93,   689,   136,   131,     1,   426,   471,   205,   471,\n",
      "          131, 12072,   131,   689,   131,   131,     7,     1,     1,   689,\n",
      "          136, 12072,   131,     7,     1,   131,     1,   115,     7,   131,\n",
      "          169,     1,   241,     1,     1,    88,   136, 12072,   136,   136,\n",
      "          131,   131,   115,     1,     1,     7,   136,   131,   136,   131,\n",
      "          131,     1,   525,   131,   136,   426,     7,   136,     1,   169,\n",
      "            1,     1,   131,     1,   882,     1,     1,     1,     7,    72,\n",
      "          689,   689,     1,   525,     1,     1,   689,   115,  1722,   369,\n",
      "          525,    93,   136,     1,     7,    90,   689,   525,   169,   115,\n",
      "            7,     1,     1,   241,   525,    93,     1,   525,     7,   525,\n",
      "            1,     7,     1,   525,   205,    90, 14399,   136,    95,     1,\n",
      "          131,   131,   131,   131,   131,   131,   131,   136, 12072,     1,\n",
      "          525,     7,     7,     7,   131,   525,   525,   525,   525,     1,\n",
      "            7,     7,   525,   136,   525,   131,   136,     7,    88,     1,\n",
      "            7,   136,   689,     1,    88,   525,   169,    88,     7,     7,\n",
      "          689,   131,     7,     7,     1,   131,     7,     7,     1,   136,\n",
      "            7,   115,     7,     1,   131,   115,   131,     1,   525,   115,\n",
      "            1,     1,    88,   689,     7,   131,   115,   131,   241,     1,\n",
      "            1,     1,   136,   131,   115,   169,     1,   136,    93,     7,\n",
      "            1,     1,     7,   136,   471,     7,   115,   131,   131,     7,\n",
      "            7,    28,   130,     1,   241,     1,     1,   525,   689,   130,\n",
      "          131,  1732,   131,   131,   131,     1,   525,   689,     7,   131,\n",
      "            1,     1,     1,     7,     7,     7,   131,   426,     1,     1,\n",
      "          136,     7,   241,     7,     1,   169,   525,     1,   115,     7,\n",
      "           88,     7,    88,     1,   115,     1,     7,     1,   525,   131,\n",
      "            7,   525,     1,  1722,   136,  1203,     7,    60,     7,   136,\n",
      "          136,   131,   525,     1,    88,   525,     1,   241,    90,   115,\n",
      "          115,   131,   131,     7,   241,   525,     1,     7,   131,   136,\n",
      "            7,     7,   525,   131,   136,   241,   115,   689,   525,   115,\n",
      "            7,     7,    29,     1,    93,   525,   159,   115,     1,   136,\n",
      "          131,   205,     1,   241,     7,   136,   131,    93,   689,   689])\n",
      "\ttensor([  131,   525,   131,   131,   136,  1732,  1732,     7,     7,   131,\n",
      "            1,     7,     1,     1,   689,   689,   136,     1,   131,     1,\n",
      "           88,     7,   525,   136,     7,   136,   525,   131,    88,     1,\n",
      "            1,     7,   115,     1,   525,     1,   136,     7,     1,     1,\n",
      "          689,     7,     1,     7,     1,     1,   241,     1,    90,   241,\n",
      "            1,   169,   136,     1,    29,     7,   689,     1,   882,     1,\n",
      "          130,     1,   136,   471,   525,    88,   689,   115,  2059,  1203,\n",
      "            1,    60,   115,   689,     7,   131,   689,     7,   230,    88,\n",
      "          169,   115,   426,   471,     1,   136,    93,   169,     1,   241,\n",
      "          169,     1,     1,     1,  1722,   525,   115,     7,    29,   115,\n",
      "          131,   131,  1732,     1,     1,   131,     1,     1,     1,     7,\n",
      "          689,   136, 12072,   689,     1,    88,   136,     1,     7,     7,\n",
      "           88,     1,     1,     1,     1,   241,   241,     7,     7,     7,\n",
      "            7,     7,   525,     7,     1,   131,   136,     1,   689,   115,\n",
      "          115,    88,     1,   136,     1,     1,     7,     7,     7,   525,\n",
      "            1,     7,   131,   525,   136,     7,   689,     7,   131,   169,\n",
      "          544,    88,   689,   115,   136,     7,     1,   169,     7,    29,\n",
      "           88,   115,     1,  1722,     7,   131,   689,   689,   689,     7,\n",
      "            7,     1,     1,   131,     1,     1,     7,   131,     1,   169,\n",
      "          115,    93,   136,     1,  3856,   241,    60,   115,     1,    72,\n",
      "          131,   131,   131,   131,   136,   131,   131,     7,     1,   131,\n",
      "            1,     1,     7,   136,   131,     7,   136,    88,   525,    88,\n",
      "            1,   689,     1,   131,   115,     1,   689,     7,     1,    93,\n",
      "          136,     1,   131,   131,     7,   131,   525,   241,   136,   525,\n",
      "          689,     7,   136,  1722,     1,   136,   525,   525,    60,  1203,\n",
      "          159,     7,   136,  1722,    28,     1,     1,   136,     1,     7,\n",
      "          169,     7,   131,    88,     1,    93,    29,   136,     1,    90,\n",
      "            7,  1346,   159,    88,   525,   426,   369,   136,    90,     1,\n",
      "          525,   689,   689,   205,    29,    28,  1346,   131,   689,     7,\n",
      "           93,   525,   136,   115,     1,   136,  1891,   130,     1,    93,\n",
      "          131,  1732,     7,   241,     1,   131,     1,     1,     1,   131,\n",
      "          131,   136,   525,   689,   131,   131,   169,     1,   525,   544,\n",
      "         1732,   131,     1,     7,   131,   689,   689,   426,     7,   136,\n",
      "            1,     1,   136,     1,     7,   131,   131,   169,     7,   115,\n",
      "          136,     7,     7,     7,   131,   525,   525,   241,     1,   115,\n",
      "            1,   525,   136,   689,   525,   169,   689,    29,   525,    93,\n",
      "            7,     1,   115,    93,   115,     7,   169,    90,   131,   136,\n",
      "            7,     7,   689,   131,   426,    29,    88,   136,   131,    72,\n",
      "            1,   689,   136,     1,     7,   426,    60,     7,     7,   130,\n",
      "            1,   136,   426,     7,    29,   525,    26,   689,   136,   131])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([64,  1, 26, 68, 76, 34, 39, 61, 64, 67, 38, 40, 73, 26, 72, 19, 15, 29,\n",
      "         3, 68, 23,  1, 41, 95, 60, 76, 89, 10, 40, 23, 44, 12, 46, 15, 15, 40,\n",
      "        52, 90, 15, 35, 73, 19, 67, 94, 76, 69, 67, 68, 73, 28, 76, 76, 72, 46,\n",
      "        26, 45, 17, 77, 72, 73, 46, 89, 30, 69, 93, 61, 73,  3, 92, 68, 91, 68,\n",
      "        92, 40, 76, 63, 79, 41, 53, 79, 68, 68, 56, 76, 72, 66, 30, 16, 30,  4,\n",
      "         1, 47, 23, 68,  3, 27, 95, 77, 22, 26])\tToken idx: tensor([  131,   131,   131,     7,   689,   131,     1,     1,   136, 12072,\n",
      "            1,     7,     7,   136, 12072,     1,   689,     1,     7,   115,\n",
      "            1,   136,   131,   131,   131,     1,   131,     1,   115,     7,\n",
      "            1,     1,   525,     1,     7,     1,     1,     1,    90,   241,\n",
      "          689,     7,     1,  1732,    90,   525,   136,     1,   241,     1,\n",
      "            7,   426,   689,   131,   525,     1,   131,   689,     1,     1,\n",
      "          136,   136,     1,     7,     1,     7,   115,   115,     7,  1722,\n",
      "          525,   159,     1,    28,    29,     1,     7,   136,     1,     1,\n",
      "           78,    28,     1,   882,   136,     7,     7,     7,   241,     7,\n",
      "          169,     1,   115,    93,     1,     7,   136,   169,   131,   169])\tRaw indices: indices_to_process=tensor([2134723,   33484,  867309, 2268011, 2535517, 1134133, 1300768, 2034534,\n",
      "        2134728, 2246723, 1267415, 1334127, 2434776,  867314, 2413488,  633708,\n",
      "         500984,  967238,  100066, 2268119,  767120,   33489, 1367604, 3168666,\n",
      "        2001311, 2534829, 2968548,  333531, 1334235,  767126, 1467533,  400237,\n",
      "        1534763,  500296,  500302, 1334121, 1734357, 3001771,  500385, 1167596,\n",
      "        2435458,  633714, 2234652, 3136914, 2534918, 2301882, 2234787, 2268005,\n",
      "        2435010,  933885, 2534835, 2535254, 2402105, 1534369,  867703, 1500886,\n",
      "         567132, 2568870, 2401417, 2434770, 1534374, 2968553, 1000591, 2301364,\n",
      "        3101830, 2034540, 2434884,  100174, 3068483, 2269726, 3035648, 2268163,\n",
      "        3068477, 1334148, 2534857, 2101240, 2634894, 1367609, 1767710, 2634888,\n",
      "        2268082, 2268032, 1867769, 2535710, 2401552, 2201305, 1000597,  533655,\n",
      "        1000831,  133419,   33522, 1567592,  767234, 2268097,  100060,  900538,\n",
      "        3168671, 2568350,  733897,  867347])\n",
      "Beam idx: tensor([22,  5, 49,  5, 96, 14, 11, 81, 34, 39, 74,  5, 23, 74, 92,  7, 24, 12,\n",
      "        64, 79, 66, 30, 11, 94, 22, 96, 52, 96, 74, 43, 39, 92, 77, 79, 57, 22,\n",
      "        54, 94, 74, 82, 96,  5, 62, 16, 12, 85, 96, 11, 22, 91, 79, 38,  9, 72,\n",
      "         5, 96,  6,  5, 17, 86, 11,  5, 99, 22, 96, 35, 74, 32, 52, 85, 19, 50,\n",
      "        56, 74, 39,  5, 91,  5, 11, 39, 94, 12, 96,  5, 43, 79, 79,  5, 11,  5,\n",
      "        94, 64, 83, 39, 39, 94, 22, 62, 22, 12])\tToken idx: tensor([   1,   88,  131,    7,    7,    1,    7,    1,    1,  689,  136,    1,\n",
      "           1,  525,    1,    1,    1,    7,    1,  136,    1,    1,    1,    1,\n",
      "          93,  115,    7,    1,    1,    7,    1,    7,  689,    1,    1,    7,\n",
      "           7,  689,  471,    1,  241, 1722,  689,    1,    1,    1,  525,  689,\n",
      "         169, 1203,  525,    7,  689,    1,   90,   28,    1,  115,    1,    1,\n",
      "          93, 1346,    1,  689,  131,    1,  689,  131,    1,  241,  689,    1,\n",
      "           1,  131,  136,   28,   60,  159,  115,   93,    7,   93,   93,  689,\n",
      "           1,  471,  131,   93,  159,  369,  426,    7,    7,  169,   29,   90,\n",
      "          28,   90,   29,  689])\tRaw indices: indices_to_process=tensor([ 733767,  166853, 1634428,  166772, 3201895,  466943,  366890, 2701594,\n",
      "        1134003, 1301456, 2468258,  166766,  767120, 2468647, 3068477,  233472,\n",
      "         800473,  400243, 2134593, 2635023, 2201299, 1000591,  366884, 3135183,\n",
      "         733859, 3202003, 1734363, 3201889, 2468123, 1434186, 1300768, 3068483,\n",
      "        2568870, 2634888, 1901122,  733773, 1801069, 3135871, 2468593, 2734947,\n",
      "        3202129,  168487, 2068575,  533649,  400237, 2835006, 3202413,  367572,\n",
      "         733935, 3036326, 2635412, 1267421,  300866, 2401417,  166855, 3201916,\n",
      "         200119,  166880,  567002, 2868359,  366976,  168111, 3301948,  734455,\n",
      "        3202019, 1167356, 2468811, 1067427, 1734357, 2835246,  634396, 1667651,\n",
      "        1867769, 2468253, 1300903,  166793, 3035183,  166924,  366998, 1300860,\n",
      "        3135189,  400329, 3201981,  167454, 1434180, 2635358, 2635018,  166858,\n",
      "         367042,  167134, 3135608, 2134599, 2768306, 1300936, 1300796, 3135272,\n",
      "         733794, 2067976,  733795,  400925])\n",
      "Beam idx: tensor([91, 97, 39, 36, 43, 74, 66, 49, 66, 33, 91, 49, 64, 82, 80,  0, 44, 95,\n",
      "        18, 46, 34, 95, 66, 33, 43, 60, 36, 49, 16, 74, 16, 63,  5, 40, 39, 38,\n",
      "         5, 63, 49, 62, 66, 47, 27, 40, 25, 66,  4, 91, 66, 34, 79, 87, 66, 87,\n",
      "        91,  0, 70,  1, 86, 66,  8, 27, 46, 93, 82, 91, 49, 76, 66, 91, 32, 98,\n",
      "        56, 55, 99, 85, 11, 66, 63, 91, 49, 91, 93, 70, 82,  5, 66, 71, 20, 36,\n",
      "        85, 66, 91, 84, 66, 91, 87,  4, 96, 10])\tToken idx: tensor([ 131,  131,  131,  131, 1732,  131,  169,    7,  689,    1,  136,  241,\n",
      "           1,  689,  131,   88,    1,    7,  131,  131,  525,    1,  426,    7,\n",
      "         544,    1,  136,  525,    1,  136,    7,    7,   88,  689,  136,    1,\n",
      "           7,  689,  115,    7,   29,  131,  136,    7,    1,    1,    7,  169,\n",
      "         286,  136,    1,    7,   26,   88,   95,    7,    1,    7,    1,  136,\n",
      "           1,  131,  136,  136,    7, 2059,  130,    1,  131,  211,    7,    1,\n",
      "           1,    1,  525,    1,    1,   93,    1,   99,   72,    1,    1,  136,\n",
      "           1,  115,  230,    1,    1,    1,  689,  525,  261,  689,   95,  872,\n",
      "           1,    1,    1,    1])\tRaw indices: indices_to_process=tensor([3035254, 3235372, 1300898, 1200839, 1435911, 2468253, 2201467, 1634304,\n",
      "        2201987, 1100650, 3035259, 1634538, 2134593, 2735635, 2668371,      88,\n",
      "        1467533, 3168542,  600485, 1534369, 1134527, 3168536, 2201724, 1100656,\n",
      "        1434723, 2001181, 1200844, 1634822,  533649, 2468258,  533655, 2101246,\n",
      "         166853, 1334809, 1300903, 1267415,  166772, 2101928, 1634412, 2067893,\n",
      "        2201327, 1567722,  900667, 1334127,  833826, 2201299,  133419, 3035292,\n",
      "        2201584, 1134138, 2634888, 2901718, 2201324, 2901799, 3035218,       7,\n",
      "        2334711,   33360, 2868359, 2201434,  266825,  900662, 1534374, 3101965,\n",
      "        2734953, 3037182, 1634427, 2534829, 2201429, 3035334, 1067303, 3268595,\n",
      "        1867769, 1834416, 3302472, 2835006,  366884, 2201391, 2101240, 3035222,\n",
      "        1634369, 3035124, 3101830, 2334846, 2734947,  166880, 2201528, 2368064,\n",
      "         667061, 1200709, 2835694, 2201823, 3035384, 2802341, 2201393, 3035995,\n",
      "        2901712,  133413, 3201889,  333531])\n",
      "Beam idx: tensor([89, 54, 47, 32, 33, 34,  8, 13, 86, 67, 18, 69, 86, 57,  0, 87, 35, 49,\n",
      "        23, 76, 34, 59, 25, 49, 97, 59, 34, 34, 41, 13, 86, 86, 34, 24, 75, 57,\n",
      "        32, 99, 86, 23, 13, 68, 86, 48, 49, 50, 13, 89,  0,  9, 74, 24, 45, 93,\n",
      "        12, 69, 16, 10, 53, 34, 68, 34, 95,  9, 59,  5, 64, 68, 97, 91, 34, 18,\n",
      "        54, 23, 75, 59, 67, 53, 34, 87, 94, 42, 34, 25, 46, 49, 32, 49, 47, 41,\n",
      "        68, 64, 86, 42, 47, 67, 34, 34, 69,  8])\tToken idx: tensor([  131,   131,   525,     7,     1,   136,     1,   689,    28,   689,\n",
      "          131,     7,     7,   169,    88,   525,     1,   136,   136,   525,\n",
      "          689,    60,     7,   131,   241,  1203,     1,   471,   131,     7,\n",
      "            1,   241,   131,     1,   169,   689,     1,   241,   115,   131,\n",
      "            1,     7,   525,     1,     1,     1,    90,   525,     7,     7,\n",
      "            7,     7,   131,   525,   131,     1,     1,     1,     7,   169,\n",
      "            1,    29,   131,    88,   525,     1,   525,   115,     7,     1,\n",
      "        12072,   136,   136,   525,   689,     1,   882,     1,   525,   136,\n",
      "          689,   525,   426,     1,   131,   689,   115,   169,   131,   136,\n",
      "           88,   136,    72,   136,     1,    90,    93,   230,   115,   136])\tRaw indices: indices_to_process=tensor([2968548, 1801193, 1568116, 1067303, 1100650, 1134138,  266825,  434278,\n",
      "        2868386, 2235340,  600485, 2301364, 2868365, 1901290,      88, 2902236,\n",
      "        1167356, 1634433,  767255, 2535353, 1134691, 1967887,  833832, 1634428,\n",
      "        3235482, 1969030, 1134003, 1134473, 1367604,  433596, 2868359, 2868599,\n",
      "        1134133,  800473, 2501644, 1901810, 1067297, 3302188, 2868473,  767250,\n",
      "         433590, 2268011, 2868883, 1600945, 1634298, 1667651,  433679, 2968942,\n",
      "              7,  300184, 2468129,  800479, 1501016, 3102354,  400367, 2301358,\n",
      "         533649,  333531, 1767716, 1134171, 2268005, 1134031, 3168666,  300265,\n",
      "        1968352,  166766, 2135117, 2268119, 3235248, 3035124, 1146074,  600490,\n",
      "        1801198,  767644, 2502164, 1967828, 2235533, 1767710, 1134527, 2901847,\n",
      "        3135871, 1401351, 1134428,  833826, 1534369, 1634986, 1067411, 1634466,\n",
      "        1567722, 1367609, 2268092, 2134728, 2868430, 1400962, 1567592, 2234741,\n",
      "        1134095, 1134232, 2301472,  266960])\n",
      "DEB: t.shape=torch.Size([8, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      "\ttensor([  131,   131,   131,   131,   131,   525,     7,   136,   136,  1732,\n",
      "          525,   131,     1,    28,   131,     7,     1,   525,   131,   136,\n",
      "          115,     7,   525,   525,     7,     1,   136,     7,     1,   131,\n",
      "          131,  1732,   525,   689,   131,   131,   169,     7,     7,     7,\n",
      "          525,   525,   131,   261,     1,     1,     1,     7,   131,   131,\n",
      "          131,     7,     7,   115,     1,   136,   241,   136,   169,   525,\n",
      "          525,    95,     7,   131,   131,     7,   169,   115,   169,    99,\n",
      "          115,   115,     1,    93,   131,   525,     1,     7,   136,   169,\n",
      "          115,   131,   115,   689,   525,   525,   115,     7,   115,     7,\n",
      "          136,   130,     1,   159,     7,     1,   131,   241,     1,   525,\n",
      "        12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426,\n",
      "          131,   525,     1,     7,   136,   131,   131,   525,    60,   131,\n",
      "          131,   131,   131,   525,   525,     7,     1,   525,  1203,   130,\n",
      "          131,   525,   131,     7,   131,     1,     1,     1,     7,   136,\n",
      "          131,   115,   131,   525,     1,   115,   115,   131,   525,   525,\n",
      "          136,   525,   525,     1,   136,     7,     1,  1722,   525,   115,\n",
      "          525,     7,   136,   169,   131,   115,   169,   136,    93,     7,\n",
      "            1,   131,   131,   525,     1,   689,   241,     7,   525,   131,\n",
      "            7,   136,   131,     7,     1,     1,     1,   136,    93,   136,\n",
      "          241,   136,   131,  1406,   689,   689,   241,   241,   136,   689,\n",
      "          131,  1722,    28,   525,     1,   130,     7,   159,     1,     1,\n",
      "          131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\n",
      "\ttensor([  131,   115,   131,   525,   131, 12072,     7,   131,   689,     1,\n",
      "            1,   131,   131,   131,     7,     7,   525,   525,     7,    90,\n",
      "         1722,     1,   131,   131,    88,     7,   241,   131,   136,   525,\n",
      "            1,   131,     1,  1203,   136,    93,   131,   159,    60,   115,\n",
      "            1,    88,     7,     1,   241,     7,     7,     7,     1,   131,\n",
      "          115,     7,     1,     1,   689,   131,     1,   241,    88,    28,\n",
      "         1722,     1,     7,     1,   525,   131,   136,     1,    88,   525,\n",
      "            7,   136,   115,   136,     1,   131,     7,     1,     7,     7,\n",
      "            1,     1,     7,    72,   131,     1,     7,   261,     7,   525,\n",
      "            1,   115,    78,   169,   131,   115,     7,    87,   115,     1,\n",
      "          131,   131,   131,   131,   131,     7,     7,   131,   131,     1,\n",
      "        12072,     7,     7,     7,     1,   241,     7,     7,   131,   689,\n",
      "          525,     7,   131,   525,     7,     7,   131,   136,     7,     7,\n",
      "            1,     7,     7,     7,   525,     1,   115,   115,     1,   241,\n",
      "          136,     7,   115,   131,     7,     1,    88,     1,   131,   241,\n",
      "          115,   241,    88,     1,   131,     7,     1,   131,   689,   525,\n",
      "          136,    88,    90,   131,     1,    72,   115,     1,    88,   115,\n",
      "          136,     1,   689,     1,     1,   131,     1,     7,    29,     1,\n",
      "          115,   131,     7,   525,   115,    60,    88,   136,     7,   136,\n",
      "            1,     1,   525,   115,  1722,     1,   525,   241,   115,     1,\n",
      "          131,   525,   131,   131,   131,   136,   525,   131,   131,   131,\n",
      "          131,   131,   169,   525,   131,   131,   131,   525,     1,   689,\n",
      "          525,   525,  1732,   131,   131,   131,   130,   525,   131,   131,\n",
      "          136,   241,     7,    29,    93,     1,     1,  1732,     1,     1,\n",
      "          136,   689,     7,   230,     1,     1,   131,     7,   131,   136,\n",
      "          136,   136,     7,   426,   241,   689,   136,     7,    28,    90,\n",
      "            1,     7,   131,     1,   115,     7,   211,   131,     1,   525,\n",
      "          455,   136,    99,     7,     1,   471,    95,     1,     1,     1,\n",
      "          241,     7,   136,   131,   136,   136,     1,    72,    91,   510,\n",
      "          131,     1,   241,   525,   525,    76,   115,   131,   502,     1,\n",
      "          131,   131,   525,   136,   131,   525,     7,     7,     1,   131,\n",
      "            7,   241,   131,   131,     7,   136,     1,   689,     1,   130,\n",
      "            1,     7,    88,   525,   131,   131,     1,   169,   136,     1,\n",
      "            7,   136,   131,   115,   131,     1,   115,    95,   115,    99,\n",
      "            1,   136,   241,   115,     7,   169,     1,     1,   131,   525,\n",
      "          525,   211,    93,   525,   525,   471,   471,  2059,   704,     1,\n",
      "          131,   131,   131,   525,     7,   131,   169,   131,   131,   525,\n",
      "          241,    29,  1722,   872,     7,     1,  1203,   525,    28,   689,\n",
      "           72,    93,   211,     7,     1,   525,     1,    93,   131,     1,\n",
      "          579,   230,    29,     1,    28,   689,  1891,    95,    93,    93])\n",
      "\ttensor([  131,  1732,  1732,   136,   131,     7,   131,     7,   241,     1,\n",
      "            1,   131,     1,     1,     7,     1,   689,   525,   525,   136,\n",
      "            7,   131,     1,     1,   131,     1,     7,   689,     1,   115,\n",
      "         1203,   115,   136,   136,   525,    93,     1,     7,     1,     1,\n",
      "          159,   525,    72,     1,     1,    28,     7,     1,    88,    60,\n",
      "            7,   131,     1,   689,     7,   136,     1,     1,   131,   115,\n",
      "          241, 12072,  1722,     1,   115,    29,   169,    88,    28,     1,\n",
      "            7,   169,   689,    88,    90,   882,     7,    29,   131,   136,\n",
      "            7,     7,   136,  3856,    93,   544,    93,    72,   689,   115,\n",
      "            1,    93,   689,   136,   131,     1,   426,   471,   205,   471,\n",
      "          131, 12072,   131,   689,   131,   131,     7,     1,     1,   689,\n",
      "          136, 12072,   131,     7,     1,   131,     1,   115,     7,   131,\n",
      "          169,     1,   241,     1,     1,    88,   136, 12072,   136,   136,\n",
      "          131,   131,   115,     1,     1,     7,   136,   131,   136,   131,\n",
      "          131,     1,   525,   131,   136,   426,     7,   136,     1,   169,\n",
      "            1,     1,   131,     1,   882,     1,     1,     1,     7,    72,\n",
      "          689,   689,     1,   525,     1,     1,   689,   115,  1722,   369,\n",
      "          525,    93,   136,     1,     7,    90,   689,   525,   169,   115,\n",
      "            7,     1,     1,   241,   525,    93,     1,   525,     7,   525,\n",
      "            1,     7,     1,   525,   205,    90, 14399,   136,    95,     1,\n",
      "          131,   131,   131,   131,   131,   131,   131,   136, 12072,     1,\n",
      "          525,     7,     7,     7,   131,   525,   525,   525,   525,     1,\n",
      "            7,     7,   525,   136,   525,   131,   136,     7,    88,     1,\n",
      "            7,   136,   689,     1,    88,   525,   169,    88,     7,     7,\n",
      "          689,   131,     7,     7,     1,   131,     7,     7,     1,   136,\n",
      "            7,   115,     7,     1,   131,   115,   131,     1,   525,   115,\n",
      "            1,     1,    88,   689,     7,   131,   115,   131,   241,     1,\n",
      "            1,     1,   136,   131,   115,   169,     1,   136,    93,     7,\n",
      "            1,     1,     7,   136,   471,     7,   115,   131,   131,     7,\n",
      "            7,    28,   130,     1,   241,     1,     1,   525,   689,   130,\n",
      "          131,  1732,   131,   131,   131,     1,   525,   689,     7,   131,\n",
      "            1,     1,     1,     7,     7,     7,   131,   426,     1,     1,\n",
      "          136,     7,   241,     7,     1,   169,   525,     1,   115,     7,\n",
      "           88,     7,    88,     1,   115,     1,     7,     1,   525,   131,\n",
      "            7,   525,     1,  1722,   136,  1203,     7,    60,     7,   136,\n",
      "          136,   131,   525,     1,    88,   525,     1,   241,    90,   115,\n",
      "          115,   131,   131,     7,   241,   525,     1,     7,   131,   136,\n",
      "            7,     7,   525,   131,   136,   241,   115,   689,   525,   115,\n",
      "            7,     7,    29,     1,    93,   525,   159,   115,     1,   136,\n",
      "          131,   205,     1,   241,     7,   136,   131,    93,   689,   689])\n",
      "\ttensor([  131,   525,   131,   131,   136,  1732,  1732,     7,     7,   131,\n",
      "            1,     7,     1,     1,   689,   689,   136,     1,   131,     1,\n",
      "           88,     7,   525,   136,     7,   136,   525,   131,    88,     1,\n",
      "            1,     7,   115,     1,   525,     1,   136,     7,     1,     1,\n",
      "          689,     7,     1,     7,     1,     1,   241,     1,    90,   241,\n",
      "            1,   169,   136,     1,    29,     7,   689,     1,   882,     1,\n",
      "          130,     1,   136,   471,   525,    88,   689,   115,  2059,  1203,\n",
      "            1,    60,   115,   689,     7,   131,   689,     7,   230,    88,\n",
      "          169,   115,   426,   471,     1,   136,    93,   169,     1,   241,\n",
      "          169,     1,     1,     1,  1722,   525,   115,     7,    29,   115,\n",
      "          131,   131,  1732,     1,     1,   131,     1,     1,     1,     7,\n",
      "          689,   136, 12072,   689,     1,    88,   136,     1,     7,     7,\n",
      "           88,     1,     1,     1,     1,   241,   241,     7,     7,     7,\n",
      "            7,     7,   525,     7,     1,   131,   136,     1,   689,   115,\n",
      "          115,    88,     1,   136,     1,     1,     7,     7,     7,   525,\n",
      "            1,     7,   131,   525,   136,     7,   689,     7,   131,   169,\n",
      "          544,    88,   689,   115,   136,     7,     1,   169,     7,    29,\n",
      "           88,   115,     1,  1722,     7,   131,   689,   689,   689,     7,\n",
      "            7,     1,     1,   131,     1,     1,     7,   131,     1,   169,\n",
      "          115,    93,   136,     1,  3856,   241,    60,   115,     1,    72,\n",
      "          131,   131,   131,   131,   136,   131,   131,     7,     1,   131,\n",
      "            1,     1,     7,   136,   131,     7,   136,    88,   525,    88,\n",
      "            1,   689,     1,   131,   115,     1,   689,     7,     1,    93,\n",
      "          136,     1,   131,   131,     7,   131,   525,   241,   136,   525,\n",
      "          689,     7,   136,  1722,     1,   136,   525,   525,    60,  1203,\n",
      "          159,     7,   136,  1722,    28,     1,     1,   136,     1,     7,\n",
      "          169,     7,   131,    88,     1,    93,    29,   136,     1,    90,\n",
      "            7,  1346,   159,    88,   525,   426,   369,   136,    90,     1,\n",
      "          525,   689,   689,   205,    29,    28,  1346,   131,   689,     7,\n",
      "           93,   525,   136,   115,     1,   136,  1891,   130,     1,    93,\n",
      "          131,  1732,     7,   241,     1,   131,     1,     1,     1,   131,\n",
      "          131,   136,   525,   689,   131,   131,   169,     1,   525,   544,\n",
      "         1732,   131,     1,     7,   131,   689,   689,   426,     7,   136,\n",
      "            1,     1,   136,     1,     7,   131,   131,   169,     7,   115,\n",
      "          136,     7,     7,     7,   131,   525,   525,   241,     1,   115,\n",
      "            1,   525,   136,   689,   525,   169,   689,    29,   525,    93,\n",
      "            7,     1,   115,    93,   115,     7,   169,    90,   131,   136,\n",
      "            7,     7,   689,   131,   426,    29,    88,   136,   131,    72,\n",
      "            1,   689,   136,     1,     7,   426,    60,     7,     7,   130,\n",
      "            1,   136,   426,     7,    29,   525,    26,   689,   136,   131])\n",
      "\ttensor([  131,   131,   131,     7,   689,   131,     1,     1,   136, 12072,\n",
      "            1,     7,     7,   136, 12072,     1,   689,     1,     7,   115,\n",
      "            1,   136,   131,   131,   131,     1,   131,     1,   115,     7,\n",
      "            1,     1,   525,     1,     7,     1,     1,     1,    90,   241,\n",
      "          689,     7,     1,  1732,    90,   525,   136,     1,   241,     1,\n",
      "            7,   426,   689,   131,   525,     1,   131,   689,     1,     1,\n",
      "          136,   136,     1,     7,     1,     7,   115,   115,     7,  1722,\n",
      "          525,   159,     1,    28,    29,     1,     7,   136,     1,     1,\n",
      "           78,    28,     1,   882,   136,     7,     7,     7,   241,     7,\n",
      "          169,     1,   115,    93,     1,     7,   136,   169,   131,   169,\n",
      "            1,    88,   131,     7,     7,     1,     7,     1,     1,   689,\n",
      "          136,     1,     1,   525,     1,     1,     1,     7,     1,   136,\n",
      "            1,     1,     1,     1,    93,   115,     7,     1,     1,     7,\n",
      "            1,     7,   689,     1,     1,     7,     7,   689,   471,     1,\n",
      "          241,  1722,   689,     1,     1,     1,   525,   689,   169,  1203,\n",
      "          525,     7,   689,     1,    90,    28,     1,   115,     1,     1,\n",
      "           93,  1346,     1,   689,   131,     1,   689,   131,     1,   241,\n",
      "          689,     1,     1,   131,   136,    28,    60,   159,   115,    93,\n",
      "            7,    93,    93,   689,     1,   471,   131,    93,   159,   369,\n",
      "          426,     7,     7,   169,    29,    90,    28,    90,    29,   689,\n",
      "          131,   131,   131,   131,  1732,   131,   169,     7,   689,     1,\n",
      "          136,   241,     1,   689,   131,    88,     1,     7,   131,   131,\n",
      "          525,     1,   426,     7,   544,     1,   136,   525,     1,   136,\n",
      "            7,     7,    88,   689,   136,     1,     7,   689,   115,     7,\n",
      "           29,   131,   136,     7,     1,     1,     7,   169,   286,   136,\n",
      "            1,     7,    26,    88,    95,     7,     1,     7,     1,   136,\n",
      "            1,   131,   136,   136,     7,  2059,   130,     1,   131,   211,\n",
      "            7,     1,     1,     1,   525,     1,     1,    93,     1,    99,\n",
      "           72,     1,     1,   136,     1,   115,   230,     1,     1,     1,\n",
      "          689,   525,   261,   689,    95,   872,     1,     1,     1,     1,\n",
      "          131,   131,   525,     7,     1,   136,     1,   689,    28,   689,\n",
      "          131,     7,     7,   169,    88,   525,     1,   136,   136,   525,\n",
      "          689,    60,     7,   131,   241,  1203,     1,   471,   131,     7,\n",
      "            1,   241,   131,     1,   169,   689,     1,   241,   115,   131,\n",
      "            1,     7,   525,     1,     1,     1,    90,   525,     7,     7,\n",
      "            7,     7,   131,   525,   131,     1,     1,     1,     7,   169,\n",
      "            1,    29,   131,    88,   525,     1,   525,   115,     7,     1,\n",
      "        12072,   136,   136,   525,   689,     1,   882,     1,   525,   136,\n",
      "          689,   525,   426,     1,   131,   689,   115,   169,   131,   136,\n",
      "           88,   136,    72,   136,     1,    90,    93,   230,   115,   136])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([32, 70, 32, 86, 96, 55, 51,  7, 82, 84, 67, 62, 34, 74,  0, 96,  8, 96,\n",
      "        37, 83, 25, 20, 70, 71, 57, 84, 48, 96, 96, 33, 96, 83, 21, 13, 83, 81,\n",
      "        10, 54, 31,  2, 42, 59, 83, 34, 51, 36, 84, 78,  7, 96, 99, 51, 83,  2,\n",
      "        37, 60, 32, 38, 58, 48, 96,  8, 96,  9, 96, 39, 82, 96, 75,  0, 21, 96,\n",
      "        37,  8, 97, 48, 83, 55, 57, 96, 64, 96, 48, 18,  7, 81, 51, 62,  8, 96,\n",
      "        75, 82, 18, 96,  6,  7, 70, 82, 37,  7])\tToken idx: tensor([  131,   131,   136,   525,     1,     1,     7,     1,     1,     1,\n",
      "        12072,     1,   131,   689,    88,   115,   689,     7,     1,     7,\n",
      "            1,     1,   136,    28,     1,     7,   689,   689,    93,     1,\n",
      "          426,    90,     1,     1,   689,     1,     1,   131,     1,    88,\n",
      "            1,     1,     1,   136,     1,     1,   241,     1,   689,   159,\n",
      "            1,    90,   369,     7,   689,   131,   169,     1,     1,   169,\n",
      "          369,     7,    29,     1,   169,   525,     7,    90,     1,     7,\n",
      "            7,  1023,   136,     1,     1,   136,   205,   689,     7,   230,\n",
      "            1,    26,   525,   689,    93,   136,   369,     7,   169,    50,\n",
      "            7,    90,     1,   455,     1,    29,   525,   369,    29,   136])\tRaw indices: indices_to_process=tensor([1067427, 2334841, 1067432, 2868883, 3201889, 1834416, 1701010,  233472,\n",
      "        2734947, 2801653, 2246723, 2067887, 1134133, 2468811,      88, 3202003,\n",
      "         267513, 3201895, 1234062, 2768306,  833826,  667061, 2334846, 2368091,\n",
      "        1901122, 2801659, 1601633, 3202577, 3201981, 1100650, 3202314, 2768389,\n",
      "         700414,  433590, 2768988, 2701594,  333531, 1801193, 1033944,   66794,\n",
      "        1400827, 1967828, 2768300, 1134138, 1701004, 1200709, 2801893, 2601535,\n",
      "         234160, 3202047, 3301948, 1701093, 2768668,   66713, 1234750, 2001311,\n",
      "        1067465, 1267415, 1934475, 1601113, 3202257,  266831, 3201917,  300178,\n",
      "        3202057, 1301292, 2734953, 3201978, 2501476,       7,  700420, 3202911,\n",
      "        1234197,  266825, 3235242, 1601080, 2768504, 1835104, 1901128, 3202118,\n",
      "        2134593, 3201914, 1601469,  601043,  233564, 2701729, 1701372, 2067893,\n",
      "         266993, 3201938, 2501482, 2735036,  600355, 3202343,  200119,  233500,\n",
      "        2335235, 2735315, 1234090,  233607])\n",
      "Beam idx: tensor([46, 49, 46, 20, 97, 13, 28, 93, 41, 49, 31, 67, 87, 14, 44,  8, 21, 69,\n",
      "        18, 84, 51, 76, 34, 31, 80, 25, 45, 76,  2, 69, 51, 58, 67, 31, 59,  7,\n",
      "        53, 69,  1,  2, 80, 25, 29, 23, 80, 29,  0, 20, 51, 33, 60, 25, 80, 81,\n",
      "        48, 10, 90, 95, 87, 69, 42, 51, 69, 97, 13, 98, 65, 47, 87, 67, 70, 71,\n",
      "        25, 88, 51, 27, 41, 97, 18, 49, 73,  4, 69, 87, 63, 42, 49, 68, 69,  2,\n",
      "        80, 46, 78, 51, 89, 93, 81, 31, 31,  2])\tToken idx: tensor([ 131,  131,  136,    1,  689,  131,    1,    1, 1732,  525,  525,    1,\n",
      "         241,    1,    1,    1,    1,  169,    1,    1,  136,  241,    1,  689,\n",
      "         136,  136,    1,    7,    7,  689,  525,    1,  115,  136,    1,    1,\n",
      "           1,  525,    7,   88,  471,  131,  689,    1,  525,  136,    1,    7,\n",
      "           1,    1,  241,  525,  689,    1,    1,    7,    1,  689,   72,  136,\n",
      "           1,  131,  131,  426,  136,    1,    1,  689, 1203,    7,    7,    1,\n",
      "         169,   28,  689,    1,  544,  882,  689,    7,    1,    1,  426,   60,\n",
      "         689,  689,  130,    1,   29,  115,  131,  169,  689,  169,    1,    7,\n",
      "          60,  882,  130,    1])\tRaw indices: indices_to_process=tensor([1534369, 1634428, 1534374,  667061, 3235930,  433720,  933885, 3101830,\n",
      "        1369205, 1634822, 1034468, 2234652, 2901952,  466943, 1467533,  266825,\n",
      "         700414, 2301526,  600355, 2801653, 1701139, 2535069, 1134003, 1034632,\n",
      "        2668376,  833961, 1500886, 2534835,   66713, 2302046, 1701528, 1934475,\n",
      "        2234766, 1034079, 1967828,  233472, 1767710, 2301882,   33360,   66794,\n",
      "        2668711,  833956,  967926,  767120, 2668765,  967373,       1,  667067,\n",
      "        1701004, 1100650, 2001421,  834350, 2668929, 2701594, 1600945,  333537,\n",
      "        3001771, 3169224, 2901783, 2301493, 1400827, 1701134, 2301488, 3235667,\n",
      "         433725, 3268595, 2167946, 1568280, 2902914, 2234658, 2334717, 2368064,\n",
      "         833994, 2935092, 1701692,  900532, 1368017, 3236123,  601043, 1634304,\n",
      "        2434770,  133413, 2301783, 2901771, 2101928, 1401515, 1634427, 2268005,\n",
      "        2301386,   66821, 2668371, 1534407, 2602223, 1701172, 2968418, 3101836,\n",
      "        2701653, 1034825, 1034073,   66707])\n",
      "Beam idx: tensor([74, 97, 81, 28, 75, 54, 19, 94,  9,  3, 48, 37, 12, 80, 37, 89, 19, 44,\n",
      "        66, 58, 94, 45, 31,  2, 27, 91, 24,  7, 60, 69, 14, 90, 89,  7, 92, 26,\n",
      "        31, 68, 35, 81, 78, 54, 72, 40, 52, 97, 19, 57, 88,  3, 13, 48, 57, 28,\n",
      "        13, 29, 65, 54,  0, 25, 52, 77, 29, 17, 67, 88, 12,  2, 19, 72,  6, 50,\n",
      "        31, 39, 65, 33, 52, 80, 31, 81,  7, 81, 21,  7, 14, 90, 37,  9, 20, 15,\n",
      "        69, 69, 83, 97, 77,  7, 89, 89, 69, 73])\tToken idx: tensor([131, 131,   1,   1,   1,   7,   7,   1,   1,  88,   7, 689,   1, 131,\n",
      "          7,   1,   1,   1, 131,   1,   7,   1,   1,  88, 131, 131,   1, 136,\n",
      "          1,   1,  88, 241, 689, 689,   7,   7, 689,   1,   1, 689,   1,   1,\n",
      "          7, 689,  39, 525, 115,   1,   1,   7,   7, 115, 689,   7, 689, 241,\n",
      "          7, 115,  88,   1,   7, 241,   1, 131,   1,   7, 689,   7, 241,   1,\n",
      "          7,   1, 136, 131, 241,   7,   1, 136, 169,   7, 131,  90,   1,   1,\n",
      "          7, 689,   1,   7, 131, 131,  90, 689,   7,   1,  72, 169,  29, 136,\n",
      "        205,   1])\tRaw indices: indices_to_process=tensor([2468253, 3235372, 2701594,  933885, 2501476, 1801069,  633714, 3135183,\n",
      "         300178,  100147, 1600951, 1234750,  400237, 2668371, 1234068, 2968418,\n",
      "         633708, 1467533, 2201429, 1934475, 3135189, 1500886, 1033944,   66794,\n",
      "         900662, 3035254,  800473,  233607, 2001181, 2301358,  467030, 3002011,\n",
      "        2969106,  234160, 3068483,  867185, 1034632, 2268005, 1167356, 2702282,\n",
      "        2601535, 1801063, 2401423, 1334809, 1734395, 3235766,  633822, 1901122,\n",
      "        2935065,  100066,  433596, 1601059, 1901810,  933891,  434278,  967478,\n",
      "        2167952, 1801177,      88,  833826, 1734363, 2568422,  967238,  567132,\n",
      "        2234652, 2935071,  400925,   66713,  633948, 2401417,  200125, 1667651,\n",
      "        1034079, 1300898, 2168186, 1100656, 1734357, 2668376, 1034112, 2701600,\n",
      "         233602, 2701683,  700414,  233472,  466949, 3002459, 1234062,  300184,\n",
      "         667191,  500426, 2301447, 2302046, 2768306, 3235242, 2568253,  233640,\n",
      "        2968446, 2968553, 2301562, 2434770])\n",
      "Beam idx: tensor([19,  7, 36, 85, 15, 40, 21, 81,  2, 77,  1, 44, 60, 39, 85, 78,  7, 26,\n",
      "         4, 88, 33, 83, 39,  7, 15, 39, 85, 30,  1, 21, 29, 19, 21, 71, 96,  1,\n",
      "        56, 20, 47, 44, 77, 64, 42, 21, 15, 38,  1,  6, 15,  7, 39, 85,  2, 29,\n",
      "        43, 81, 52, 55, 11, 85, 36, 15, 88, 27, 85, 38, 11, 21,  0, 85, 15, 15,\n",
      "        37, 82, 14,  7, 53, 51, 39, 14, 21, 58, 90, 35, 58, 14, 78, 15, 21, 62,\n",
      "        26, 77, 51, 35, 88, 36, 72, 29, 40, 85])\tToken idx: tensor([ 131,  689,    1,    7,  131,    1,    7,  131,  131,    1,    7,    1,\n",
      "           1,   88,   90,  131,   90,    1,    1,    1,    1,    1,    7,    7,\n",
      "         136,    1,  689,    1,  115,    1,  689,  136,  241,  689,  241,   88,\n",
      "           1,    7,  131,    7,    7,  131,  131,   28,  689,  136,    1,    1,\n",
      "         169,    1,  115,    1,  136,    1,    1,  136,    7,    1,  689,  369,\n",
      "           7,    7,    7,    1,  205,  131,    1,  115,   88,  115,   29,  230,\n",
      "           1,    7,  689,  205,  131,  131, 1722,    7,   93,  136,    1,    1,\n",
      "         131,    1,  136,    1,   90,   88,    7,    3,  136,    7,   88,   93,\n",
      "           7,  169,    7,  426])\tRaw indices: indices_to_process=tensor([ 633838,  234160, 1200709, 2835012,  500426, 1334121,  700420, 2701724,\n",
      "          66837, 2568182,   33360, 1467533, 2001181, 1300855, 2835095, 2601665,\n",
      "         233561,  867179,  133413, 2935065, 1100650, 2768300, 1300774,  233478,\n",
      "         500431, 1300768, 2835694, 1000591,   33468,  700414,  967926,  633843,\n",
      "         700654, 2368752, 3202129,   33441, 1867769,  667067, 1567722, 1467539,\n",
      "        2568188, 2134723, 1400957,  700441,  500984, 1267550,   33354,  200119,\n",
      "         500464,  233472, 1300882, 2835006,   66842,  967238, 1434180, 2701729,\n",
      "        1734363, 1834416,  367572, 2835374, 1200715,  500302, 2935071,  900532,\n",
      "        2835210, 1267545,  366884,  700528,      88, 2835120,  500324,  500525,\n",
      "        1234062, 2734953,  467631,  233676, 1767840, 1701134, 1302489,  466949,\n",
      "         700506, 1934610, 3001771, 1167356, 1934605,  466943, 2601670,  500296,\n",
      "         700503, 2067974,  867185, 2568184, 1701139, 1167362, 2935152, 1200801,\n",
      "        2401423,  967406, 1334127, 2835431])\n",
      "DEB: t.shape=torch.Size([9, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      "\ttensor([  131,   131,   131,   131,   131,   525,     7,   136,   136,  1732,\n",
      "          525,   131,     1,    28,   131,     7,     1,   525,   131,   136,\n",
      "          115,     7,   525,   525,     7,     1,   136,     7,     1,   131,\n",
      "          131,  1732,   525,   689,   131,   131,   169,     7,     7,     7,\n",
      "          525,   525,   131,   261,     1,     1,     1,     7,   131,   131,\n",
      "          131,     7,     7,   115,     1,   136,   241,   136,   169,   525,\n",
      "          525,    95,     7,   131,   131,     7,   169,   115,   169,    99,\n",
      "          115,   115,     1,    93,   131,   525,     1,     7,   136,   169,\n",
      "          115,   131,   115,   689,   525,   525,   115,     7,   115,     7,\n",
      "          136,   130,     1,   159,     7,     1,   131,   241,     1,   525,\n",
      "        12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426,\n",
      "          131,   525,     1,     7,   136,   131,   131,   525,    60,   131,\n",
      "          131,   131,   131,   525,   525,     7,     1,   525,  1203,   130,\n",
      "          131,   525,   131,     7,   131,     1,     1,     1,     7,   136,\n",
      "          131,   115,   131,   525,     1,   115,   115,   131,   525,   525,\n",
      "          136,   525,   525,     1,   136,     7,     1,  1722,   525,   115,\n",
      "          525,     7,   136,   169,   131,   115,   169,   136,    93,     7,\n",
      "            1,   131,   131,   525,     1,   689,   241,     7,   525,   131,\n",
      "            7,   136,   131,     7,     1,     1,     1,   136,    93,   136,\n",
      "          241,   136,   131,  1406,   689,   689,   241,   241,   136,   689,\n",
      "          131,  1722,    28,   525,     1,   130,     7,   159,     1,     1,\n",
      "          131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\n",
      "\ttensor([  131,   115,   131,   525,   131, 12072,     7,   131,   689,     1,\n",
      "            1,   131,   131,   131,     7,     7,   525,   525,     7,    90,\n",
      "         1722,     1,   131,   131,    88,     7,   241,   131,   136,   525,\n",
      "            1,   131,     1,  1203,   136,    93,   131,   159,    60,   115,\n",
      "            1,    88,     7,     1,   241,     7,     7,     7,     1,   131,\n",
      "          115,     7,     1,     1,   689,   131,     1,   241,    88,    28,\n",
      "         1722,     1,     7,     1,   525,   131,   136,     1,    88,   525,\n",
      "            7,   136,   115,   136,     1,   131,     7,     1,     7,     7,\n",
      "            1,     1,     7,    72,   131,     1,     7,   261,     7,   525,\n",
      "            1,   115,    78,   169,   131,   115,     7,    87,   115,     1,\n",
      "          131,   131,   131,   131,   131,     7,     7,   131,   131,     1,\n",
      "        12072,     7,     7,     7,     1,   241,     7,     7,   131,   689,\n",
      "          525,     7,   131,   525,     7,     7,   131,   136,     7,     7,\n",
      "            1,     7,     7,     7,   525,     1,   115,   115,     1,   241,\n",
      "          136,     7,   115,   131,     7,     1,    88,     1,   131,   241,\n",
      "          115,   241,    88,     1,   131,     7,     1,   131,   689,   525,\n",
      "          136,    88,    90,   131,     1,    72,   115,     1,    88,   115,\n",
      "          136,     1,   689,     1,     1,   131,     1,     7,    29,     1,\n",
      "          115,   131,     7,   525,   115,    60,    88,   136,     7,   136,\n",
      "            1,     1,   525,   115,  1722,     1,   525,   241,   115,     1,\n",
      "          131,   525,   131,   131,   131,   136,   525,   131,   131,   131,\n",
      "          131,   131,   169,   525,   131,   131,   131,   525,     1,   689,\n",
      "          525,   525,  1732,   131,   131,   131,   130,   525,   131,   131,\n",
      "          136,   241,     7,    29,    93,     1,     1,  1732,     1,     1,\n",
      "          136,   689,     7,   230,     1,     1,   131,     7,   131,   136,\n",
      "          136,   136,     7,   426,   241,   689,   136,     7,    28,    90,\n",
      "            1,     7,   131,     1,   115,     7,   211,   131,     1,   525,\n",
      "          455,   136,    99,     7,     1,   471,    95,     1,     1,     1,\n",
      "          241,     7,   136,   131,   136,   136,     1,    72,    91,   510,\n",
      "          131,     1,   241,   525,   525,    76,   115,   131,   502,     1,\n",
      "          131,   131,   525,   136,   131,   525,     7,     7,     1,   131,\n",
      "            7,   241,   131,   131,     7,   136,     1,   689,     1,   130,\n",
      "            1,     7,    88,   525,   131,   131,     1,   169,   136,     1,\n",
      "            7,   136,   131,   115,   131,     1,   115,    95,   115,    99,\n",
      "            1,   136,   241,   115,     7,   169,     1,     1,   131,   525,\n",
      "          525,   211,    93,   525,   525,   471,   471,  2059,   704,     1,\n",
      "          131,   131,   131,   525,     7,   131,   169,   131,   131,   525,\n",
      "          241,    29,  1722,   872,     7,     1,  1203,   525,    28,   689,\n",
      "           72,    93,   211,     7,     1,   525,     1,    93,   131,     1,\n",
      "          579,   230,    29,     1,    28,   689,  1891,    95,    93,    93])\n",
      "\ttensor([  131,  1732,  1732,   136,   131,     7,   131,     7,   241,     1,\n",
      "            1,   131,     1,     1,     7,     1,   689,   525,   525,   136,\n",
      "            7,   131,     1,     1,   131,     1,     7,   689,     1,   115,\n",
      "         1203,   115,   136,   136,   525,    93,     1,     7,     1,     1,\n",
      "          159,   525,    72,     1,     1,    28,     7,     1,    88,    60,\n",
      "            7,   131,     1,   689,     7,   136,     1,     1,   131,   115,\n",
      "          241, 12072,  1722,     1,   115,    29,   169,    88,    28,     1,\n",
      "            7,   169,   689,    88,    90,   882,     7,    29,   131,   136,\n",
      "            7,     7,   136,  3856,    93,   544,    93,    72,   689,   115,\n",
      "            1,    93,   689,   136,   131,     1,   426,   471,   205,   471,\n",
      "          131, 12072,   131,   689,   131,   131,     7,     1,     1,   689,\n",
      "          136, 12072,   131,     7,     1,   131,     1,   115,     7,   131,\n",
      "          169,     1,   241,     1,     1,    88,   136, 12072,   136,   136,\n",
      "          131,   131,   115,     1,     1,     7,   136,   131,   136,   131,\n",
      "          131,     1,   525,   131,   136,   426,     7,   136,     1,   169,\n",
      "            1,     1,   131,     1,   882,     1,     1,     1,     7,    72,\n",
      "          689,   689,     1,   525,     1,     1,   689,   115,  1722,   369,\n",
      "          525,    93,   136,     1,     7,    90,   689,   525,   169,   115,\n",
      "            7,     1,     1,   241,   525,    93,     1,   525,     7,   525,\n",
      "            1,     7,     1,   525,   205,    90, 14399,   136,    95,     1,\n",
      "          131,   131,   131,   131,   131,   131,   131,   136, 12072,     1,\n",
      "          525,     7,     7,     7,   131,   525,   525,   525,   525,     1,\n",
      "            7,     7,   525,   136,   525,   131,   136,     7,    88,     1,\n",
      "            7,   136,   689,     1,    88,   525,   169,    88,     7,     7,\n",
      "          689,   131,     7,     7,     1,   131,     7,     7,     1,   136,\n",
      "            7,   115,     7,     1,   131,   115,   131,     1,   525,   115,\n",
      "            1,     1,    88,   689,     7,   131,   115,   131,   241,     1,\n",
      "            1,     1,   136,   131,   115,   169,     1,   136,    93,     7,\n",
      "            1,     1,     7,   136,   471,     7,   115,   131,   131,     7,\n",
      "            7,    28,   130,     1,   241,     1,     1,   525,   689,   130,\n",
      "          131,  1732,   131,   131,   131,     1,   525,   689,     7,   131,\n",
      "            1,     1,     1,     7,     7,     7,   131,   426,     1,     1,\n",
      "          136,     7,   241,     7,     1,   169,   525,     1,   115,     7,\n",
      "           88,     7,    88,     1,   115,     1,     7,     1,   525,   131,\n",
      "            7,   525,     1,  1722,   136,  1203,     7,    60,     7,   136,\n",
      "          136,   131,   525,     1,    88,   525,     1,   241,    90,   115,\n",
      "          115,   131,   131,     7,   241,   525,     1,     7,   131,   136,\n",
      "            7,     7,   525,   131,   136,   241,   115,   689,   525,   115,\n",
      "            7,     7,    29,     1,    93,   525,   159,   115,     1,   136,\n",
      "          131,   205,     1,   241,     7,   136,   131,    93,   689,   689])\n",
      "\ttensor([  131,   525,   131,   131,   136,  1732,  1732,     7,     7,   131,\n",
      "            1,     7,     1,     1,   689,   689,   136,     1,   131,     1,\n",
      "           88,     7,   525,   136,     7,   136,   525,   131,    88,     1,\n",
      "            1,     7,   115,     1,   525,     1,   136,     7,     1,     1,\n",
      "          689,     7,     1,     7,     1,     1,   241,     1,    90,   241,\n",
      "            1,   169,   136,     1,    29,     7,   689,     1,   882,     1,\n",
      "          130,     1,   136,   471,   525,    88,   689,   115,  2059,  1203,\n",
      "            1,    60,   115,   689,     7,   131,   689,     7,   230,    88,\n",
      "          169,   115,   426,   471,     1,   136,    93,   169,     1,   241,\n",
      "          169,     1,     1,     1,  1722,   525,   115,     7,    29,   115,\n",
      "          131,   131,  1732,     1,     1,   131,     1,     1,     1,     7,\n",
      "          689,   136, 12072,   689,     1,    88,   136,     1,     7,     7,\n",
      "           88,     1,     1,     1,     1,   241,   241,     7,     7,     7,\n",
      "            7,     7,   525,     7,     1,   131,   136,     1,   689,   115,\n",
      "          115,    88,     1,   136,     1,     1,     7,     7,     7,   525,\n",
      "            1,     7,   131,   525,   136,     7,   689,     7,   131,   169,\n",
      "          544,    88,   689,   115,   136,     7,     1,   169,     7,    29,\n",
      "           88,   115,     1,  1722,     7,   131,   689,   689,   689,     7,\n",
      "            7,     1,     1,   131,     1,     1,     7,   131,     1,   169,\n",
      "          115,    93,   136,     1,  3856,   241,    60,   115,     1,    72,\n",
      "          131,   131,   131,   131,   136,   131,   131,     7,     1,   131,\n",
      "            1,     1,     7,   136,   131,     7,   136,    88,   525,    88,\n",
      "            1,   689,     1,   131,   115,     1,   689,     7,     1,    93,\n",
      "          136,     1,   131,   131,     7,   131,   525,   241,   136,   525,\n",
      "          689,     7,   136,  1722,     1,   136,   525,   525,    60,  1203,\n",
      "          159,     7,   136,  1722,    28,     1,     1,   136,     1,     7,\n",
      "          169,     7,   131,    88,     1,    93,    29,   136,     1,    90,\n",
      "            7,  1346,   159,    88,   525,   426,   369,   136,    90,     1,\n",
      "          525,   689,   689,   205,    29,    28,  1346,   131,   689,     7,\n",
      "           93,   525,   136,   115,     1,   136,  1891,   130,     1,    93,\n",
      "          131,  1732,     7,   241,     1,   131,     1,     1,     1,   131,\n",
      "          131,   136,   525,   689,   131,   131,   169,     1,   525,   544,\n",
      "         1732,   131,     1,     7,   131,   689,   689,   426,     7,   136,\n",
      "            1,     1,   136,     1,     7,   131,   131,   169,     7,   115,\n",
      "          136,     7,     7,     7,   131,   525,   525,   241,     1,   115,\n",
      "            1,   525,   136,   689,   525,   169,   689,    29,   525,    93,\n",
      "            7,     1,   115,    93,   115,     7,   169,    90,   131,   136,\n",
      "            7,     7,   689,   131,   426,    29,    88,   136,   131,    72,\n",
      "            1,   689,   136,     1,     7,   426,    60,     7,     7,   130,\n",
      "            1,   136,   426,     7,    29,   525,    26,   689,   136,   131])\n",
      "\ttensor([  131,   131,   131,     7,   689,   131,     1,     1,   136, 12072,\n",
      "            1,     7,     7,   136, 12072,     1,   689,     1,     7,   115,\n",
      "            1,   136,   131,   131,   131,     1,   131,     1,   115,     7,\n",
      "            1,     1,   525,     1,     7,     1,     1,     1,    90,   241,\n",
      "          689,     7,     1,  1732,    90,   525,   136,     1,   241,     1,\n",
      "            7,   426,   689,   131,   525,     1,   131,   689,     1,     1,\n",
      "          136,   136,     1,     7,     1,     7,   115,   115,     7,  1722,\n",
      "          525,   159,     1,    28,    29,     1,     7,   136,     1,     1,\n",
      "           78,    28,     1,   882,   136,     7,     7,     7,   241,     7,\n",
      "          169,     1,   115,    93,     1,     7,   136,   169,   131,   169,\n",
      "            1,    88,   131,     7,     7,     1,     7,     1,     1,   689,\n",
      "          136,     1,     1,   525,     1,     1,     1,     7,     1,   136,\n",
      "            1,     1,     1,     1,    93,   115,     7,     1,     1,     7,\n",
      "            1,     7,   689,     1,     1,     7,     7,   689,   471,     1,\n",
      "          241,  1722,   689,     1,     1,     1,   525,   689,   169,  1203,\n",
      "          525,     7,   689,     1,    90,    28,     1,   115,     1,     1,\n",
      "           93,  1346,     1,   689,   131,     1,   689,   131,     1,   241,\n",
      "          689,     1,     1,   131,   136,    28,    60,   159,   115,    93,\n",
      "            7,    93,    93,   689,     1,   471,   131,    93,   159,   369,\n",
      "          426,     7,     7,   169,    29,    90,    28,    90,    29,   689,\n",
      "          131,   131,   131,   131,  1732,   131,   169,     7,   689,     1,\n",
      "          136,   241,     1,   689,   131,    88,     1,     7,   131,   131,\n",
      "          525,     1,   426,     7,   544,     1,   136,   525,     1,   136,\n",
      "            7,     7,    88,   689,   136,     1,     7,   689,   115,     7,\n",
      "           29,   131,   136,     7,     1,     1,     7,   169,   286,   136,\n",
      "            1,     7,    26,    88,    95,     7,     1,     7,     1,   136,\n",
      "            1,   131,   136,   136,     7,  2059,   130,     1,   131,   211,\n",
      "            7,     1,     1,     1,   525,     1,     1,    93,     1,    99,\n",
      "           72,     1,     1,   136,     1,   115,   230,     1,     1,     1,\n",
      "          689,   525,   261,   689,    95,   872,     1,     1,     1,     1,\n",
      "          131,   131,   525,     7,     1,   136,     1,   689,    28,   689,\n",
      "          131,     7,     7,   169,    88,   525,     1,   136,   136,   525,\n",
      "          689,    60,     7,   131,   241,  1203,     1,   471,   131,     7,\n",
      "            1,   241,   131,     1,   169,   689,     1,   241,   115,   131,\n",
      "            1,     7,   525,     1,     1,     1,    90,   525,     7,     7,\n",
      "            7,     7,   131,   525,   131,     1,     1,     1,     7,   169,\n",
      "            1,    29,   131,    88,   525,     1,   525,   115,     7,     1,\n",
      "        12072,   136,   136,   525,   689,     1,   882,     1,   525,   136,\n",
      "          689,   525,   426,     1,   131,   689,   115,   169,   131,   136,\n",
      "           88,   136,    72,   136,     1,    90,    93,   230,   115,   136])\n",
      "\ttensor([  131,   131,   136,   525,     1,     1,     7,     1,     1,     1,\n",
      "        12072,     1,   131,   689,    88,   115,   689,     7,     1,     7,\n",
      "            1,     1,   136,    28,     1,     7,   689,   689,    93,     1,\n",
      "          426,    90,     1,     1,   689,     1,     1,   131,     1,    88,\n",
      "            1,     1,     1,   136,     1,     1,   241,     1,   689,   159,\n",
      "            1,    90,   369,     7,   689,   131,   169,     1,     1,   169,\n",
      "          369,     7,    29,     1,   169,   525,     7,    90,     1,     7,\n",
      "            7,  1023,   136,     1,     1,   136,   205,   689,     7,   230,\n",
      "            1,    26,   525,   689,    93,   136,   369,     7,   169,    50,\n",
      "            7,    90,     1,   455,     1,    29,   525,   369,    29,   136,\n",
      "          131,   131,   136,     1,   689,   131,     1,     1,  1732,   525,\n",
      "          525,     1,   241,     1,     1,     1,     1,   169,     1,     1,\n",
      "          136,   241,     1,   689,   136,   136,     1,     7,     7,   689,\n",
      "          525,     1,   115,   136,     1,     1,     1,   525,     7,    88,\n",
      "          471,   131,   689,     1,   525,   136,     1,     7,     1,     1,\n",
      "          241,   525,   689,     1,     1,     7,     1,   689,    72,   136,\n",
      "            1,   131,   131,   426,   136,     1,     1,   689,  1203,     7,\n",
      "            7,     1,   169,    28,   689,     1,   544,   882,   689,     7,\n",
      "            1,     1,   426,    60,   689,   689,   130,     1,    29,   115,\n",
      "          131,   169,   689,   169,     1,     7,    60,   882,   130,     1,\n",
      "          131,   131,     1,     1,     1,     7,     7,     1,     1,    88,\n",
      "            7,   689,     1,   131,     7,     1,     1,     1,   131,     1,\n",
      "            7,     1,     1,    88,   131,   131,     1,   136,     1,     1,\n",
      "           88,   241,   689,   689,     7,     7,   689,     1,     1,   689,\n",
      "            1,     1,     7,   689,    39,   525,   115,     1,     1,     7,\n",
      "            7,   115,   689,     7,   689,   241,     7,   115,    88,     1,\n",
      "            7,   241,     1,   131,     1,     7,   689,     7,   241,     1,\n",
      "            7,     1,   136,   131,   241,     7,     1,   136,   169,     7,\n",
      "          131,    90,     1,     1,     7,   689,     1,     7,   131,   131,\n",
      "           90,   689,     7,     1,    72,   169,    29,   136,   205,     1,\n",
      "          131,   689,     1,     7,   131,     1,     7,   131,   131,     1,\n",
      "            7,     1,     1,    88,    90,   131,    90,     1,     1,     1,\n",
      "            1,     1,     7,     7,   136,     1,   689,     1,   115,     1,\n",
      "          689,   136,   241,   689,   241,    88,     1,     7,   131,     7,\n",
      "            7,   131,   131,    28,   689,   136,     1,     1,   169,     1,\n",
      "          115,     1,   136,     1,     1,   136,     7,     1,   689,   369,\n",
      "            7,     7,     7,     1,   205,   131,     1,   115,    88,   115,\n",
      "           29,   230,     1,     7,   689,   205,   131,   131,  1722,     7,\n",
      "           93,   136,     1,     1,   131,     1,   136,     1,    90,    88,\n",
      "            7,     3,   136,     7,    88,    93,     7,   169,     7,   426])\n",
      ")\n",
      "DEB: next_token_log_probas.shape=torch.Size([400, 33353])\tlog_probas.shape=torch.Size([400])\n",
      "Beam idx: tensor([65, 49, 50, 29, 35, 80, 88, 44, 24,  5, 11, 43, 12, 85, 65, 98,  3, 94,\n",
      "        47, 16, 43, 63, 98, 87, 98, 80,  5, 24, 90, 69, 49, 14, 30, 45, 49, 49,\n",
      "        87, 92, 85, 28, 88, 52, 77, 16, 16, 14, 88, 33, 86, 40, 98, 98, 76, 10,\n",
      "        72, 85, 56, 56, 15, 49,  4, 43, 65, 30, 42, 88, 27, 17, 88, 11, 49, 87,\n",
      "        52, 91, 73, 71, 28, 54, 12, 50, 36, 22,  1, 20, 43, 26, 50, 61,  0,  1,\n",
      "        59, 98, 43, 46, 65, 29, 43,  3, 26, 80])\tToken idx: tensor([ 131,   28,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,  136,  689,  131,    1,    1,  689,    7,    1,  426,  525,\n",
      "         169,    7,    7,    7,    1,  525,  261,    7,    1,    1,    1,    7,\n",
      "         136,    1,    7,   60,    7,    1,  689,   90,    7,  689,  169,    1,\n",
      "         525,    1,    1,    7,  689,    1,    1,    3,    1,    7,    1,  174,\n",
      "           1,   93,  169,    7,    1,   93,  689,    1,  689,  689,  525,  689,\n",
      "         689,    1,    1,    7, 1203,  689,    7,    3,    1,    7,    7,    1,\n",
      "         159,    7,  689,    1,   88,   88,    1,   26,    3,  525,  689,    3,\n",
      "         115,  136,  689,  115])\tRaw indices: indices_to_process=tensor([2168076, 1634325, 1667651,  967238, 1167356, 2668241, 2935065, 1467533,\n",
      "         800473,  166766,  366884, 1434180,  400237, 2835006, 2168081, 3269283,\n",
      "         100190, 3135183, 1567592,  534337, 1434186, 2101240, 3269020, 2902236,\n",
      "        3268763, 2668247,  166772,  800479, 3001771, 2301882, 1634558,  466949,\n",
      "        1000591, 1500886, 1634298, 1634304, 2901847, 3068477, 2835012,  933944,\n",
      "        2935071, 1734357, 2568870,  533738,  533655,  467631, 2935233, 1100650,\n",
      "        2868883, 1334121, 3268595, 3268601, 2535517,  333531, 2401417, 2835008,\n",
      "        1867769, 1867775,  500296, 1634471,  133413, 1434272, 2168114, 1000597,\n",
      "        1400827, 2935157,  901220,  567002, 2935753,  367572, 1634822, 2902400,\n",
      "        1735045, 3035124, 2434770, 2368070,  935087, 1801751,  400243, 1667653,\n",
      "        1200709,  733773,   33360,  667061, 1434338,  867185, 1668339, 2034534,\n",
      "             88,   33441, 1967828, 3268620, 1434182, 1534763, 2168634,  967240,\n",
      "        1434294,  100195,  867867, 2668355])\n",
      "Beam idx: tensor([15, 61,  3, 86, 75, 26, 61, 30, 37, 16, 66, 50, 36, 14, 56, 24, 98, 77,\n",
      "        40, 34, 72, 75, 37, 22, 61, 61, 43, 92, 52, 26, 52, 85, 61, 99,  9, 57,\n",
      "         7, 77, 37,  6, 83,  9, 11, 30, 79, 82,  8, 83, 62, 53, 44, 24, 55, 55,\n",
      "        66, 45, 35, 77, 37, 74, 51, 50, 15, 61, 37,  3, 26,  3, 26, 57, 54, 52,\n",
      "        26, 55, 61, 84, 17,  5, 86, 64, 61, 21, 61,  5, 75, 19, 55, 36, 59, 24,\n",
      "        96, 55, 38, 37, 75, 15, 30, 40, 82, 92])\tToken idx: tensor([   1,    7,    1,  131,    1,    1,    1,  131,  131,    1,    1,  131,\n",
      "           1,    1,    1,    1,  131,    7,    1,    1,    1,    7,  689,    1,\n",
      "           3,  115,    1,  689,    7,  689,  689,    1,   88,    1,  131,  689,\n",
      "           1,  689,  136,    1,    7,  689,    1,  136,  525,    1,    1,  241,\n",
      "           1,    1,  131,  241,  689,  136,  689,    1,    1,   90,    1,  689,\n",
      "         131,  136,    7, 1346,  169,  689,  169,    3,   93,    1,    1,   90,\n",
      "           3,  169,  369,    1,    1,    1,  525,    7,   93,    1,  689,    7,\n",
      "          90,    1,    1,  689,    1,   72,    7,   29,    1,  230,  689,    3,\n",
      "           1,   93,    7,    1])\tRaw indices: indices_to_process=tensor([ 500296, 2034540,  100060, 2868489, 2501476,  867179, 2034534, 1000721,\n",
      "        1234192,  533649, 2201299, 1667781, 1200709,  466943, 1867769,  800473,\n",
      "        3268725, 2568188, 1334121, 1134003, 2401417, 2501482, 1234750,  733767,\n",
      "        2034536, 2034648, 1434180, 3069165, 1734363,  867867, 1735045, 2835006,\n",
      "        2034621, 3301948,  300308, 1901810,  233472, 2568870, 1234197,  200119,\n",
      "        2768306,  300866,  366884, 1000726, 2635412, 2734947,  266825, 2768540,\n",
      "        2067887, 1767710, 1467663,  800713, 1835104, 1834551, 2201987, 1500886,\n",
      "        1167356, 2568271, 1234062, 2468811, 1701134, 1667786,  500302, 2035879,\n",
      "        1234230,  100748,  867347,  100062,  867271, 1901122, 1801063, 1734446,\n",
      "         867181, 1834584, 2034902, 2801653,  567002,  166766, 2868883, 2134599,\n",
      "        2034626,  700414, 2035222,  166772, 2501565,  633708, 1834416, 1201397,\n",
      "        1967828,  800544, 3201895, 1834444, 1267415, 1234291, 2502164,  500298,\n",
      "        1000591, 1334213, 2734953, 3068477])\n",
      "Beam idx: tensor([74, 41, 22, 64, 16, 59, 76, 18,  1, 43, 71, 95, 99, 95, 62, 51,  8, 38,\n",
      "        82, 18, 63,  1, 41, 44, 41, 18, 32, 93, 11, 32, 74, 51, 86, 51, 56, 98,\n",
      "        47, 49, 53, 22, 85, 18, 61, 75, 62, 51, 22, 53, 42, 51, 30, 61, 59,  4,\n",
      "        42,  0,  1, 46, 96, 79, 30, 61, 61, 55, 53, 36, 41, 23, 43, 53, 59, 51,\n",
      "         1, 23, 95, 53, 59, 32, 61, 41, 60, 42, 22, 95, 22, 23, 23, 61, 55, 41,\n",
      "        23, 28, 25,  8, 30, 53, 30, 41, 66, 63])\tToken idx: tensor([  131,     1,     1,     1,     1,     1,     1,     7,    88,   689,\n",
      "            1,     7,     1,     1,     1,   136,     1,     1,     1,     1,\n",
      "            7,     7,     3,     1,     7,    88,   689,     1,   689,     7,\n",
      "          136, 12072,     1,   525,     1,     1,     1,   525,     3,     7,\n",
      "          689,   115,     1,     1,     7,   689,     3,     1,   689,   471,\n",
      "          689,   169,   689,     1,     1,    88,     1,   136,     1,     1,\n",
      "          169,   689,   525,   525,   136,   689,   689,     1,    90,   169,\n",
      "            3,   131,   115,   689,   241,    29,     7,    90,    93,    90,\n",
      "            1,   169,   115,   115,   369,     7,    28,   136,   136,    93,\n",
      "          169,     1,     1,     7,    29,   689,     1,   369,   689,     1])\tRaw indices: indices_to_process=tensor([2468253, 1367474,  733767, 2134593,  533649, 1967828, 2534829,  600361,\n",
      "          33441, 1434868, 2368064, 3168542, 3301948, 3168536, 2067887, 1701139,\n",
      "         266825, 1267415, 2734947,  600355, 2101246,   33360, 1367476, 1467533,\n",
      "        1367480,  600442, 1067985, 3101830,  367572, 1067303, 2468258, 1713075,\n",
      "        2868359, 1701528, 1867769, 3268595, 1567592, 1634822, 1767712,  733773,\n",
      "        2835694,  600469, 2034534, 2501476, 2067893, 1701692,  733769, 1767710,\n",
      "        1401515, 1701474, 1001279, 2034702, 1968516,  133413, 1400827,      88,\n",
      "          33354, 1534374, 3201889, 2634888, 1000759, 2035222, 2035058, 1834940,\n",
      "        1767845, 1201397, 1368162,  767120, 1434269, 1767878, 1967830, 1701134,\n",
      "          33468,  767808, 3168776, 1767738, 1967834, 1067386, 2034626, 1367563,\n",
      "        2001181, 1400995,  733881, 3168650,  734135,  767126,  767147, 2034669,\n",
      "        1834551, 1367566,  767288,  933885,  833826,  266831, 1000619, 1768398,\n",
      "        1000591, 1367842, 2201987, 2101240])\n",
      "Beam idx: tensor([66, 17, 63, 65, 80, 98, 61, 80,  3, 84, 98,  3, 74,  3, 76, 80, 99, 57,\n",
      "        84, 70, 70, 98, 74, 84, 79, 79, 92, 22, 83,  3, 61, 80,  5, 65, 61, 18,\n",
      "        22, 74, 31,  4, 73, 63, 92, 17, 25, 87, 12, 48, 76, 73, 61, 66, 22, 25,\n",
      "        79,  3, 95, 63,  3,  9, 99, 16, 33, 84, 63, 28, 54, 70, 45, 98, 22,  3,\n",
      "        28, 66, 63, 80, 17, 76, 31, 61,  3, 73,  3, 65,  0,  3, 22, 31, 28, 31,\n",
      "         9, 84, 61,  8, 45, 80, 24, 70, 63, 84])\tToken idx: tensor([    1,     1,     1,     1,  1203,   689,   689,   241,   689,     1,\n",
      "          525,   525,   689,   136,     7,    60,     7,     1,    88,   169,\n",
      "          689,   136,    90,     7,   525,   689,     1,   136,     1,   169,\n",
      "          169,     1,     1,     7,     1,     1,   525,     7,     1,     1,\n",
      "          136,     7,     7,     7,   241,     1,     1,     1,   115,   131,\n",
      "          426,     3,   689,     7,   136,    29,  1203,     3,   131,     1,\n",
      "          241,     1,     1,   115,    93,   689,     1,   426,     7,     1,\n",
      "            1,     1, 12072,     7,   241,     7,     3,     1,   169,    29,\n",
      "          230,     1,   471,     3,    88,   882,   169,     7,     1,   689,\n",
      "            7,  1722,     3,     1,     1,    72,     1,    29,   525,   241])\tRaw indices: indices_to_process=tensor([2201299,  567002, 2101240, 2167946, 2669443, 3269283, 2035222, 2668481,\n",
      "         100748, 2801653, 3269119,  100584, 2468811,  100195, 2534835, 2668300,\n",
      "        3301954, 1901122, 2801740, 2334879, 2335399, 3268730, 2468212, 2801659,\n",
      "        2635412, 2635576, 3068477,  733902, 2768300,  100228, 2034702, 2668241,\n",
      "         166766, 2167952, 2034534,  600355,  734291, 2468129, 1033944,  133413,\n",
      "        2434905, 2101246, 3068483,  567008,  834066, 2901712,  400237, 1600945,\n",
      "        2534943, 2434900, 2034959, 2201301,  734455,  833832, 2635023,  100088,\n",
      "        3169738, 2101242,  100190,  300178, 3302188,  533649, 1100650, 2801767,\n",
      "        2101332,  934573, 1801063, 2335136, 1500892, 3268595,  733767,  100060,\n",
      "         945956, 2201305, 2101480, 2668247,  567004, 2534829, 1034112, 2034562,\n",
      "         100289, 2434770,  100530, 2167948,      88,  100941,  733935, 1033950,\n",
      "         933885, 1034632,  300184, 2803374, 2034536,  266825, 1500886, 2668312,\n",
      "         800473, 2334739, 2101764, 2801893])\n",
      "DEB: t.shape=torch.Size([10, 400])\n",
      "tensor=(\n",
      "\ttensor([   7,  115,  159,  261,   93,    1,   29,  118, 2059,   91,   50,  169,\n",
      "        1165,  371,  704,  579,  426, 1023,  130,   78, 1893,  872,  709, 2964,\n",
      "        1123,  462, 3694,  369,  239,   95,  510,  262,  689,   87,  236,  131,\n",
      "         518,  455, 1213, 1497,  874, 1203,  305,   60,   40,   99, 1385,  706,\n",
      "         241, 1913, 1302,  283,   26,  190,  885,  235,  877, 1804,   28,  447,\n",
      "        4740,  206,  286,  230,  212,  124,  148, 2416, 1231,  719,  502,  322,\n",
      "         298, 4303,  158,   75,  141,   65,  523, 1604,  198, 1891, 1602, 3414,\n",
      "         204,  788,  205, 1169, 2066,  769, 1214,    8, 2438, 1799,  964, 1963,\n",
      "        1474, 1338,  368,  864,    1,  525,    7,   93,  136,  130,  115,  159,\n",
      "          29,   28, 1023,  689,  426,  131,  169, 1722,  241,  455,   60,  230,\n",
      "         369,   65,  471,  261,  212,   50, 2059,   91,  205,  502,  510, 1203,\n",
      "          26,   90,   99,  371,  305,   87,  343,   76,  239,  523, 1496,   95,\n",
      "        1169,  198,  118,  528,  243,  882, 1500, 1318,  719,  435,  704,  190,\n",
      "         283,    8,   72, 1346,  124,  158,  709,  431,  204, 1970, 1214,  998,\n",
      "        1469,  114,   78,  463,  885,   43, 2351, 1497,  237, 1123, 1491,  456,\n",
      "        1604, 1549, 1150,  211,  329,  872,  206,   39,   53, 2028,  153,  119,\n",
      "         171,  579, 1385,  518,  129, 3414, 2325,  447,  525,    1,  130,  136,\n",
      "          93,  131,    7,  115,   28, 1023,  689,   29,   60,  159,  426,  241,\n",
      "         471,  169, 1722, 1203,  230,  455,  369,   65, 2059, 1318,  510,  261,\n",
      "         502,  212,   72,  205,    8, 1169,  528,  435,   50,  343,   91,   90,\n",
      "        1150,   43,   87,   76,  124, 1496, 1500,   26, 1970,  463,  305,   99,\n",
      "        1469,  998,  243, 1491,  719,  523,  114,   95,  239, 2325,  118,  882,\n",
      "        2351, 1346,  371, 1549, 1959,  543,  456,  431, 3732,  329,  283,  198,\n",
      "         158,  148,  204,  237,  709, 1604,  579,  190,  885, 2028,  153,   53,\n",
      "        4071, 1214,  704,  171,  307,  119, 1684,  250,  569, 1497,   71,  549,\n",
      "         525,  130,  131,  136,    1,    7,   93,  689,  241,   28,   29,  115,\n",
      "        1203,  169, 1023,  426,  159,   60, 2059,  230,  528,  455,   65,  471,\n",
      "          99,  502,   91,  261,   90,  510, 1318,  882,  205, 1722,   76,    8,\n",
      "         579,   95,  148,   26,  305,   72,  704,  369, 1500,  118,  709,  719,\n",
      "         463, 1214,  190,   39,  239,  431, 1469, 1169,   50,   43,  124,   87,\n",
      "         114,  157,   80,  243,   74,  212,  204,  872,  343,  158, 1970,  371,\n",
      "         435,  283,  134, 2028,   78, 1496, 1150,  198,  518,  523,  456,  885,\n",
      "        1346,   53,  586, 1604,  211,  543,  998,  419, 1491,  895,  237,   71,\n",
      "         171,  153,  763,  129])\n",
      "\ttensor([   28,     7,   131,  1203,    60,     7,     1,   525,   525,    93,\n",
      "          115,     1,    29,   872,   241,   261,   136,     7,   525,   525,\n",
      "          131,   159,     7,   115,   689,     1,   169,   136,     1,     7,\n",
      "          261,     7,   426,   525,   174,   131,  1023,   130,   525,   130,\n",
      "          471,    72,     7, 12072,     7,   525,  1722,     7,   131,   525,\n",
      "         1383,   115,    91,  2059,   115,   115,   159,   525,   515,     1,\n",
      "          525,   131,  1406,     7,     1,    93,  1496,     7,    93,    93,\n",
      "          130,    50,    93,   525,   689,   525,  1231,   525,   455,   471,\n",
      "            7,    28,   241,   525,     7,     1,   689,   130,  1722,   525,\n",
      "          305,   118,     7,   882,   131,   169,   115,   239,   510,    29,\n",
      "          131,   131,   525,     1,   525,     7,   525,   115,   525,  1203,\n",
      "          136,  1732,     7,   241,     7,    28,   525,   525,   525,    60,\n",
      "           93,   131,   136,   130,   115,   159,   689,     7,   115,   525,\n",
      "          525,   525,     7,     7,   136,   131,     7,   143,     1,     7,\n",
      "          130,     1,   872,     1,   261,    29,   525,   130,   525,     1,\n",
      "           72,     1,    28,   115,     7,   369,   131,  3460,   525,     1,\n",
      "          525,     1,   115,  5278,   525, 12072,     1,   130,   426,     7,\n",
      "          261,   525,    93,  1023,   515,   131,   689,   169,   174,   169,\n",
      "          159,   130,   525,   131,   371,    29,   471,  1406,     7,     1,\n",
      "            7,   169,    90,   241,   115,    93,     7,   136,    60,   544,\n",
      "          131,   131,   525,     7,     1,   136,   261,   525,   579,   525,\n",
      "         2059,    93,    29,   525,   169,  1891,   704,   525,  1732,     1,\n",
      "          115,  1203,    60,   159,   872,    95,    99,   525,   510,    91,\n",
      "          241,     7,     1,     7,     7,   131,    28,   305,   709,   130,\n",
      "          118,     1,   525,   426,   115,     7,   211,     7,    78,   230,\n",
      "            7,  1231,    93,   689,   130,   455,   525,   239,   143,   141,\n",
      "          136,     7,     7,   525,   769,  1023,   130,   502,    72,    26,\n",
      "          131,   136,   136,   371,  1469,   115,    50,   115,   525,   124,\n",
      "         1213,   241,    93,   130,   115,     1,   438,  1406,   131,    65,\n",
      "            1,    28,    93,    94,     7,   689,   261,   689,   525,  1123,\n",
      "          131,   131,   525,     7,   136,   261,   579,  2059,    93,    29,\n",
      "          169,  1891,   704,     1,   115,   159,   872,    95,    99,   510,\n",
      "           91,     1,   305,   709,   130,   118,   525,   426,     7,   211,\n",
      "           78,   230,  1231,   689,   455,     7,     1,   239,   525,   141,\n",
      "          525,   769,  1023,   136,   525,   502,  1732,    26,     7,   371,\n",
      "         1469,   115,   130,    50,    60,   124,  1213,     7,   438,    65,\n",
      "         1203,   689,    94,  1123,  4740,   115,   190,   235,   158,    40,\n",
      "            1,   525,    93,    28,  1203,   447,   439,   518,   525,   241,\n",
      "          206,     7,    87,  3236,   887,   428,     7,   719,  1722,   877,\n",
      "         1804,   130,  1893,   204,  1604,    60,   241,  1385,  2416,  1751])\n",
      "\ttensor([ 131,  131,  131, 1732,  525,  131,  131, 1732,    7,    1,  131,  131,\n",
      "         131,  131,  525,  525,  525,  131,  131,  131,  131,   28,  525,  872,\n",
      "         131,  525,    1,  115,    7,    1,  131,    7,  131,    7,  131,  525,\n",
      "         525,    7,    7,  261,  131,  525,  130,  525,  525,  261,  136,  136,\n",
      "           7,  136,  525,    7,  241,    7,    1,  525,  525,    1,    1,  136,\n",
      "        1722,  525,   60,   60,    7,  241,   60,    1,  131,    1, 1203,  704,\n",
      "         136,  131, 1203, 1203,    1,  136,  159,   88,  241,  544,  115,  689,\n",
      "         525,    7,   60,  115,    1,    1,  136,    7,  689,  131,  136,  241,\n",
      "          28,  241,  136,  241,  131,  131,  131,  131,  131,  131,  525,  525,\n",
      "         131,  131,  131,  131,   28,   60,  525,  131,    7,    7,    7,  525,\n",
      "           7,  131,    7,  525,  136,    7,  136,  131,    1,  525,    7, 1203,\n",
      "         136,    1,  131,  115,    7,  241, 1203,    1,  131,  136, 1203,  261,\n",
      "           1,  115,  689,   60,  525,  261,  136,    1,    1,  115,  525,  241,\n",
      "         169,    1,  241,    1,  115,  131,  169,    1,  241,    7,    1,   28,\n",
      "           1,    7,    7,   95,  136,    7,    1,    7,  131,    7,  525,  525,\n",
      "         131,    1,    7,    7,    7,    1,    1,  704,  174,  115,  241,  131,\n",
      "         525,    1,  130,  525,  525,   99,  115,  131,  131,  131,  131,  131,\n",
      "         525,  131,  131,  525,  131,    7,  525,  131,    7,  131,  115,  525,\n",
      "           7,  872,  131,  131,    1,  525,  525,    1,    7,  525,  525,  136,\n",
      "         515,    1,    7,  131,    7,    1,  115,  131,  130,  131,  115, 1383,\n",
      "           7,  241,    1,  525,  136,  241,  115,  525,    7,  525,  525,   60,\n",
      "           7, 1891,    7,    7,  525,    7,    7,   60,    1,   93,  525,    7,\n",
      "         136,  689,  115,    7,    7,    7,  525,    7, 1203,   95,  241,    1,\n",
      "           7,    1,  131,  115,  211,  115,  136,   60, 1203,  131,    7,    7,\n",
      "           1,  136,    1,    7,  525,    1,    7,  872,  169,  131,  579,  639,\n",
      "         131,  131,  131,   28, 1203,   60,  131,  131,  525,    7,    1,    7,\n",
      "         131,  515,    7,    7,    7,  241,  525,  525,    7, 1732,  131,  131,\n",
      "           1,    7,    7,  131,  169,  131,  115,  525,    7,  115,  115, 1383,\n",
      "         525,    1,  241,    7,    1,  525,  130,    7,  115,  689,    7,    1,\n",
      "           1,  136, 1406,  525,    1,  525,  426,   93,  525,    1,  872,   28,\n",
      "         525,  525,    7,  136,    1,  241,   29,    7,    1,  515,  115,    7,\n",
      "         261,   72,  525,    1, 5278,   72,  169,  689,  525,  241,    7,    7,\n",
      "           7,   28,  136,  115,    7,  115,   93, 3248,  261,  115,    7,    7,\n",
      "           7,  241,  689,  689])\n",
      "\ttensor([  131,   131,   131,   131,   131,   525,     7,   136,   136,  1732,\n",
      "          525,   131,     1,    28,   131,     7,     1,   525,   131,   136,\n",
      "          115,     7,   525,   525,     7,     1,   136,     7,     1,   131,\n",
      "          131,  1732,   525,   689,   131,   131,   169,     7,     7,     7,\n",
      "          525,   525,   131,   261,     1,     1,     1,     7,   131,   131,\n",
      "          131,     7,     7,   115,     1,   136,   241,   136,   169,   525,\n",
      "          525,    95,     7,   131,   131,     7,   169,   115,   169,    99,\n",
      "          115,   115,     1,    93,   131,   525,     1,     7,   136,   169,\n",
      "          115,   131,   115,   689,   525,   525,   115,     7,   115,     7,\n",
      "          136,   130,     1,   159,     7,     1,   131,   241,     1,   525,\n",
      "        12072,   525,   131,   131,   131,   525,     1,   131,   131,   131,\n",
      "          525,   131,     7,   525,     7,   131,     1,     1,   136,   525,\n",
      "          136,   131,   131,   131,   136,   169,   131,   131,     1,   136,\n",
      "          131,   131,     1,   136,     1,   131,   131,   136,   525,   689,\n",
      "          136,   525,   169,   241,   689,   525, 14399,   131,   131,   130,\n",
      "          525,   525,     7,   115,   136,    29,    93,     1,     1,     1,\n",
      "            1,   131,     7,   525,     7,   131,   130,   115,   689,    93,\n",
      "          131,   131,   471,   241,   115,     7,   136,   689,   169,     1,\n",
      "          115,   471,   525,   169,    93,     1,   169,   136,    29,   241,\n",
      "            7,   525,   169,   525,     1,   241,    29,    29,     1,   426,\n",
      "          131,   525,     1,     7,   136,   131,   131,   525,    60,   131,\n",
      "          131,   131,   131,   525,   525,     7,     1,   525,  1203,   130,\n",
      "          131,   525,   131,     7,   131,     1,     1,     1,     7,   136,\n",
      "          131,   115,   131,   525,     1,   115,   115,   131,   525,   525,\n",
      "          136,   525,   525,     1,   136,     7,     1,  1722,   525,   115,\n",
      "          525,     7,   136,   169,   131,   115,   169,   136,    93,     7,\n",
      "            1,   131,   131,   525,     1,   689,   241,     7,   525,   131,\n",
      "            7,   136,   131,     7,     1,     1,     1,   136,    93,   136,\n",
      "          241,   136,   131,  1406,   689,   689,   241,   241,   136,   689,\n",
      "          131,  1722,    28,   525,     1,   130,     7,   159,     1,     1,\n",
      "          131,   131,   131,   131,     7,     7,     7,   525,     7,   136,\n",
      "          241,     7,     1,   525,     7,   136,   115,     7,     7,   115,\n",
      "         1203,   525,   136,     1,   169,     1,     1,     7,   241,   689,\n",
      "           60,     7,   525,     7,   525,     1,     7,   115,     7,   131,\n",
      "           88,   525,   525,     7,   525, 12072,   115,     1,   131,   525,\n",
      "          689,   115,   131,    60,   115,   525,   136,   525,   241,   525,\n",
      "            1,   136,   525,   131,   241,   131,     1,     1,     1,   169,\n",
      "          131,   426,   136,   131,   136,   131,    72,    72,   689,    93,\n",
      "          525,   525,   115,     7,   115,   131,     1,     1,   115,     1,\n",
      "           26,   115,     7,  1203,   689,   131,   169,   136,   131,   131])\n",
      "\ttensor([  131,   115,   131,   525,   131, 12072,     7,   131,   689,     1,\n",
      "            1,   131,   131,   131,     7,     7,   525,   525,     7,    90,\n",
      "         1722,     1,   131,   131,    88,     7,   241,   131,   136,   525,\n",
      "            1,   131,     1,  1203,   136,    93,   131,   159,    60,   115,\n",
      "            1,    88,     7,     1,   241,     7,     7,     7,     1,   131,\n",
      "          115,     7,     1,     1,   689,   131,     1,   241,    88,    28,\n",
      "         1722,     1,     7,     1,   525,   131,   136,     1,    88,   525,\n",
      "            7,   136,   115,   136,     1,   131,     7,     1,     7,     7,\n",
      "            1,     1,     7,    72,   131,     1,     7,   261,     7,   525,\n",
      "            1,   115,    78,   169,   131,   115,     7,    87,   115,     1,\n",
      "          131,   131,   131,   131,   131,     7,     7,   131,   131,     1,\n",
      "        12072,     7,     7,     7,     1,   241,     7,     7,   131,   689,\n",
      "          525,     7,   131,   525,     7,     7,   131,   136,     7,     7,\n",
      "            1,     7,     7,     7,   525,     1,   115,   115,     1,   241,\n",
      "          136,     7,   115,   131,     7,     1,    88,     1,   131,   241,\n",
      "          115,   241,    88,     1,   131,     7,     1,   131,   689,   525,\n",
      "          136,    88,    90,   131,     1,    72,   115,     1,    88,   115,\n",
      "          136,     1,   689,     1,     1,   131,     1,     7,    29,     1,\n",
      "          115,   131,     7,   525,   115,    60,    88,   136,     7,   136,\n",
      "            1,     1,   525,   115,  1722,     1,   525,   241,   115,     1,\n",
      "          131,   525,   131,   131,   131,   136,   525,   131,   131,   131,\n",
      "          131,   131,   169,   525,   131,   131,   131,   525,     1,   689,\n",
      "          525,   525,  1732,   131,   131,   131,   130,   525,   131,   131,\n",
      "          136,   241,     7,    29,    93,     1,     1,  1732,     1,     1,\n",
      "          136,   689,     7,   230,     1,     1,   131,     7,   131,   136,\n",
      "          136,   136,     7,   426,   241,   689,   136,     7,    28,    90,\n",
      "            1,     7,   131,     1,   115,     7,   211,   131,     1,   525,\n",
      "          455,   136,    99,     7,     1,   471,    95,     1,     1,     1,\n",
      "          241,     7,   136,   131,   136,   136,     1,    72,    91,   510,\n",
      "          131,     1,   241,   525,   525,    76,   115,   131,   502,     1,\n",
      "          131,   131,   525,   136,   131,   525,     7,     7,     1,   131,\n",
      "            7,   241,   131,   131,     7,   136,     1,   689,     1,   130,\n",
      "            1,     7,    88,   525,   131,   131,     1,   169,   136,     1,\n",
      "            7,   136,   131,   115,   131,     1,   115,    95,   115,    99,\n",
      "            1,   136,   241,   115,     7,   169,     1,     1,   131,   525,\n",
      "          525,   211,    93,   525,   525,   471,   471,  2059,   704,     1,\n",
      "          131,   131,   131,   525,     7,   131,   169,   131,   131,   525,\n",
      "          241,    29,  1722,   872,     7,     1,  1203,   525,    28,   689,\n",
      "           72,    93,   211,     7,     1,   525,     1,    93,   131,     1,\n",
      "          579,   230,    29,     1,    28,   689,  1891,    95,    93,    93])\n",
      "\ttensor([  131,  1732,  1732,   136,   131,     7,   131,     7,   241,     1,\n",
      "            1,   131,     1,     1,     7,     1,   689,   525,   525,   136,\n",
      "            7,   131,     1,     1,   131,     1,     7,   689,     1,   115,\n",
      "         1203,   115,   136,   136,   525,    93,     1,     7,     1,     1,\n",
      "          159,   525,    72,     1,     1,    28,     7,     1,    88,    60,\n",
      "            7,   131,     1,   689,     7,   136,     1,     1,   131,   115,\n",
      "          241, 12072,  1722,     1,   115,    29,   169,    88,    28,     1,\n",
      "            7,   169,   689,    88,    90,   882,     7,    29,   131,   136,\n",
      "            7,     7,   136,  3856,    93,   544,    93,    72,   689,   115,\n",
      "            1,    93,   689,   136,   131,     1,   426,   471,   205,   471,\n",
      "          131, 12072,   131,   689,   131,   131,     7,     1,     1,   689,\n",
      "          136, 12072,   131,     7,     1,   131,     1,   115,     7,   131,\n",
      "          169,     1,   241,     1,     1,    88,   136, 12072,   136,   136,\n",
      "          131,   131,   115,     1,     1,     7,   136,   131,   136,   131,\n",
      "          131,     1,   525,   131,   136,   426,     7,   136,     1,   169,\n",
      "            1,     1,   131,     1,   882,     1,     1,     1,     7,    72,\n",
      "          689,   689,     1,   525,     1,     1,   689,   115,  1722,   369,\n",
      "          525,    93,   136,     1,     7,    90,   689,   525,   169,   115,\n",
      "            7,     1,     1,   241,   525,    93,     1,   525,     7,   525,\n",
      "            1,     7,     1,   525,   205,    90, 14399,   136,    95,     1,\n",
      "          131,   131,   131,   131,   131,   131,   131,   136, 12072,     1,\n",
      "          525,     7,     7,     7,   131,   525,   525,   525,   525,     1,\n",
      "            7,     7,   525,   136,   525,   131,   136,     7,    88,     1,\n",
      "            7,   136,   689,     1,    88,   525,   169,    88,     7,     7,\n",
      "          689,   131,     7,     7,     1,   131,     7,     7,     1,   136,\n",
      "            7,   115,     7,     1,   131,   115,   131,     1,   525,   115,\n",
      "            1,     1,    88,   689,     7,   131,   115,   131,   241,     1,\n",
      "            1,     1,   136,   131,   115,   169,     1,   136,    93,     7,\n",
      "            1,     1,     7,   136,   471,     7,   115,   131,   131,     7,\n",
      "            7,    28,   130,     1,   241,     1,     1,   525,   689,   130,\n",
      "          131,  1732,   131,   131,   131,     1,   525,   689,     7,   131,\n",
      "            1,     1,     1,     7,     7,     7,   131,   426,     1,     1,\n",
      "          136,     7,   241,     7,     1,   169,   525,     1,   115,     7,\n",
      "           88,     7,    88,     1,   115,     1,     7,     1,   525,   131,\n",
      "            7,   525,     1,  1722,   136,  1203,     7,    60,     7,   136,\n",
      "          136,   131,   525,     1,    88,   525,     1,   241,    90,   115,\n",
      "          115,   131,   131,     7,   241,   525,     1,     7,   131,   136,\n",
      "            7,     7,   525,   131,   136,   241,   115,   689,   525,   115,\n",
      "            7,     7,    29,     1,    93,   525,   159,   115,     1,   136,\n",
      "          131,   205,     1,   241,     7,   136,   131,    93,   689,   689])\n",
      "\ttensor([  131,   525,   131,   131,   136,  1732,  1732,     7,     7,   131,\n",
      "            1,     7,     1,     1,   689,   689,   136,     1,   131,     1,\n",
      "           88,     7,   525,   136,     7,   136,   525,   131,    88,     1,\n",
      "            1,     7,   115,     1,   525,     1,   136,     7,     1,     1,\n",
      "          689,     7,     1,     7,     1,     1,   241,     1,    90,   241,\n",
      "            1,   169,   136,     1,    29,     7,   689,     1,   882,     1,\n",
      "          130,     1,   136,   471,   525,    88,   689,   115,  2059,  1203,\n",
      "            1,    60,   115,   689,     7,   131,   689,     7,   230,    88,\n",
      "          169,   115,   426,   471,     1,   136,    93,   169,     1,   241,\n",
      "          169,     1,     1,     1,  1722,   525,   115,     7,    29,   115,\n",
      "          131,   131,  1732,     1,     1,   131,     1,     1,     1,     7,\n",
      "          689,   136, 12072,   689,     1,    88,   136,     1,     7,     7,\n",
      "           88,     1,     1,     1,     1,   241,   241,     7,     7,     7,\n",
      "            7,     7,   525,     7,     1,   131,   136,     1,   689,   115,\n",
      "          115,    88,     1,   136,     1,     1,     7,     7,     7,   525,\n",
      "            1,     7,   131,   525,   136,     7,   689,     7,   131,   169,\n",
      "          544,    88,   689,   115,   136,     7,     1,   169,     7,    29,\n",
      "           88,   115,     1,  1722,     7,   131,   689,   689,   689,     7,\n",
      "            7,     1,     1,   131,     1,     1,     7,   131,     1,   169,\n",
      "          115,    93,   136,     1,  3856,   241,    60,   115,     1,    72,\n",
      "          131,   131,   131,   131,   136,   131,   131,     7,     1,   131,\n",
      "            1,     1,     7,   136,   131,     7,   136,    88,   525,    88,\n",
      "            1,   689,     1,   131,   115,     1,   689,     7,     1,    93,\n",
      "          136,     1,   131,   131,     7,   131,   525,   241,   136,   525,\n",
      "          689,     7,   136,  1722,     1,   136,   525,   525,    60,  1203,\n",
      "          159,     7,   136,  1722,    28,     1,     1,   136,     1,     7,\n",
      "          169,     7,   131,    88,     1,    93,    29,   136,     1,    90,\n",
      "            7,  1346,   159,    88,   525,   426,   369,   136,    90,     1,\n",
      "          525,   689,   689,   205,    29,    28,  1346,   131,   689,     7,\n",
      "           93,   525,   136,   115,     1,   136,  1891,   130,     1,    93,\n",
      "          131,  1732,     7,   241,     1,   131,     1,     1,     1,   131,\n",
      "          131,   136,   525,   689,   131,   131,   169,     1,   525,   544,\n",
      "         1732,   131,     1,     7,   131,   689,   689,   426,     7,   136,\n",
      "            1,     1,   136,     1,     7,   131,   131,   169,     7,   115,\n",
      "          136,     7,     7,     7,   131,   525,   525,   241,     1,   115,\n",
      "            1,   525,   136,   689,   525,   169,   689,    29,   525,    93,\n",
      "            7,     1,   115,    93,   115,     7,   169,    90,   131,   136,\n",
      "            7,     7,   689,   131,   426,    29,    88,   136,   131,    72,\n",
      "            1,   689,   136,     1,     7,   426,    60,     7,     7,   130,\n",
      "            1,   136,   426,     7,    29,   525,    26,   689,   136,   131])\n",
      "\ttensor([  131,   131,   131,     7,   689,   131,     1,     1,   136, 12072,\n",
      "            1,     7,     7,   136, 12072,     1,   689,     1,     7,   115,\n",
      "            1,   136,   131,   131,   131,     1,   131,     1,   115,     7,\n",
      "            1,     1,   525,     1,     7,     1,     1,     1,    90,   241,\n",
      "          689,     7,     1,  1732,    90,   525,   136,     1,   241,     1,\n",
      "            7,   426,   689,   131,   525,     1,   131,   689,     1,     1,\n",
      "          136,   136,     1,     7,     1,     7,   115,   115,     7,  1722,\n",
      "          525,   159,     1,    28,    29,     1,     7,   136,     1,     1,\n",
      "           78,    28,     1,   882,   136,     7,     7,     7,   241,     7,\n",
      "          169,     1,   115,    93,     1,     7,   136,   169,   131,   169,\n",
      "            1,    88,   131,     7,     7,     1,     7,     1,     1,   689,\n",
      "          136,     1,     1,   525,     1,     1,     1,     7,     1,   136,\n",
      "            1,     1,     1,     1,    93,   115,     7,     1,     1,     7,\n",
      "            1,     7,   689,     1,     1,     7,     7,   689,   471,     1,\n",
      "          241,  1722,   689,     1,     1,     1,   525,   689,   169,  1203,\n",
      "          525,     7,   689,     1,    90,    28,     1,   115,     1,     1,\n",
      "           93,  1346,     1,   689,   131,     1,   689,   131,     1,   241,\n",
      "          689,     1,     1,   131,   136,    28,    60,   159,   115,    93,\n",
      "            7,    93,    93,   689,     1,   471,   131,    93,   159,   369,\n",
      "          426,     7,     7,   169,    29,    90,    28,    90,    29,   689,\n",
      "          131,   131,   131,   131,  1732,   131,   169,     7,   689,     1,\n",
      "          136,   241,     1,   689,   131,    88,     1,     7,   131,   131,\n",
      "          525,     1,   426,     7,   544,     1,   136,   525,     1,   136,\n",
      "            7,     7,    88,   689,   136,     1,     7,   689,   115,     7,\n",
      "           29,   131,   136,     7,     1,     1,     7,   169,   286,   136,\n",
      "            1,     7,    26,    88,    95,     7,     1,     7,     1,   136,\n",
      "            1,   131,   136,   136,     7,  2059,   130,     1,   131,   211,\n",
      "            7,     1,     1,     1,   525,     1,     1,    93,     1,    99,\n",
      "           72,     1,     1,   136,     1,   115,   230,     1,     1,     1,\n",
      "          689,   525,   261,   689,    95,   872,     1,     1,     1,     1,\n",
      "          131,   131,   525,     7,     1,   136,     1,   689,    28,   689,\n",
      "          131,     7,     7,   169,    88,   525,     1,   136,   136,   525,\n",
      "          689,    60,     7,   131,   241,  1203,     1,   471,   131,     7,\n",
      "            1,   241,   131,     1,   169,   689,     1,   241,   115,   131,\n",
      "            1,     7,   525,     1,     1,     1,    90,   525,     7,     7,\n",
      "            7,     7,   131,   525,   131,     1,     1,     1,     7,   169,\n",
      "            1,    29,   131,    88,   525,     1,   525,   115,     7,     1,\n",
      "        12072,   136,   136,   525,   689,     1,   882,     1,   525,   136,\n",
      "          689,   525,   426,     1,   131,   689,   115,   169,   131,   136,\n",
      "           88,   136,    72,   136,     1,    90,    93,   230,   115,   136])\n",
      "\ttensor([  131,   131,   136,   525,     1,     1,     7,     1,     1,     1,\n",
      "        12072,     1,   131,   689,    88,   115,   689,     7,     1,     7,\n",
      "            1,     1,   136,    28,     1,     7,   689,   689,    93,     1,\n",
      "          426,    90,     1,     1,   689,     1,     1,   131,     1,    88,\n",
      "            1,     1,     1,   136,     1,     1,   241,     1,   689,   159,\n",
      "            1,    90,   369,     7,   689,   131,   169,     1,     1,   169,\n",
      "          369,     7,    29,     1,   169,   525,     7,    90,     1,     7,\n",
      "            7,  1023,   136,     1,     1,   136,   205,   689,     7,   230,\n",
      "            1,    26,   525,   689,    93,   136,   369,     7,   169,    50,\n",
      "            7,    90,     1,   455,     1,    29,   525,   369,    29,   136,\n",
      "          131,   131,   136,     1,   689,   131,     1,     1,  1732,   525,\n",
      "          525,     1,   241,     1,     1,     1,     1,   169,     1,     1,\n",
      "          136,   241,     1,   689,   136,   136,     1,     7,     7,   689,\n",
      "          525,     1,   115,   136,     1,     1,     1,   525,     7,    88,\n",
      "          471,   131,   689,     1,   525,   136,     1,     7,     1,     1,\n",
      "          241,   525,   689,     1,     1,     7,     1,   689,    72,   136,\n",
      "            1,   131,   131,   426,   136,     1,     1,   689,  1203,     7,\n",
      "            7,     1,   169,    28,   689,     1,   544,   882,   689,     7,\n",
      "            1,     1,   426,    60,   689,   689,   130,     1,    29,   115,\n",
      "          131,   169,   689,   169,     1,     7,    60,   882,   130,     1,\n",
      "          131,   131,     1,     1,     1,     7,     7,     1,     1,    88,\n",
      "            7,   689,     1,   131,     7,     1,     1,     1,   131,     1,\n",
      "            7,     1,     1,    88,   131,   131,     1,   136,     1,     1,\n",
      "           88,   241,   689,   689,     7,     7,   689,     1,     1,   689,\n",
      "            1,     1,     7,   689,    39,   525,   115,     1,     1,     7,\n",
      "            7,   115,   689,     7,   689,   241,     7,   115,    88,     1,\n",
      "            7,   241,     1,   131,     1,     7,   689,     7,   241,     1,\n",
      "            7,     1,   136,   131,   241,     7,     1,   136,   169,     7,\n",
      "          131,    90,     1,     1,     7,   689,     1,     7,   131,   131,\n",
      "           90,   689,     7,     1,    72,   169,    29,   136,   205,     1,\n",
      "          131,   689,     1,     7,   131,     1,     7,   131,   131,     1,\n",
      "            7,     1,     1,    88,    90,   131,    90,     1,     1,     1,\n",
      "            1,     1,     7,     7,   136,     1,   689,     1,   115,     1,\n",
      "          689,   136,   241,   689,   241,    88,     1,     7,   131,     7,\n",
      "            7,   131,   131,    28,   689,   136,     1,     1,   169,     1,\n",
      "          115,     1,   136,     1,     1,   136,     7,     1,   689,   369,\n",
      "            7,     7,     7,     1,   205,   131,     1,   115,    88,   115,\n",
      "           29,   230,     1,     7,   689,   205,   131,   131,  1722,     7,\n",
      "           93,   136,     1,     1,   131,     1,   136,     1,    90,    88,\n",
      "            7,     3,   136,     7,    88,    93,     7,   169,     7,   426])\n",
      "\ttensor([  131,    28,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,   136,   689,   131,     1,     1,   689,\n",
      "            7,     1,   426,   525,   169,     7,     7,     7,     1,   525,\n",
      "          261,     7,     1,     1,     1,     7,   136,     1,     7,    60,\n",
      "            7,     1,   689,    90,     7,   689,   169,     1,   525,     1,\n",
      "            1,     7,   689,     1,     1,     3,     1,     7,     1,   174,\n",
      "            1,    93,   169,     7,     1,    93,   689,     1,   689,   689,\n",
      "          525,   689,   689,     1,     1,     7,  1203,   689,     7,     3,\n",
      "            1,     7,     7,     1,   159,     7,   689,     1,    88,    88,\n",
      "            1,    26,     3,   525,   689,     3,   115,   136,   689,   115,\n",
      "            1,     7,     1,   131,     1,     1,     1,   131,   131,     1,\n",
      "            1,   131,     1,     1,     1,     1,   131,     7,     1,     1,\n",
      "            1,     7,   689,     1,     3,   115,     1,   689,     7,   689,\n",
      "          689,     1,    88,     1,   131,   689,     1,   689,   136,     1,\n",
      "            7,   689,     1,   136,   525,     1,     1,   241,     1,     1,\n",
      "          131,   241,   689,   136,   689,     1,     1,    90,     1,   689,\n",
      "          131,   136,     7,  1346,   169,   689,   169,     3,    93,     1,\n",
      "            1,    90,     3,   169,   369,     1,     1,     1,   525,     7,\n",
      "           93,     1,   689,     7,    90,     1,     1,   689,     1,    72,\n",
      "            7,    29,     1,   230,   689,     3,     1,    93,     7,     1,\n",
      "          131,     1,     1,     1,     1,     1,     1,     7,    88,   689,\n",
      "            1,     7,     1,     1,     1,   136,     1,     1,     1,     1,\n",
      "            7,     7,     3,     1,     7,    88,   689,     1,   689,     7,\n",
      "          136, 12072,     1,   525,     1,     1,     1,   525,     3,     7,\n",
      "          689,   115,     1,     1,     7,   689,     3,     1,   689,   471,\n",
      "          689,   169,   689,     1,     1,    88,     1,   136,     1,     1,\n",
      "          169,   689,   525,   525,   136,   689,   689,     1,    90,   169,\n",
      "            3,   131,   115,   689,   241,    29,     7,    90,    93,    90,\n",
      "            1,   169,   115,   115,   369,     7,    28,   136,   136,    93,\n",
      "          169,     1,     1,     7,    29,   689,     1,   369,   689,     1,\n",
      "            1,     1,     1,     1,  1203,   689,   689,   241,   689,     1,\n",
      "          525,   525,   689,   136,     7,    60,     7,     1,    88,   169,\n",
      "          689,   136,    90,     7,   525,   689,     1,   136,     1,   169,\n",
      "          169,     1,     1,     7,     1,     1,   525,     7,     1,     1,\n",
      "          136,     7,     7,     7,   241,     1,     1,     1,   115,   131,\n",
      "          426,     3,   689,     7,   136,    29,  1203,     3,   131,     1,\n",
      "          241,     1,     1,   115,    93,   689,     1,   426,     7,     1,\n",
      "            1,     1, 12072,     7,   241,     7,     3,     1,   169,    29,\n",
      "          230,     1,   471,     3,    88,   882,   169,     7,     1,   689,\n",
      "            7,  1722,     3,     1,     1,    72,     1,    29,   525,   241])\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9]])"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_lens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:52.719387900Z",
     "start_time": "2024-04-15T20:35:52.681950Z"
    }
   },
   "execution_count": 231
  },
  {
   "cell_type": "code",
   "source": [
    "for start_tokens_elem, start_tokens_lens_elem, decoded_tokens_elem, decoded_lens_elem, log_probas_elem in zip(\n",
    "    start_tokens.T, start_tokens_lens,\n",
    "    decoded_tokens.permute(1, 2, 0), decoded_lens.permute(0, 1), log_probas.permute(0, 1)\n",
    "):\n",
    "    start_tokens_elem = start_tokens_elem[:start_tokens_lens_elem].tolist()\n",
    "    start_words = np.array(lm_model.vocab.get_itos())[np.array(start_tokens_elem)]\n",
    "    \n",
    "    print(' '.join(start_words))\n",
    "    start_text = ' '.join(start_words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "    # display(Markdown(f'<div class=\"alert alert-block alert-info\"> <b>{start_text}</b></div>'))\n",
    "    \n",
    "    for idx, (hyp, hyp_len, hyp_log_prob) in enumerate(zip(decoded_tokens_elem, decoded_lens_elem, log_probas_elem)):\n",
    "        if idx >= 3:\n",
    "            break\n",
    "            \n",
    "        hyp = hyp[:hyp_len].tolist()\n",
    "        # print(f\"Tokens: {tokens}\")\n",
    "        hyp_words = np.array(lm_model.vocab.get_itos())[np.array(hyp)]\n",
    "        print(f\"Text: {' '.join(hyp_words)}, log proba: {hyp_log_prob:.4f}\")\n",
    "        # hyp_text = ' '.join(hyp_words).replace('<', '&lt;').replace('>', '&gt;')\n",
    "        # display(Markdown(\n",
    "        #     f'<div class=\"alert alert-block alert-success\"> <b>{hyp_log_prob:.3f}: {hyp_text}</b></div>'\n",
    "        # ))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:52.815120500Z",
     "start_time": "2024-04-15T20:35:52.689389800Z"
    }
   },
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>\n",
      "Text: movie time seen seen seen seen seen seen seen, log proba: -17.9353\n",
      "Text: film movie seen seen film ago ever seen seen, log proba: -17.9499\n",
      "Text: first seen seen seen seen ago seen seen made, log proba: -16.2724\n",
      "<sos> <unk> favorite movie\n",
      "Text: <unk> seen seen festival seen seen seen <unk> seen, log proba: -16.6200\n",
      "Text: ever seen seen ever seen festival seen times seen, log proba: -16.5165\n",
      "Text: movie ever seen seen seen seen ago seen made, log proba: -17.1615\n",
      "<sos> <unk> best movie\n",
      "Text: ever seen seen seen seen seen seen seen seen, log proba: -18.0969\n",
      "Text: <unk> seen seen ever ever seen seen seen seen, log proba: -18.2445\n",
      "Text: ive ever seen <unk> seen seen seen seen <unk>, log proba: -16.7330\n",
      "<sos> <unk> worst movie\n",
      "Text: ever seen seen seen seen seen seen seen seen, log proba: -17.5466\n",
      "Text: ive seen seen seen seen ago ago seen bad, log proba: -15.8310\n",
      "Text: seen ever seen seen ever seen movie ever <unk>, log proba: -15.9171\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуйте выполнить декодинг для разных `beam_size`. Убедитесь, что при `beam_search=1` семплирование совпадает с top-1 (greedy decoding) подходом. \n",
    "\n",
    "Сравните результаты Beam Search с top-k семплированием и жадным декодированием. Опишите ваши наблюдения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:52.829122Z",
     "start_time": "2024-04-15T20:35:52.801604Z"
    }
   },
   "execution_count": 233,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Бонус. Существенное улучшение качества (до 6 баллов)`"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:04:51.678260Z",
     "start_time": "2021-04-02T15:04:51.673587Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Та модель, которая использовалась в предыдущей части во многом заимствует улучшения LSTM из статьи [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf). Вы можете попробовать применить другие варианты регуляризации из данной статьи для существенного улучшения качества LM.\n",
    "\n",
    "Например:\n",
    "1. Dropout для эмбеддингов **(+0.25)**\n",
    "2. Dropout входов и выходов RNN **(+0.25)**\n",
    "3. Регуляризация активаций (AR/TAR) **(+1.0)**\n",
    "4. NT-ASGD **(+1.5)**\n",
    "5. Tied веса эмбеддингов и софтмакса **(+1.0)**\n",
    "6. Attention **(+2.0)**\n",
    "\n",
    "**Полные баллы ставятся только при наличии качественного и количественного сравнения с бейзлайном.**\n",
    "\n",
    "**Для эксперимента с Attention необходимо изобразить Attention Maps для нескольких примеров.**"
   ],
   "metadata": {}
  }
 ]
}
